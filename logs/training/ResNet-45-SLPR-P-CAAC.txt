You have chosen to seed training. This will slow down your training!
Construct dataset.
5131 training items found.
1791 valid items found.
Construct model.
Model(
  (backbone): ResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (decoder): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(size=(4, 32), mode=nearest)
        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=6068, bias=True)
)
The parameters size of model is 18.143252 MB
Construct learner.
Start training.
epoch 1 iter 40: loss = 8.5110, smooth loss = 9.9770, ce loss = 3.4817, contrastive loss = 1.5475, lr = 0.00022954798257166908
epoch 2 iter 80: loss = 7.5237, smooth loss = 8.4355, ce loss = 3.0177, contrastive loss = 1.4883, lr = 0.00031746436089163076
epoch 3 iter 120: loss = 7.2497, smooth loss = 7.7077, ce loss = 2.8757, contrastive loss = 1.4983, lr = 0.00046158434194791716
epoch 4 iter 160: loss = 6.9607, smooth loss = 7.2923, ce loss = 2.7438, contrastive loss = 1.4731, lr = 0.0006583592135001254
epoch 5 iter 200: loss = 6.5216, smooth loss = 6.9795, ce loss = 2.5212, contrastive loss = 1.4792, lr = 0.0009029437251522861
epoch 6 iter 240: loss = 6.5036, smooth loss = 6.6817, ce loss = 2.5300, contrastive loss = 1.4437, lr = 0.0011893153944980642
epoch 7 iter 280: loss = 5.9489, smooth loss = 6.4229, ce loss = 2.2674, contrastive loss = 1.4142, lr = 0.0015104228006250875
epoch 8 iter 320: loss = 5.9425, smooth loss = 6.2012, ce loss = 2.2652, contrastive loss = 1.4120, lr = 0.001858359213500126
epoch 9 iter 360: loss = 5.4188, smooth loss = 5.9174, ce loss = 2.0219, contrastive loss = 1.3750, lr = 0.0022245572839034457
epoch 10 iter 400: loss = 5.0241, smooth loss = 5.5603, ce loss = 1.8260, contrastive loss = 1.3720, lr = 0.0026
epoch 11 iter 440: loss = 4.5059, smooth loss = 5.1988, ce loss = 1.5900, contrastive loss = 1.3259, lr = 0.0029754427160965545
epoch 12 iter 480: loss = 4.9141, smooth loss = 4.8954, ce loss = 1.7850, contrastive loss = 1.3442, lr = 0.003341640786499874
epoch 13 iter 520: loss = 4.0900, smooth loss = 4.6049, ce loss = 1.3810, contrastive loss = 1.3280, lr = 0.0036895771993749123
epoch 14 iter 560: loss = 3.5125, smooth loss = 4.1907, ce loss = 1.1313, contrastive loss = 1.2499, lr = 0.0040106846055019355
epoch 15 iter 600: loss = 3.3087, smooth loss = 3.8829, ce loss = 1.0507, contrastive loss = 1.2073, lr = 0.004297056274847714
epoch 16 iter 640: loss = 3.5313, smooth loss = 3.6854, ce loss = 1.1356, contrastive loss = 1.2601, lr = 0.0045416407864998735
epoch 17 iter 680: loss = 2.7874, smooth loss = 3.4159, ce loss = 0.7963, contrastive loss = 1.1948, lr = 0.004738415658052083
epoch 18 iter 720: loss = 2.6235, smooth loss = 3.2326, ce loss = 0.7285, contrastive loss = 1.1664, lr = 0.004882535639108369
epoch 19 iter 760: loss = 3.3414, smooth loss = 3.0698, ce loss = 1.0532, contrastive loss = 1.2350, lr = 0.004970452017428331
epoch 20 iter 800: loss = 2.8546, smooth loss = 2.9363, ce loss = 0.8238, contrastive loss = 1.2071, lr = 0.005
epoch 21 iter 840: loss = 2.7420, smooth loss = 2.7851, ce loss = 0.7761, contrastive loss = 1.1898, lr = 0.004999619239414027
epoch 22 iter 880: loss = 2.7726, smooth loss = 2.6854, ce loss = 0.7824, contrastive loss = 1.2079, lr = 0.00499847707363947
epoch 23 iter 920: loss = 2.6104, smooth loss = 2.6079, ce loss = 0.7165, contrastive loss = 1.1774, lr = 0.004996573850591087
epoch 24 iter 960: loss = 2.1608, smooth loss = 2.5493, ce loss = 0.5229, contrastive loss = 1.1150, lr = 0.004993910150009058
epoch 25 iter 1000: loss = 2.2779, smooth loss = 2.4414, ce loss = 0.5668, contrastive loss = 1.1442, lr = 0.004990486783282383
epoch 26 iter 1040: loss = 2.3715, smooth loss = 2.4124, ce loss = 0.6195, contrastive loss = 1.1326, lr = 0.0049863047932017296
epoch 27 iter 1080: loss = 1.8786, smooth loss = 2.3268, ce loss = 0.4004, contrastive loss = 1.0777, lr = 0.004981365453641789
epoch 28 iter 1120: loss = 2.0229, smooth loss = 2.2980, ce loss = 0.4482, contrastive loss = 1.1264, lr = 0.004975670269173239
epoch 29 iter 1160: loss = 2.3022, smooth loss = 2.2719, ce loss = 0.5809, contrastive loss = 1.1404, lr = 0.004969220974604439
epoch 30 iter 1200: loss = 1.9290, smooth loss = 2.2321, ce loss = 0.4130, contrastive loss = 1.1029, lr = 0.00496201953445299
epoch 31 iter 1240: loss = 2.3025, smooth loss = 2.1844, ce loss = 0.5896, contrastive loss = 1.1233, lr = 0.004954068142347326
epoch 32 iter 1280: loss = 1.9204, smooth loss = 2.1456, ce loss = 0.4191, contrastive loss = 1.0822, lr = 0.004945369220358507
epoch 33 iter 1320: loss = 2.2309, smooth loss = 2.1345, ce loss = 0.5594, contrastive loss = 1.1121, lr = 0.004935925418262441
epoch 34 iter 1360: loss = 1.8791, smooth loss = 2.0977, ce loss = 0.4030, contrastive loss = 1.0732, lr = 0.004925739612732728
epoch 35 iter 1400: loss = 1.9262, smooth loss = 2.0734, ce loss = 0.4215, contrastive loss = 1.0832, lr = 0.004914814906464408
epoch 36 iter 1440: loss = 1.8154, smooth loss = 2.0708, ce loss = 0.3754, contrastive loss = 1.0646, lr = 0.004903154627228838
epoch 37 iter 1480: loss = 2.1952, smooth loss = 2.0371, ce loss = 0.5416, contrastive loss = 1.1119, lr = 0.004890762326860029
epoch 38 iter 1520: loss = 1.8756, smooth loss = 2.0120, ce loss = 0.4019, contrastive loss = 1.0719, lr = 0.004877641780172721
epoch 39 iter 1560: loss = 2.0378, smooth loss = 1.9969, ce loss = 0.4752, contrastive loss = 1.0874, lr = 0.004863796983812537
epoch 40 iter 1600: loss = 1.8492, smooth loss = 1.9638, ce loss = 0.3877, contrastive loss = 1.0738, lr = 0.004849232155038563
epoch 41 iter 1640: loss = 1.7458, smooth loss = 1.9466, ce loss = 0.3544, contrastive loss = 1.0370, lr = 0.004833951730438739
epoch 42 iter 1680: loss = 1.6196, smooth loss = 1.9331, ce loss = 0.2913, contrastive loss = 1.0370, lr = 0.004817960364578423
epoch 43 iter 1720: loss = 1.6139, smooth loss = 1.9223, ce loss = 0.2970, contrastive loss = 1.0200, lr = 0.0048012629285825665
epoch 44 iter 1760: loss = 1.8539, smooth loss = 1.8902, ce loss = 0.3939, contrastive loss = 1.0661, lr = 0.004783864508651926
epoch 45 iter 1800: loss = 1.9391, smooth loss = 1.8776, ce loss = 0.4361, contrastive loss = 1.0669, lr = 0.004765770404513755
epoch 46 iter 1840: loss = 1.7612, smooth loss = 1.8606, ce loss = 0.3559, contrastive loss = 1.0494, lr = 0.004746986127807455
epoch 47 iter 1880: loss = 1.6965, smooth loss = 1.8590, ce loss = 0.3386, contrastive loss = 1.0194, lr = 0.004727517400405678
epoch 48 iter 1920: loss = 2.0412, smooth loss = 1.8522, ce loss = 0.4687, contrastive loss = 1.1039, lr = 0.004707370152671389
epoch 49 iter 1960: loss = 1.8831, smooth loss = 1.8340, ce loss = 0.4043, contrastive loss = 1.0746, lr = 0.004686550521651418
epoch 50 iter 2000: loss = 1.6900, smooth loss = 1.8273, ce loss = 0.3222, contrastive loss = 1.0457, lr = 0.00466506484920706
epoch 51 iter 2040: loss = 1.6563, smooth loss = 1.8261, ce loss = 0.3130, contrastive loss = 1.0303, lr = 0.004642919680082274
epoch 52 iter 2080: loss = 1.8873, smooth loss = 1.8012, ce loss = 0.4116, contrastive loss = 1.0641, lr = 0.004620121759910103
epoch 53 iter 2120: loss = 1.7490, smooth loss = 1.7785, ce loss = 0.3443, contrastive loss = 1.0605, lr = 0.004596678033157881
epoch 54 iter 2160: loss = 1.5305, smooth loss = 1.7616, ce loss = 0.2559, contrastive loss = 1.0187, lr = 0.004572595641011879
epoch 55 iter 2200: loss = 1.8208, smooth loss = 1.7726, ce loss = 0.3814, contrastive loss = 1.0579, lr = 0.004547881919202037
epoch 56 iter 2240: loss = 1.7417, smooth loss = 1.7709, ce loss = 0.3568, contrastive loss = 1.0282, lr = 0.004522544395767425
epoch 57 iter 2280: loss = 1.7600, smooth loss = 1.7581, ce loss = 0.3545, contrastive loss = 1.0510, lr = 0.004496590788763132
epoch 58 iter 2320: loss = 1.8401, smooth loss = 1.7644, ce loss = 0.3954, contrastive loss = 1.0493, lr = 0.004470029003909268
epoch 59 iter 2360: loss = 1.8780, smooth loss = 1.7663, ce loss = 0.4049, contrastive loss = 1.0683, lr = 0.004442867132182813
epoch 60 iter 2400: loss = 1.7384, smooth loss = 1.7506, ce loss = 0.3455, contrastive loss = 1.0475, lr = 0.004415113447353014
epoch 61 iter 2440: loss = 1.9972, smooth loss = 1.7406, ce loss = 0.4580, contrastive loss = 1.0811, lr = 0.0043867764034611284
epoch 62 iter 2480: loss = 1.6486, smooth loss = 1.7050, ce loss = 0.3158, contrastive loss = 1.0170, lr = 0.0043578646322452305
epoch 63 iter 2520: loss = 1.5799, smooth loss = 1.6973, ce loss = 0.2846, contrastive loss = 1.0106, lr = 0.00432838694051091
epoch 64 iter 2560: loss = 1.6697, smooth loss = 1.6998, ce loss = 0.3179, contrastive loss = 1.0339, lr = 0.004298352307448625
epoch 65 iter 2600: loss = 1.4944, smooth loss = 1.7088, ce loss = 0.2443, contrastive loss = 1.0059, lr = 0.004267769881898557
epoch 66 iter 2640: loss = 1.6305, smooth loss = 1.6974, ce loss = 0.3005, contrastive loss = 1.0294, lr = 0.004236648979563789
epoch 67 iter 2680: loss = 1.7216, smooth loss = 1.6665, ce loss = 0.3452, contrastive loss = 1.0312, lr = 0.0042049990801726455
epoch 68 iter 2720: loss = 1.5734, smooth loss = 1.6740, ce loss = 0.2774, contrastive loss = 1.0186, lr = 0.004172829824591082
epoch 69 iter 2760: loss = 1.4000, smooth loss = 1.6467, ce loss = 0.2025, contrastive loss = 0.9950, lr = 0.004140151011885978
epoch 70 iter 2800: loss = 1.5259, smooth loss = 1.6415, ce loss = 0.2521, contrastive loss = 1.0217, lr = 0.004106972596340251
epoch 71 iter 2840: loss = 1.5512, smooth loss = 1.6490, ce loss = 0.2661, contrastive loss = 1.0191, lr = 0.004073304684420683
epoch 72 iter 2880: loss = 1.4340, smooth loss = 1.6368, ce loss = 0.2125, contrastive loss = 1.0090, lr = 0.004039157531699393
epoch 73 iter 2920: loss = 1.4434, smooth loss = 1.6021, ce loss = 0.2211, contrastive loss = 1.0012, lr = 0.00400454153972989
epoch 74 iter 2960: loss = 1.4609, smooth loss = 1.6029, ce loss = 0.2225, contrastive loss = 1.0159, lr = 0.00396946725287866
epoch 75 iter 3000: loss = 1.5551, smooth loss = 1.6195, ce loss = 0.2682, contrastive loss = 1.0187, lr = 0.003933945355113252
epoch 76 iter 3040: loss = 1.7420, smooth loss = 1.6265, ce loss = 0.3422, contrastive loss = 1.0575, lr = 0.0038979866667478323
epoch 77 iter 3080: loss = 1.5262, smooth loss = 1.6105, ce loss = 0.2614, contrastive loss = 1.0034, lr = 0.003861602141147218
epoch 78 iter 3120: loss = 1.7449, smooth loss = 1.6069, ce loss = 0.3551, contrastive loss = 1.0346, lr = 0.00382480286139037
epoch 79 iter 3160: loss = 1.5423, smooth loss = 1.6030, ce loss = 0.2658, contrastive loss = 1.0107, lr = 0.0037876000368943868
epoch 80 iter 3200: loss = 1.5047, smooth loss = 1.5955, ce loss = 0.2547, contrastive loss = 0.9953, lr = 0.003750005
epoch 81 iter 3240: loss = 1.4897, smooth loss = 1.5849, ce loss = 0.2393, contrastive loss = 1.0112, lr = 0.0037120292025196408
epoch 82 iter 3280: loss = 1.5696, smooth loss = 1.5764, ce loss = 0.2749, contrastive loss = 1.0197, lr = 0.003673684212249099
epoch 83 iter 3320: loss = 1.4337, smooth loss = 1.5411, ce loss = 0.2231, contrastive loss = 0.9874, lr = 0.0036349817094438693
epoch 84 iter 3360: loss = 1.4952, smooth loss = 1.5462, ce loss = 0.2442, contrastive loss = 1.0067, lr = 0.0035959334832612257
epoch 85 iter 3400: loss = 1.5760, smooth loss = 1.5546, ce loss = 0.2768, contrastive loss = 1.0224, lr = 0.0035565514281691315
epoch 86 iter 3440: loss = 1.4683, smooth loss = 1.5344, ce loss = 0.2346, contrastive loss = 0.9991, lr = 0.00351684754032307
epoch 87 iter 3480: loss = 1.4435, smooth loss = 1.5373, ce loss = 0.2283, contrastive loss = 0.9870, lr = 0.003476833913911899
epoch 88 iter 3520: loss = 1.5303, smooth loss = 1.5420, ce loss = 0.2581, contrastive loss = 1.0140, lr = 0.0034365227374738463
epoch 89 iter 3560: loss = 1.5619, smooth loss = 1.5256, ce loss = 0.2698, contrastive loss = 1.0224, lr = 0.0033959262901837547
epoch 90 iter 3600: loss = 1.4379, smooth loss = 1.5359, ce loss = 0.2159, contrastive loss = 1.0060, lr = 0.003355056938112739
epoch 91 iter 3640: loss = 1.4997, smooth loss = 1.5079, ce loss = 0.2450, contrastive loss = 1.0096, lr = 0.0033139271304613474
epoch 92 iter 3680: loss = 1.6069, smooth loss = 1.5224, ce loss = 0.2878, contrastive loss = 1.0314, lr = 0.003272549395767425
epoch 93 iter 3720: loss = 1.3392, smooth loss = 1.5324, ce loss = 0.1802, contrastive loss = 0.9788, lr = 0.0032309363380897947
epoch 94 iter 3760: loss = 1.3771, smooth loss = 1.5051, ce loss = 0.1961, contrastive loss = 0.9848, lr = 0.0031891006331689394
epoch 95 iter 3800: loss = 1.4568, smooth loss = 1.5046, ce loss = 0.2285, contrastive loss = 0.9998, lr = 0.003147055024565851
epoch 96 iter 3840: loss = 1.3647, smooth loss = 1.5014, ce loss = 0.1985, contrastive loss = 0.9676, lr = 0.003104812319780213
epoch 97 iter 3880: loss = 1.5619, smooth loss = 1.5058, ce loss = 0.2707, contrastive loss = 1.0205, lr = 0.0030623853863491193
epoch 98 iter 3920: loss = 1.5310, smooth loss = 1.5140, ce loss = 0.2662, contrastive loss = 0.9986, lr = 0.0030197871479274896
epoch 99 iter 3960: loss = 1.4253, smooth loss = 1.4817, ce loss = 0.2150, contrastive loss = 0.9954, lr = 0.0029770305803514083
epoch 100 iter 4000: loss = 1.3622, smooth loss = 1.4601, ce loss = 0.1943, contrastive loss = 0.9737, lr = 0.0029341287076855497
Save model train-seed-42-SLPR-P-32-256-bs-128-lr-0.005-d_model-512-epoch-200-decay-cos-grad-clip-20-AdamW-wd-0.1-PA-decoder-max-len-25-sup_con-1.0-ce-2.0-temperature-0.15-warm_up-0.1_100_4000
epoch 101 iter 4040: loss = 1.4850, smooth loss = 1.4594, ce loss = 0.2362, contrastive loss = 1.0125, lr = 0.002891094598255927
average data time = 0.0913s, average running time = 0.5683s
epoch 101 iter 4040: eval loss = 0.3473,  ccr = 0.9162,  cwr = 0.8420,  ted = 621.0000,  ned = 115.3270,  ted/w = 0.3467, 
Better model found at epoch 101, iter 4040 with accuracy value: 0.8420.
epoch 102 iter 4080: loss = 1.3250, smooth loss = 1.4447, ce loss = 0.1782, contrastive loss = 0.9685, lr = 0.002847941360669154
average data time = 0.0912s, average running time = 0.5697s
epoch 102 iter 4080: eval loss = 0.3301,  ccr = 0.9181,  cwr = 0.8476,  ted = 619.0000,  ned = 113.9016,  ted/w = 0.3456, 
Better model found at epoch 102, iter 4080 with accuracy value: 0.8476.
epoch 103 iter 4120: loss = 1.4140, smooth loss = 1.4555, ce loss = 0.2161, contrastive loss = 0.9819, lr = 0.0028046821398194344
average data time = 0.0911s, average running time = 0.5711s
epoch 103 iter 4120: eval loss = 0.3141,  ccr = 0.9249,  cwr = 0.8481,  ted = 600.0000,  ned = 116.6175,  ted/w = 0.3350, 
Better model found at epoch 103, iter 4120 with accuracy value: 0.8481.
epoch 104 iter 4160: loss = 1.3944, smooth loss = 1.4312, ce loss = 0.1999, contrastive loss = 0.9946, lr = 0.002761330112884501
average data time = 0.0911s, average running time = 0.5726s
epoch 104 iter 4160: eval loss = 0.3417,  ccr = 0.9129,  cwr = 0.8375,  ted = 652.0000,  ned = 117.9310,  ted/w = 0.3640, 
epoch 105 iter 4200: loss = 1.5020, smooth loss = 1.4177, ce loss = 0.2430, contrastive loss = 1.0160, lr = 0.0027178984853117186
average data time = 0.0910s, average running time = 0.5735s
epoch 105 iter 4200: eval loss = 0.3475,  ccr = 0.9148,  cwr = 0.8420,  ted = 623.0000,  ned = 114.5147,  ted/w = 0.3479, 
epoch 106 iter 4240: loss = 1.4236, smooth loss = 1.4139, ce loss = 0.2134, contrastive loss = 0.9968, lr = 0.0026744004867955756
average data time = 0.0910s, average running time = 0.5745s
epoch 106 iter 4240: eval loss = 0.3149,  ccr = 0.9233,  cwr = 0.8532,  ted = 585.0000,  ned = 109.1313,  ted/w = 0.3266, 
Better model found at epoch 106, iter 4240 with accuracy value: 0.8532.
epoch 107 iter 4280: loss = 1.4038, smooth loss = 1.3979, ce loss = 0.2003, contrastive loss = 1.0032, lr = 0.0026308493672477975
average data time = 0.0910s, average running time = 0.5757s
epoch 107 iter 4280: eval loss = 0.3349,  ccr = 0.9176,  cwr = 0.8425,  ted = 620.0000,  ned = 117.6675,  ted/w = 0.3462, 
epoch 108 iter 4320: loss = 1.2696, smooth loss = 1.4122, ce loss = 0.1473, contrastive loss = 0.9750, lr = 0.002587258392761286
average data time = 0.0909s, average running time = 0.5766s
epoch 108 iter 4320: eval loss = 0.2998,  ccr = 0.9257,  cwr = 0.8476,  ted = 577.0000,  ned = 109.3532,  ted/w = 0.3222, 
epoch 109 iter 4360: loss = 1.1946, smooth loss = 1.3962, ce loss = 0.1254, contrastive loss = 0.9438, lr = 0.002543640841569145
average data time = 0.0908s, average running time = 0.5774s
epoch 109 iter 4360: eval loss = 0.3458,  ccr = 0.9110,  cwr = 0.8286,  ted = 679.0000,  ned = 128.2845,  ted/w = 0.3791, 
epoch 110 iter 4400: loss = 1.3663, smooth loss = 1.4036, ce loss = 0.2004, contrastive loss = 0.9654, lr = 0.00250001
average data time = 0.0908s, average running time = 0.5783s
epoch 110 iter 4400: eval loss = 0.3358,  ccr = 0.9191,  cwr = 0.8476,  ted = 618.0000,  ned = 115.6611,  ted/w = 0.3451, 
epoch 111 iter 4440: loss = 1.3470, smooth loss = 1.3987, ce loss = 0.1882, contrastive loss = 0.9705, lr = 0.0024563791584308555
average data time = 0.0908s, average running time = 0.5792s
epoch 111 iter 4440: eval loss = 0.3265,  ccr = 0.9212,  cwr = 0.8515,  ted = 583.0000,  ned = 108.0960,  ted/w = 0.3255, 
epoch 112 iter 4480: loss = 1.4978, smooth loss = 1.3942, ce loss = 0.2519, contrastive loss = 0.9940, lr = 0.0024127616072387153
average data time = 0.0907s, average running time = 0.5800s
epoch 112 iter 4480: eval loss = 0.3396,  ccr = 0.9175,  cwr = 0.8481,  ted = 616.0000,  ned = 112.0190,  ted/w = 0.3439, 
epoch 113 iter 4520: loss = 1.1688, smooth loss = 1.3837, ce loss = 0.1041, contrastive loss = 0.9606, lr = 0.002369170632752203
average data time = 0.0906s, average running time = 0.5809s
epoch 113 iter 4520: eval loss = 0.3293,  ccr = 0.9185,  cwr = 0.8470,  ted = 627.0000,  ned = 117.1794,  ted/w = 0.3501, 
epoch 114 iter 4560: loss = 1.3683, smooth loss = 1.3603, ce loss = 0.1914, contrastive loss = 0.9854, lr = 0.002325619513204424
average data time = 0.0906s, average running time = 0.5817s
epoch 114 iter 4560: eval loss = 0.3143,  ccr = 0.9262,  cwr = 0.8599,  ted = 556.0000,  ned = 103.0365,  ted/w = 0.3104, 
Better model found at epoch 114, iter 4560 with accuracy value: 0.8599.
epoch 115 iter 4600: loss = 1.2053, smooth loss = 1.3741, ce loss = 0.1151, contrastive loss = 0.9751, lr = 0.002282121514688282
average data time = 0.0906s, average running time = 0.5828s
epoch 115 iter 4600: eval loss = 0.3128,  ccr = 0.9257,  cwr = 0.8520,  ted = 566.0000,  ned = 102.9536,  ted/w = 0.3160, 
epoch 116 iter 4640: loss = 1.3785, smooth loss = 1.3678, ce loss = 0.1933, contrastive loss = 0.9919, lr = 0.0022386898871154994
average data time = 0.0907s, average running time = 0.5836s
epoch 116 iter 4640: eval loss = 0.3311,  ccr = 0.9217,  cwr = 0.8492,  ted = 590.0000,  ned = 111.3401,  ted/w = 0.3294, 
epoch 117 iter 4680: loss = 1.3812, smooth loss = 1.3503, ce loss = 0.1977, contrastive loss = 0.9858, lr = 0.0021953378601805656
average data time = 0.0907s, average running time = 0.5844s
epoch 117 iter 4680: eval loss = 0.3246,  ccr = 0.9253,  cwr = 0.8582,  ted = 582.0000,  ned = 107.2599,  ted/w = 0.3250, 
epoch 118 iter 4720: loss = 1.4012, smooth loss = 1.3336, ce loss = 0.2101, contrastive loss = 0.9809, lr = 0.0021520786393308465
average data time = 0.0907s, average running time = 0.5851s
epoch 118 iter 4720: eval loss = 0.3073,  ccr = 0.9281,  cwr = 0.8654,  ted = 550.0000,  ned = 99.2683,  ted/w = 0.3071, 
Better model found at epoch 118, iter 4720 with accuracy value: 0.8654.
epoch 119 iter 4760: loss = 1.3585, smooth loss = 1.3451, ce loss = 0.1885, contrastive loss = 0.9815, lr = 0.0021089254017440727
average data time = 0.0907s, average running time = 0.5862s
epoch 119 iter 4760: eval loss = 0.3449,  ccr = 0.9223,  cwr = 0.8548,  ted = 606.0000,  ned = 113.2655,  ted/w = 0.3384, 
epoch 120 iter 4800: loss = 1.3063, smooth loss = 1.3458, ce loss = 0.1675, contrastive loss = 0.9714, lr = 0.0020658912923144507
average data time = 0.0907s, average running time = 0.5869s
epoch 120 iter 4800: eval loss = 0.3226,  ccr = 0.9236,  cwr = 0.8526,  ted = 568.0000,  ned = 104.6317,  ted/w = 0.3171, 
epoch 121 iter 4840: loss = 1.4247, smooth loss = 1.3445, ce loss = 0.2175, contrastive loss = 0.9896, lr = 0.0020229894196485917
average data time = 0.0907s, average running time = 0.5875s
epoch 121 iter 4840: eval loss = 0.3313,  ccr = 0.9257,  cwr = 0.8554,  ted = 562.0000,  ned = 104.1512,  ted/w = 0.3138, 
epoch 122 iter 4880: loss = 1.2585, smooth loss = 1.3269, ce loss = 0.1406, contrastive loss = 0.9773, lr = 0.0019802328520725104
average data time = 0.0906s, average running time = 0.5882s
epoch 122 iter 4880: eval loss = 0.3129,  ccr = 0.9243,  cwr = 0.8660,  ted = 563.0000,  ned = 100.3651,  ted/w = 0.3143, 
Better model found at epoch 122, iter 4880 with accuracy value: 0.8660.
epoch 123 iter 4920: loss = 1.2840, smooth loss = 1.3111, ce loss = 0.1538, contrastive loss = 0.9763, lr = 0.0019376346136508816
average data time = 0.0907s, average running time = 0.5893s
epoch 123 iter 4920: eval loss = 0.3230,  ccr = 0.9233,  cwr = 0.8599,  ted = 578.0000,  ned = 104.0611,  ted/w = 0.3227, 
epoch 124 iter 4960: loss = 1.2955, smooth loss = 1.3010, ce loss = 0.1671, contrastive loss = 0.9613, lr = 0.0018952076802197873
average data time = 0.0907s, average running time = 0.5898s
epoch 124 iter 4960: eval loss = 0.3285,  ccr = 0.9201,  cwr = 0.8537,  ted = 595.0000,  ned = 108.2810,  ted/w = 0.3322, 
epoch 125 iter 5000: loss = 1.4640, smooth loss = 1.3066, ce loss = 0.2274, contrastive loss = 1.0091, lr = 0.0018529649754341492
average data time = 0.0906s, average running time = 0.5904s
epoch 125 iter 5000: eval loss = 0.3329,  ccr = 0.9186,  cwr = 0.8476,  ted = 613.0000,  ned = 116.4373,  ted/w = 0.3423, 
epoch 126 iter 5040: loss = 1.0996, smooth loss = 1.2889, ce loss = 0.0881, contrastive loss = 0.9234, lr = 0.0018109193668310606
average data time = 0.0905s, average running time = 0.5911s
epoch 126 iter 5040: eval loss = 0.3243,  ccr = 0.9245,  cwr = 0.8643,  ted = 556.0000,  ned = 102.6683,  ted/w = 0.3104, 
epoch 127 iter 5080: loss = 1.1618, smooth loss = 1.2795, ce loss = 0.1142, contrastive loss = 0.9334, lr = 0.0017690836619102058
average data time = 0.0905s, average running time = 0.5917s
epoch 127 iter 5080: eval loss = 0.3296,  ccr = 0.9264,  cwr = 0.8660,  ted = 536.0000,  ned = 97.1488,  ted/w = 0.2993, 
epoch 128 iter 5120: loss = 1.3206, smooth loss = 1.2815, ce loss = 0.1723, contrastive loss = 0.9761, lr = 0.0017274706042325755
average data time = 0.0904s, average running time = 0.5924s
epoch 128 iter 5120: eval loss = 0.3370,  ccr = 0.9219,  cwr = 0.8532,  ted = 582.0000,  ned = 107.4302,  ted/w = 0.3250, 
epoch 129 iter 5160: loss = 1.3749, smooth loss = 1.2800, ce loss = 0.2011, contrastive loss = 0.9728, lr = 0.0016860928695386535
average data time = 0.0904s, average running time = 0.5930s
epoch 129 iter 5160: eval loss = 0.3184,  ccr = 0.9272,  cwr = 0.8554,  ted = 556.0000,  ned = 101.8734,  ted/w = 0.3104, 
epoch 130 iter 5200: loss = 1.2681, smooth loss = 1.2636, ce loss = 0.1582, contrastive loss = 0.9516, lr = 0.0016449630618872617
average data time = 0.0904s, average running time = 0.5936s
epoch 130 iter 5200: eval loss = 0.3161,  ccr = 0.9259,  cwr = 0.8593,  ted = 570.0000,  ned = 106.3929,  ted/w = 0.3183, 
epoch 131 iter 5240: loss = 1.0915, smooth loss = 1.2565, ce loss = 0.0847, contrastive loss = 0.9220, lr = 0.0016040937098162449
average data time = 0.0903s, average running time = 0.5941s
epoch 131 iter 5240: eval loss = 0.3201,  ccr = 0.9281,  cwr = 0.8677,  ted = 540.0000,  ned = 97.5508,  ted/w = 0.3015, 
Better model found at epoch 131, iter 5240 with accuracy value: 0.8677.
epoch 132 iter 5280: loss = 1.2827, smooth loss = 1.2624, ce loss = 0.1634, contrastive loss = 0.9559, lr = 0.001563497262526154
average data time = 0.0902s, average running time = 0.5951s
epoch 132 iter 5280: eval loss = 0.3240,  ccr = 0.9254,  cwr = 0.8615,  ted = 569.0000,  ned = 104.0956,  ted/w = 0.3177, 
epoch 133 iter 5320: loss = 1.2522, smooth loss = 1.2678, ce loss = 0.1495, contrastive loss = 0.9532, lr = 0.001523186086088101
average data time = 0.0902s, average running time = 0.5957s
epoch 133 iter 5320: eval loss = 0.2963,  ccr = 0.9270,  cwr = 0.8649,  ted = 545.0000,  ned = 100.3456,  ted/w = 0.3043, 
epoch 134 iter 5360: loss = 1.2871, smooth loss = 1.2761, ce loss = 0.1575, contrastive loss = 0.9721, lr = 0.001483172459676931
average data time = 0.0902s, average running time = 0.5964s
epoch 134 iter 5360: eval loss = 0.3101,  ccr = 0.9294,  cwr = 0.8621,  ted = 540.0000,  ned = 101.1710,  ted/w = 0.3015, 
epoch 135 iter 5400: loss = 1.2048, smooth loss = 1.2651, ce loss = 0.1328, contrastive loss = 0.9391, lr = 0.0014434685718308694
average data time = 0.0902s, average running time = 0.5969s
epoch 135 iter 5400: eval loss = 0.3117,  ccr = 0.9305,  cwr = 0.8643,  ted = 531.0000,  ned = 100.2671,  ted/w = 0.2965, 
epoch 136 iter 5440: loss = 1.2678, smooth loss = 1.2437, ce loss = 0.1507, contrastive loss = 0.9664, lr = 0.0014040865167387743
average data time = 0.0903s, average running time = 0.5974s
epoch 136 iter 5440: eval loss = 0.3022,  ccr = 0.9307,  cwr = 0.8654,  ted = 533.0000,  ned = 100.0897,  ted/w = 0.2976, 
epoch 137 iter 5480: loss = 1.1764, smooth loss = 1.2312, ce loss = 0.1128, contrastive loss = 0.9508, lr = 0.0013650382905561306
average data time = 0.0902s, average running time = 0.5978s
epoch 137 iter 5480: eval loss = 0.3108,  ccr = 0.9286,  cwr = 0.8593,  ted = 552.0000,  ned = 102.8913,  ted/w = 0.3082, 
epoch 138 iter 5520: loss = 1.1688, smooth loss = 1.2215, ce loss = 0.1159, contrastive loss = 0.9370, lr = 0.0013263357877509017
average data time = 0.0901s, average running time = 0.5983s
epoch 138 iter 5520: eval loss = 0.3082,  ccr = 0.9329,  cwr = 0.8666,  ted = 507.0000,  ned = 95.3825,  ted/w = 0.2831, 
epoch 139 iter 5560: loss = 1.1102, smooth loss = 1.2278, ce loss = 0.0923, contrastive loss = 0.9256, lr = 0.00128799079748036
average data time = 0.0901s, average running time = 0.5989s
epoch 139 iter 5560: eval loss = 0.3230,  ccr = 0.9306,  cwr = 0.8610,  ted = 538.0000,  ned = 98.3163,  ted/w = 0.3004, 
epoch 140 iter 5600: loss = 1.1217, smooth loss = 1.2233, ce loss = 0.0895, contrastive loss = 0.9428, lr = 0.0012500150000000008
average data time = 0.0900s, average running time = 0.5994s
epoch 140 iter 5600: eval loss = 0.3170,  ccr = 0.9284,  cwr = 0.8643,  ted = 545.0000,  ned = 100.4794,  ted/w = 0.3043, 
epoch 141 iter 5640: loss = 1.2525, smooth loss = 1.2171, ce loss = 0.1494, contrastive loss = 0.9537, lr = 0.0012124199631056137
average data time = 0.0900s, average running time = 0.5999s
epoch 141 iter 5640: eval loss = 0.2960,  ccr = 0.9317,  cwr = 0.8604,  ted = 520.0000,  ned = 96.8048,  ted/w = 0.2903, 
epoch 142 iter 5680: loss = 1.2696, smooth loss = 1.2134, ce loss = 0.1519, contrastive loss = 0.9658, lr = 0.0011752171386096306
average data time = 0.0901s, average running time = 0.6003s
epoch 142 iter 5680: eval loss = 0.2873,  ccr = 0.9349,  cwr = 0.8716,  ted = 489.0000,  ned = 90.2413,  ted/w = 0.2730, 
Better model found at epoch 142, iter 5680 with accuracy value: 0.8716.
epoch 143 iter 5720: loss = 1.2413, smooth loss = 1.1918, ce loss = 0.1446, contrastive loss = 0.9521, lr = 0.0011384178588527826
average data time = 0.0901s, average running time = 0.6011s
epoch 143 iter 5720: eval loss = 0.3137,  ccr = 0.9331,  cwr = 0.8682,  ted = 515.0000,  ned = 92.8778,  ted/w = 0.2875, 
epoch 144 iter 5760: loss = 1.1454, smooth loss = 1.1916, ce loss = 0.1018, contrastive loss = 0.9417, lr = 0.0011020333332521681
average data time = 0.0901s, average running time = 0.6016s
epoch 144 iter 5760: eval loss = 0.3119,  ccr = 0.9330,  cwr = 0.8721,  ted = 517.0000,  ned = 93.9167,  ted/w = 0.2887, 
Better model found at epoch 144, iter 5760 with accuracy value: 0.8721.
epoch 145 iter 5800: loss = 1.2875, smooth loss = 1.1964, ce loss = 0.1676, contrastive loss = 0.9523, lr = 0.001066074644886749
average data time = 0.0901s, average running time = 0.6024s
epoch 145 iter 5800: eval loss = 0.3120,  ccr = 0.9349,  cwr = 0.8677,  ted = 499.0000,  ned = 92.1821,  ted/w = 0.2786, 
epoch 146 iter 5840: loss = 1.1568, smooth loss = 1.1954, ce loss = 0.1078, contrastive loss = 0.9412, lr = 0.0010305527471213406
average data time = 0.0900s, average running time = 0.6028s
epoch 146 iter 5840: eval loss = 0.3106,  ccr = 0.9340,  cwr = 0.8727,  ted = 484.0000,  ned = 86.3310,  ted/w = 0.2702, 
Better model found at epoch 146, iter 5840 with accuracy value: 0.8727.
epoch 147 iter 5880: loss = 1.1550, smooth loss = 1.1782, ce loss = 0.1112, contrastive loss = 0.9325, lr = 0.0009954784602701108
average data time = 0.0900s, average running time = 0.6035s
epoch 147 iter 5880: eval loss = 0.2980,  ccr = 0.9329,  cwr = 0.8705,  ted = 514.0000,  ned = 94.3825,  ted/w = 0.2870, 
epoch 148 iter 5920: loss = 1.0064, smooth loss = 1.1789, ce loss = 0.0490, contrastive loss = 0.9083, lr = 0.0009608624683006075
average data time = 0.0900s, average running time = 0.6040s
epoch 148 iter 5920: eval loss = 0.3007,  ccr = 0.9323,  cwr = 0.8699,  ted = 502.0000,  ned = 92.9202,  ted/w = 0.2803, 
epoch 149 iter 5960: loss = 1.1393, smooth loss = 1.1725, ce loss = 0.0946, contrastive loss = 0.9502, lr = 0.0009267153155793173
average data time = 0.0901s, average running time = 0.6045s
epoch 149 iter 5960: eval loss = 0.3219,  ccr = 0.9305,  cwr = 0.8671,  ted = 523.0000,  ned = 97.3183,  ted/w = 0.2920, 
epoch 150 iter 6000: loss = 1.1817, smooth loss = 1.1548, ce loss = 0.1241, contrastive loss = 0.9335, lr = 0.0008930474036597485
average data time = 0.0900s, average running time = 0.6049s
epoch 150 iter 6000: eval loss = 0.3151,  ccr = 0.9325,  cwr = 0.8677,  ted = 511.0000,  ned = 94.8079,  ted/w = 0.2853, 
epoch 151 iter 6040: loss = 1.1160, smooth loss = 1.1630, ce loss = 0.0896, contrastive loss = 0.9368, lr = 0.000859868988114022
average data time = 0.0900s, average running time = 0.6053s
epoch 151 iter 6040: eval loss = 0.3223,  ccr = 0.9312,  cwr = 0.8638,  ted = 532.0000,  ned = 97.0155,  ted/w = 0.2970, 
epoch 152 iter 6080: loss = 1.1786, smooth loss = 1.1648, ce loss = 0.1192, contrastive loss = 0.9402, lr = 0.0008271901754089188
average data time = 0.0900s, average running time = 0.6057s
epoch 152 iter 6080: eval loss = 0.3024,  ccr = 0.9340,  cwr = 0.8738,  ted = 506.0000,  ned = 91.6290,  ted/w = 0.2825, 
Better model found at epoch 152, iter 6080 with accuracy value: 0.8738.
epoch 153 iter 6120: loss = 1.1485, smooth loss = 1.1618, ce loss = 0.1109, contrastive loss = 0.9268, lr = 0.0007950209198273547
average data time = 0.0900s, average running time = 0.6063s
epoch 153 iter 6120: eval loss = 0.3008,  ccr = 0.9307,  cwr = 0.8682,  ted = 521.0000,  ned = 96.2246,  ted/w = 0.2909, 
epoch 154 iter 6160: loss = 1.1032, smooth loss = 1.1517, ce loss = 0.0881, contrastive loss = 0.9269, lr = 0.0007633710204362112
average data time = 0.0900s, average running time = 0.6068s
epoch 154 iter 6160: eval loss = 0.3200,  ccr = 0.9289,  cwr = 0.8693,  ted = 520.0000,  ned = 92.9972,  ted/w = 0.2903, 
epoch 155 iter 6200: loss = 1.1361, smooth loss = 1.1516, ce loss = 0.0974, contrastive loss = 0.9412, lr = 0.0007322501181014433
average data time = 0.0900s, average running time = 0.6072s
epoch 155 iter 6200: eval loss = 0.3121,  ccr = 0.9321,  cwr = 0.8727,  ted = 506.0000,  ned = 92.4790,  ted/w = 0.2825, 
epoch 156 iter 6240: loss = 1.0490, smooth loss = 1.1384, ce loss = 0.0692, contrastive loss = 0.9106, lr = 0.000701667692551376
average data time = 0.0899s, average running time = 0.6075s
epoch 156 iter 6240: eval loss = 0.2978,  ccr = 0.9334,  cwr = 0.8777,  ted = 496.0000,  ned = 91.0302,  ted/w = 0.2769, 
Better model found at epoch 156, iter 6240 with accuracy value: 0.8777.
epoch 157 iter 6280: loss = 1.1070, smooth loss = 1.1269, ce loss = 0.0909, contrastive loss = 0.9251, lr = 0.00067163305948909
average data time = 0.0899s, average running time = 0.6082s
epoch 157 iter 6280: eval loss = 0.3090,  ccr = 0.9323,  cwr = 0.8738,  ted = 499.0000,  ned = 90.6659,  ted/w = 0.2786, 
epoch 158 iter 6320: loss = 1.0608, smooth loss = 1.1234, ce loss = 0.0710, contrastive loss = 0.9189, lr = 0.0006421553677547689
average data time = 0.0899s, average running time = 0.6087s
epoch 158 iter 6320: eval loss = 0.3152,  ccr = 0.9317,  cwr = 0.8733,  ted = 501.0000,  ned = 90.5044,  ted/w = 0.2797, 
epoch 159 iter 6360: loss = 1.1576, smooth loss = 1.1234, ce loss = 0.1097, contrastive loss = 0.9381, lr = 0.0006132435965388722
average data time = 0.0900s, average running time = 0.6092s
epoch 159 iter 6360: eval loss = 0.3147,  ccr = 0.9331,  cwr = 0.8744,  ted = 511.0000,  ned = 93.7218,  ted/w = 0.2853, 
epoch 160 iter 6400: loss = 1.1413, smooth loss = 1.1083, ce loss = 0.1117, contrastive loss = 0.9179, lr = 0.0005849065526469865
average data time = 0.0900s, average running time = 0.6095s
epoch 160 iter 6400: eval loss = 0.3095,  ccr = 0.9344,  cwr = 0.8766,  ted = 488.0000,  ned = 89.5313,  ted/w = 0.2725, 
epoch 161 iter 6440: loss = 1.0309, smooth loss = 1.1043, ce loss = 0.0675, contrastive loss = 0.8958, lr = 0.0005571528678171874
average data time = 0.0899s, average running time = 0.6098s
epoch 161 iter 6440: eval loss = 0.3075,  ccr = 0.9346,  cwr = 0.8777,  ted = 496.0000,  ned = 91.1040,  ted/w = 0.2769, 
epoch 162 iter 6480: loss = 1.0140, smooth loss = 1.1106, ce loss = 0.0562, contrastive loss = 0.9016, lr = 0.0005299909960907313
average data time = 0.0899s, average running time = 0.6102s
epoch 162 iter 6480: eval loss = 0.3046,  ccr = 0.9325,  cwr = 0.8699,  ted = 508.0000,  ned = 93.4587,  ted/w = 0.2836, 
epoch 163 iter 6520: loss = 1.1626, smooth loss = 1.1222, ce loss = 0.1086, contrastive loss = 0.9453, lr = 0.0005034292112368689
average data time = 0.0899s, average running time = 0.6106s
epoch 163 iter 6520: eval loss = 0.3107,  ccr = 0.9339,  cwr = 0.8749,  ted = 503.0000,  ned = 91.5262,  ted/w = 0.2808, 
epoch 164 iter 6560: loss = 1.0736, smooth loss = 1.1144, ce loss = 0.0810, contrastive loss = 0.9117, lr = 0.0004774756042325754
average data time = 0.0899s, average running time = 0.6110s
epoch 164 iter 6560: eval loss = 0.3008,  ccr = 0.9336,  cwr = 0.8766,  ted = 498.0000,  ned = 90.7778,  ted/w = 0.2781, 
epoch 165 iter 6600: loss = 1.0070, smooth loss = 1.1077, ce loss = 0.0591, contrastive loss = 0.8887, lr = 0.00045213808079796314
average data time = 0.0898s, average running time = 0.6114s
epoch 165 iter 6600: eval loss = 0.3258,  ccr = 0.9313,  cwr = 0.8733,  ted = 514.0000,  ned = 92.9067,  ted/w = 0.2870, 
epoch 166 iter 6640: loss = 1.0516, smooth loss = 1.0969, ce loss = 0.0718, contrastive loss = 0.9081, lr = 0.0004274243589881215
average data time = 0.0897s, average running time = 0.6117s
epoch 166 iter 6640: eval loss = 0.3099,  ccr = 0.9337,  cwr = 0.8766,  ted = 493.0000,  ned = 89.3940,  ted/w = 0.2753, 
epoch 167 iter 6680: loss = 1.1026, smooth loss = 1.0916, ce loss = 0.0882, contrastive loss = 0.9262, lr = 0.0004033419668421196
average data time = 0.0898s, average running time = 0.6122s
epoch 167 iter 6680: eval loss = 0.3071,  ccr = 0.9330,  cwr = 0.8760,  ted = 502.0000,  ned = 90.6377,  ted/w = 0.2803, 
epoch 168 iter 6720: loss = 1.0390, smooth loss = 1.0841, ce loss = 0.0617, contrastive loss = 0.9155, lr = 0.0003798982400898967
average data time = 0.0898s, average running time = 0.6125s
epoch 168 iter 6720: eval loss = 0.3032,  ccr = 0.9345,  cwr = 0.8749,  ted = 490.0000,  ned = 90.0964,  ted/w = 0.2736, 
epoch 169 iter 6760: loss = 1.1231, smooth loss = 1.0841, ce loss = 0.0941, contrastive loss = 0.9349, lr = 0.0003571003199177265
average data time = 0.0898s, average running time = 0.6129s
epoch 169 iter 6760: eval loss = 0.3164,  ccr = 0.9344,  cwr = 0.8744,  ted = 495.0000,  ned = 91.3214,  ted/w = 0.2764, 
epoch 170 iter 6800: loss = 1.0877, smooth loss = 1.0858, ce loss = 0.0809, contrastive loss = 0.9258, lr = 0.0003349551507929411
average data time = 0.0898s, average running time = 0.6132s
epoch 170 iter 6800: eval loss = 0.3046,  ccr = 0.9364,  cwr = 0.8850,  ted = 474.0000,  ned = 86.1909,  ted/w = 0.2647, 
Better model found at epoch 170, iter 6800 with accuracy value: 0.8850.
epoch 171 iter 6840: loss = 1.1507, smooth loss = 1.0814, ce loss = 0.1085, contrastive loss = 0.9337, lr = 0.000313469478348582
average data time = 0.0897s, average running time = 0.6138s
epoch 171 iter 6840: eval loss = 0.3055,  ccr = 0.9364,  cwr = 0.8822,  ted = 479.0000,  ned = 88.2766,  ted/w = 0.2674, 
epoch 172 iter 6880: loss = 0.9949, smooth loss = 1.0778, ce loss = 0.0486, contrastive loss = 0.8976, lr = 0.0002926498473286111
average data time = 0.0896s, average running time = 0.6142s
epoch 172 iter 6880: eval loss = 0.3086,  ccr = 0.9373,  cwr = 0.8816,  ted = 467.0000,  ned = 85.9087,  ted/w = 0.2607, 
epoch 173 iter 6920: loss = 1.0854, smooth loss = 1.0741, ce loss = 0.0822, contrastive loss = 0.9210, lr = 0.0002725025995943224
average data time = 0.0896s, average running time = 0.6146s
epoch 173 iter 6920: eval loss = 0.3151,  ccr = 0.9350,  cwr = 0.8811,  ted = 485.0000,  ned = 89.4421,  ted/w = 0.2708, 
epoch 174 iter 6960: loss = 1.0900, smooth loss = 1.0748, ce loss = 0.0839, contrastive loss = 0.9221, lr = 0.00025303387219254594
average data time = 0.0896s, average running time = 0.6150s
epoch 174 iter 6960: eval loss = 0.3037,  ccr = 0.9361,  cwr = 0.8772,  ted = 487.0000,  ned = 89.7202,  ted/w = 0.2719, 
epoch 175 iter 7000: loss = 1.1072, smooth loss = 1.0764, ce loss = 0.0866, contrastive loss = 0.9341, lr = 0.00023424959548624553
average data time = 0.0895s, average running time = 0.6153s
epoch 175 iter 7000: eval loss = 0.3084,  ccr = 0.9337,  cwr = 0.8794,  ted = 495.0000,  ned = 90.0202,  ted/w = 0.2764, 
epoch 176 iter 7040: loss = 1.0250, smooth loss = 1.0720, ce loss = 0.0598, contrastive loss = 0.9055, lr = 0.00021615549134807398
average data time = 0.0895s, average running time = 0.6157s
epoch 176 iter 7040: eval loss = 0.3200,  ccr = 0.9356,  cwr = 0.8805,  ted = 485.0000,  ned = 89.2750,  ted/w = 0.2708, 
epoch 177 iter 7080: loss = 1.0786, smooth loss = 1.0694, ce loss = 0.0835, contrastive loss = 0.9116, lr = 0.00019875707141743358
average data time = 0.0895s, average running time = 0.6160s
epoch 177 iter 7080: eval loss = 0.3099,  ccr = 0.9376,  cwr = 0.8811,  ted = 467.0000,  ned = 86.8492,  ted/w = 0.2607, 
epoch 178 iter 7120: loss = 1.0636, smooth loss = 1.0658, ce loss = 0.0721, contrastive loss = 0.9193, lr = 0.00018205963542157737
average data time = 0.0895s, average running time = 0.6162s
epoch 178 iter 7120: eval loss = 0.3074,  ccr = 0.9361,  cwr = 0.8760,  ted = 477.0000,  ned = 88.9298,  ted/w = 0.2663, 
epoch 179 iter 7160: loss = 1.0059, smooth loss = 1.0643, ce loss = 0.0503, contrastive loss = 0.9053, lr = 0.00016606826956126088
average data time = 0.0895s, average running time = 0.6165s
epoch 179 iter 7160: eval loss = 0.3061,  ccr = 0.9355,  cwr = 0.8794,  ted = 482.0000,  ned = 89.0607,  ted/w = 0.2691, 
epoch 180 iter 7200: loss = 1.0750, smooth loss = 1.0668, ce loss = 0.0762, contrastive loss = 0.9226, lr = 0.00015078784496143707
average data time = 0.0894s, average running time = 0.6169s
epoch 180 iter 7200: eval loss = 0.3101,  ccr = 0.9359,  cwr = 0.8805,  ted = 475.0000,  ned = 87.0579,  ted/w = 0.2652, 
epoch 181 iter 7240: loss = 1.0936, smooth loss = 1.0637, ce loss = 0.0900, contrastive loss = 0.9135, lr = 0.00013622301618746387
average data time = 0.0894s, average running time = 0.6173s
epoch 181 iter 7240: eval loss = 0.3106,  ccr = 0.9374,  cwr = 0.8833,  ted = 465.0000,  ned = 85.6357,  ted/w = 0.2596, 
epoch 182 iter 7280: loss = 1.1245, smooth loss = 1.0610, ce loss = 0.1004, contrastive loss = 0.9237, lr = 0.00012237821982727913
average data time = 0.0894s, average running time = 0.6176s
epoch 182 iter 7280: eval loss = 0.3062,  ccr = 0.9364,  cwr = 0.8822,  ted = 470.0000,  ned = 86.0052,  ted/w = 0.2624, 
epoch 183 iter 7320: loss = 0.9931, smooth loss = 1.0522, ce loss = 0.0453, contrastive loss = 0.9024, lr = 0.00010925767313997105
average data time = 0.0894s, average running time = 0.6179s
epoch 183 iter 7320: eval loss = 0.3121,  ccr = 0.9361,  cwr = 0.8794,  ted = 480.0000,  ned = 87.5421,  ted/w = 0.2680, 
epoch 184 iter 7360: loss = 1.0615, smooth loss = 1.0660, ce loss = 0.0658, contrastive loss = 0.9298, lr = 9.686537277116215e-05
average data time = 0.0894s, average running time = 0.6182s
epoch 184 iter 7360: eval loss = 0.3134,  ccr = 0.9354,  cwr = 0.8800,  ted = 488.0000,  ned = 88.5480,  ted/w = 0.2725, 
epoch 185 iter 7400: loss = 1.0178, smooth loss = 1.0603, ce loss = 0.0477, contrastive loss = 0.9224, lr = 8.520509353559239e-05
average data time = 0.0894s, average running time = 0.6186s
epoch 185 iter 7400: eval loss = 0.3169,  ccr = 0.9356,  cwr = 0.8816,  ted = 476.0000,  ned = 87.4524,  ted/w = 0.2658, 
epoch 186 iter 7440: loss = 1.1241, smooth loss = 1.0520, ce loss = 0.0987, contrastive loss = 0.9266, lr = 7.428038726727158e-05
average data time = 0.0893s, average running time = 0.6189s
epoch 186 iter 7440: eval loss = 0.3164,  ccr = 0.9360,  cwr = 0.8833,  ted = 476.0000,  ned = 88.0480,  ted/w = 0.2658, 
epoch 187 iter 7480: loss = 1.0813, smooth loss = 1.0563, ce loss = 0.0803, contrastive loss = 0.9206, lr = 6.409458173756002e-05
average data time = 0.0893s, average running time = 0.6191s
epoch 187 iter 7480: eval loss = 0.3111,  ccr = 0.9370,  cwr = 0.8822,  ted = 474.0000,  ned = 87.2897,  ted/w = 0.2647, 
epoch 188 iter 7520: loss = 1.0075, smooth loss = 1.0519, ce loss = 0.0472, contrastive loss = 0.9131, lr = 5.465077964149312e-05
average data time = 0.0893s, average running time = 0.6195s
epoch 188 iter 7520: eval loss = 0.3144,  ccr = 0.9365,  cwr = 0.8833,  ted = 473.0000,  ned = 87.2325,  ted/w = 0.2641, 
epoch 189 iter 7560: loss = 1.0640, smooth loss = 1.0527, ce loss = 0.0611, contrastive loss = 0.9418, lr = 4.595185765267454e-05
average data time = 0.0894s, average running time = 0.6197s
epoch 189 iter 7560: eval loss = 0.3100,  ccr = 0.9369,  cwr = 0.8822,  ted = 474.0000,  ned = 87.7758,  ted/w = 0.2647, 
epoch 190 iter 7600: loss = 1.0800, smooth loss = 1.0435, ce loss = 0.0794, contrastive loss = 0.9212, lr = 3.8000465547010077e-05
average data time = 0.0894s, average running time = 0.6199s
epoch 190 iter 7600: eval loss = 0.3125,  ccr = 0.9360,  cwr = 0.8805,  ted = 480.0000,  ned = 88.8536,  ted/w = 0.2680, 
epoch 191 iter 7640: loss = 1.0524, smooth loss = 1.0404, ce loss = 0.0590, contrastive loss = 0.9344, lr = 3.079902539556181e-05
average data time = 0.0894s, average running time = 0.6202s
epoch 191 iter 7640: eval loss = 0.3100,  ccr = 0.9371,  cwr = 0.8833,  ted = 472.0000,  ned = 87.1119,  ted/w = 0.2635, 
epoch 192 iter 7680: loss = 1.0931, smooth loss = 1.0356, ce loss = 0.0866, contrastive loss = 0.9198, lr = 2.434973082676151e-05
average data time = 0.0893s, average running time = 0.6205s
epoch 192 iter 7680: eval loss = 0.3097,  ccr = 0.9365,  cwr = 0.8827,  ted = 475.0000,  ned = 87.4940,  ted/w = 0.2652, 
epoch 193 iter 7720: loss = 1.0827, smooth loss = 1.0420, ce loss = 0.0810, contrastive loss = 0.9207, lr = 1.8654546358211455e-05
average data time = 0.0894s, average running time = 0.6207s
epoch 193 iter 7720: eval loss = 0.3121,  ccr = 0.9370,  cwr = 0.8822,  ted = 470.0000,  ned = 86.9329,  ted/w = 0.2624, 
epoch 194 iter 7760: loss = 1.0530, smooth loss = 1.0424, ce loss = 0.0667, contrastive loss = 0.9196, lr = 1.3715206798270458e-05
average data time = 0.0894s, average running time = 0.6209s
epoch 194 iter 7760: eval loss = 0.3124,  ccr = 0.9365,  cwr = 0.8822,  ted = 474.0000,  ned = 87.6190,  ted/w = 0.2647, 
epoch 195 iter 7800: loss = 1.0595, smooth loss = 1.0456, ce loss = 0.0770, contrastive loss = 0.9055, lr = 9.533216717617054e-06
average data time = 0.0895s, average running time = 0.6215s
epoch 195 iter 7800: eval loss = 0.3115,  ccr = 0.9364,  cwr = 0.8816,  ted = 475.0000,  ned = 87.4940,  ted/w = 0.2652, 
epoch 196 iter 7840: loss = 1.1001, smooth loss = 1.0467, ce loss = 0.0950, contrastive loss = 0.9101, lr = 6.1098499909421046e-06
average data time = 0.0895s, average running time = 0.6218s
epoch 196 iter 7840: eval loss = 0.3127,  ccr = 0.9364,  cwr = 0.8805,  ted = 476.0000,  ned = 87.8829,  ted/w = 0.2658, 
epoch 197 iter 7880: loss = 1.0420, smooth loss = 1.0449, ce loss = 0.0664, contrastive loss = 0.9092, lr = 3.4461494089129626e-06
average data time = 0.0897s, average running time = 0.6221s
epoch 197 iter 7880: eval loss = 0.3118,  ccr = 0.9364,  cwr = 0.8822,  ted = 476.0000,  ned = 87.7246,  ted/w = 0.2658, 
epoch 198 iter 7920: loss = 1.0278, smooth loss = 1.0454, ce loss = 0.0579, contrastive loss = 0.9120, lr = 1.5429263605307858e-06
average data time = 0.0897s, average running time = 0.6223s
epoch 198 iter 7920: eval loss = 0.3114,  ccr = 0.9365,  cwr = 0.8811,  ted = 475.0000,  ned = 87.5635,  ted/w = 0.2652, 
epoch 199 iter 7960: loss = 1.0871, smooth loss = 1.0435, ce loss = 0.0867, contrastive loss = 0.9138, lr = 4.0076058597339e-07
average data time = 0.0897s, average running time = 0.6227s
epoch 199 iter 7960: eval loss = 0.3123,  ccr = 0.9364,  cwr = 0.8811,  ted = 476.0000,  ned = 87.6885,  ted/w = 0.2658, 
Save model train-seed-42-SLPR-P-32-256-bs-128-lr-0.005-d_model-512-epoch-200-decay-cos-grad-clip-20-AdamW-wd-0.1-PA-decoder-max-len-25-sup_con-1.0-ce-2.0-temperature-0.15-warm_up-0.1_199_7960
