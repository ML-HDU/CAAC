You have chosen to seed training. This will slow down your training!
Construct dataset.
5131 training items found.
1791 valid items found.
Construct model.
Model(
  (backbone): ResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (decoder): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(size=(4, 32), mode=nearest)
        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=6068, bias=True)
)
The parameters size of model is 18.143252 MB
Construct learner.
Start training.
epoch 1 iter 40: loss = 3.4954, smooth loss = 4.2246, ce loss = 3.4954, contrastive loss = 0.0000, lr = 0.00022954798257166908
epoch 2 iter 80: loss = 3.0674, smooth loss = 3.4590, ce loss = 3.0674, contrastive loss = 0.0000, lr = 0.00031746436089163076
epoch 3 iter 120: loss = 2.9465, smooth loss = 3.1386, ce loss = 2.9465, contrastive loss = 0.0000, lr = 0.00046158434194791716
epoch 4 iter 160: loss = 2.7950, smooth loss = 2.9397, ce loss = 2.7950, contrastive loss = 0.0000, lr = 0.0006583592135001254
epoch 5 iter 200: loss = 2.5741, smooth loss = 2.8023, ce loss = 2.5741, contrastive loss = 0.0000, lr = 0.0009029437251522861
epoch 6 iter 240: loss = 2.5465, smooth loss = 2.6781, ce loss = 2.5465, contrastive loss = 0.0000, lr = 0.0011893153944980642
epoch 7 iter 280: loss = 2.4039, smooth loss = 2.5569, ce loss = 2.4039, contrastive loss = 0.0000, lr = 0.0015104228006250875
epoch 8 iter 320: loss = 2.3628, smooth loss = 2.4623, ce loss = 2.3628, contrastive loss = 0.0000, lr = 0.001858359213500126
epoch 9 iter 360: loss = 2.1702, smooth loss = 2.3559, ce loss = 2.1702, contrastive loss = 0.0000, lr = 0.0022245572839034457
epoch 10 iter 400: loss = 2.0333, smooth loss = 2.2251, ce loss = 2.0333, contrastive loss = 0.0000, lr = 0.0026
epoch 11 iter 440: loss = 1.9124, smooth loss = 2.1004, ce loss = 1.9124, contrastive loss = 0.0000, lr = 0.0029754427160965545
epoch 12 iter 480: loss = 1.8546, smooth loss = 1.9751, ce loss = 1.8546, contrastive loss = 0.0000, lr = 0.003341640786499874
epoch 13 iter 520: loss = 1.7366, smooth loss = 1.8478, ce loss = 1.7366, contrastive loss = 0.0000, lr = 0.0036895771993749123
epoch 14 iter 560: loss = 1.4183, smooth loss = 1.6931, ce loss = 1.4183, contrastive loss = 0.0000, lr = 0.0040106846055019355
epoch 15 iter 600: loss = 1.2631, smooth loss = 1.5668, ce loss = 1.2631, contrastive loss = 0.0000, lr = 0.004297056274847714
epoch 16 iter 640: loss = 1.3930, smooth loss = 1.4579, ce loss = 1.3930, contrastive loss = 0.0000, lr = 0.0045416407864998735
epoch 17 iter 680: loss = 1.1578, smooth loss = 1.3459, ce loss = 1.1578, contrastive loss = 0.0000, lr = 0.004738415658052083
epoch 18 iter 720: loss = 1.0715, smooth loss = 1.2567, ce loss = 1.0715, contrastive loss = 0.0000, lr = 0.004882535639108369
epoch 19 iter 760: loss = 1.1511, smooth loss = 1.1891, ce loss = 1.1511, contrastive loss = 0.0000, lr = 0.004970452017428331
epoch 20 iter 800: loss = 0.9632, smooth loss = 1.1053, ce loss = 0.9632, contrastive loss = 0.0000, lr = 0.005
epoch 21 iter 840: loss = 0.9569, smooth loss = 1.0449, ce loss = 0.9569, contrastive loss = 0.0000, lr = 0.004999619239414027
epoch 22 iter 880: loss = 1.0483, smooth loss = 0.9922, ce loss = 1.0483, contrastive loss = 0.0000, lr = 0.00499847707363947
epoch 23 iter 920: loss = 0.8634, smooth loss = 0.9377, ce loss = 0.8634, contrastive loss = 0.0000, lr = 0.004996573850591087
epoch 24 iter 960: loss = 0.6432, smooth loss = 0.8888, ce loss = 0.6432, contrastive loss = 0.0000, lr = 0.004993910150009058
epoch 25 iter 1000: loss = 0.7520, smooth loss = 0.8543, ce loss = 0.7520, contrastive loss = 0.0000, lr = 0.004990486783282383
epoch 26 iter 1040: loss = 0.8165, smooth loss = 0.8344, ce loss = 0.8165, contrastive loss = 0.0000, lr = 0.0049863047932017296
epoch 27 iter 1080: loss = 0.6821, smooth loss = 0.7879, ce loss = 0.6821, contrastive loss = 0.0000, lr = 0.004981365453641789
epoch 28 iter 1120: loss = 0.7315, smooth loss = 0.7633, ce loss = 0.7315, contrastive loss = 0.0000, lr = 0.004975670269173239
epoch 29 iter 1160: loss = 0.9756, smooth loss = 0.7395, ce loss = 0.9756, contrastive loss = 0.0000, lr = 0.004969220974604439
epoch 30 iter 1200: loss = 0.5221, smooth loss = 0.7207, ce loss = 0.5221, contrastive loss = 0.0000, lr = 0.00496201953445299
epoch 31 iter 1240: loss = 0.6688, smooth loss = 0.7116, ce loss = 0.6688, contrastive loss = 0.0000, lr = 0.004954068142347326
epoch 32 iter 1280: loss = 0.4208, smooth loss = 0.6833, ce loss = 0.4208, contrastive loss = 0.0000, lr = 0.004945369220358507
epoch 33 iter 1320: loss = 0.4862, smooth loss = 0.6626, ce loss = 0.4862, contrastive loss = 0.0000, lr = 0.004935925418262441
epoch 34 iter 1360: loss = 0.5359, smooth loss = 0.6437, ce loss = 0.5359, contrastive loss = 0.0000, lr = 0.004925739612732728
epoch 35 iter 1400: loss = 0.6274, smooth loss = 0.6281, ce loss = 0.6274, contrastive loss = 0.0000, lr = 0.004914814906464408
epoch 36 iter 1440: loss = 0.5254, smooth loss = 0.6219, ce loss = 0.5254, contrastive loss = 0.0000, lr = 0.004903154627228838
epoch 37 iter 1480: loss = 0.6478, smooth loss = 0.6043, ce loss = 0.6478, contrastive loss = 0.0000, lr = 0.004890762326860029
epoch 38 iter 1520: loss = 0.4512, smooth loss = 0.5875, ce loss = 0.4512, contrastive loss = 0.0000, lr = 0.004877641780172721
epoch 39 iter 1560: loss = 0.6279, smooth loss = 0.5723, ce loss = 0.6279, contrastive loss = 0.0000, lr = 0.004863796983812537
epoch 40 iter 1600: loss = 0.5662, smooth loss = 0.5629, ce loss = 0.5662, contrastive loss = 0.0000, lr = 0.004849232155038563
epoch 41 iter 1640: loss = 0.4059, smooth loss = 0.5514, ce loss = 0.4059, contrastive loss = 0.0000, lr = 0.004833951730438739
epoch 42 iter 1680: loss = 0.3914, smooth loss = 0.5533, ce loss = 0.3914, contrastive loss = 0.0000, lr = 0.004817960364578423
epoch 43 iter 1720: loss = 0.4120, smooth loss = 0.5305, ce loss = 0.4120, contrastive loss = 0.0000, lr = 0.0048012629285825665
epoch 44 iter 1760: loss = 0.5473, smooth loss = 0.5455, ce loss = 0.5473, contrastive loss = 0.0000, lr = 0.004783864508651926
epoch 45 iter 1800: loss = 0.5201, smooth loss = 0.5345, ce loss = 0.5201, contrastive loss = 0.0000, lr = 0.004765770404513755
epoch 46 iter 1840: loss = 0.4992, smooth loss = 0.5216, ce loss = 0.4992, contrastive loss = 0.0000, lr = 0.004746986127807455
epoch 47 iter 1880: loss = 0.3525, smooth loss = 0.5128, ce loss = 0.3525, contrastive loss = 0.0000, lr = 0.004727517400405678
epoch 48 iter 1920: loss = 0.4239, smooth loss = 0.4919, ce loss = 0.4239, contrastive loss = 0.0000, lr = 0.004707370152671389
epoch 49 iter 1960: loss = 0.5525, smooth loss = 0.5042, ce loss = 0.5525, contrastive loss = 0.0000, lr = 0.004686550521651418
epoch 50 iter 2000: loss = 0.5320, smooth loss = 0.4843, ce loss = 0.5320, contrastive loss = 0.0000, lr = 0.00466506484920706
epoch 51 iter 2040: loss = 0.4705, smooth loss = 0.4891, ce loss = 0.4705, contrastive loss = 0.0000, lr = 0.004642919680082274
epoch 52 iter 2080: loss = 0.6013, smooth loss = 0.4858, ce loss = 0.6013, contrastive loss = 0.0000, lr = 0.004620121759910103
epoch 53 iter 2120: loss = 0.4840, smooth loss = 0.4644, ce loss = 0.4840, contrastive loss = 0.0000, lr = 0.004596678033157881
epoch 54 iter 2160: loss = 0.3966, smooth loss = 0.4696, ce loss = 0.3966, contrastive loss = 0.0000, lr = 0.004572595641011879
epoch 55 iter 2200: loss = 0.4062, smooth loss = 0.4653, ce loss = 0.4062, contrastive loss = 0.0000, lr = 0.004547881919202037
epoch 56 iter 2240: loss = 0.4730, smooth loss = 0.4700, ce loss = 0.4730, contrastive loss = 0.0000, lr = 0.004522544395767425
epoch 57 iter 2280: loss = 0.4418, smooth loss = 0.4619, ce loss = 0.4418, contrastive loss = 0.0000, lr = 0.004496590788763132
epoch 58 iter 2320: loss = 0.4559, smooth loss = 0.4574, ce loss = 0.4559, contrastive loss = 0.0000, lr = 0.004470029003909268
epoch 59 iter 2360: loss = 0.3644, smooth loss = 0.4644, ce loss = 0.3644, contrastive loss = 0.0000, lr = 0.004442867132182813
epoch 60 iter 2400: loss = 0.4845, smooth loss = 0.4593, ce loss = 0.4845, contrastive loss = 0.0000, lr = 0.004415113447353014
epoch 61 iter 2440: loss = 0.4137, smooth loss = 0.4446, ce loss = 0.4137, contrastive loss = 0.0000, lr = 0.0043867764034611284
epoch 62 iter 2480: loss = 0.3624, smooth loss = 0.4481, ce loss = 0.3624, contrastive loss = 0.0000, lr = 0.0043578646322452305
epoch 63 iter 2520: loss = 0.3895, smooth loss = 0.4235, ce loss = 0.3895, contrastive loss = 0.0000, lr = 0.00432838694051091
epoch 64 iter 2560: loss = 0.4718, smooth loss = 0.4244, ce loss = 0.4718, contrastive loss = 0.0000, lr = 0.004298352307448625
epoch 65 iter 2600: loss = 0.3669, smooth loss = 0.4207, ce loss = 0.3669, contrastive loss = 0.0000, lr = 0.004267769881898557
epoch 66 iter 2640: loss = 0.5168, smooth loss = 0.4262, ce loss = 0.5168, contrastive loss = 0.0000, lr = 0.004236648979563789
epoch 67 iter 2680: loss = 0.2799, smooth loss = 0.4106, ce loss = 0.2799, contrastive loss = 0.0000, lr = 0.0042049990801726455
epoch 68 iter 2720: loss = 0.2843, smooth loss = 0.4003, ce loss = 0.2843, contrastive loss = 0.0000, lr = 0.004172829824591082
epoch 69 iter 2760: loss = 0.2926, smooth loss = 0.4143, ce loss = 0.2926, contrastive loss = 0.0000, lr = 0.004140151011885978
epoch 70 iter 2800: loss = 0.2757, smooth loss = 0.3961, ce loss = 0.2757, contrastive loss = 0.0000, lr = 0.004106972596340251
epoch 71 iter 2840: loss = 0.4352, smooth loss = 0.3990, ce loss = 0.4352, contrastive loss = 0.0000, lr = 0.004073304684420683
epoch 72 iter 2880: loss = 0.2980, smooth loss = 0.3846, ce loss = 0.2980, contrastive loss = 0.0000, lr = 0.004039157531699393
epoch 73 iter 2920: loss = 0.3934, smooth loss = 0.3915, ce loss = 0.3934, contrastive loss = 0.0000, lr = 0.00400454153972989
epoch 74 iter 2960: loss = 0.3296, smooth loss = 0.3872, ce loss = 0.3296, contrastive loss = 0.0000, lr = 0.00396946725287866
epoch 75 iter 3000: loss = 0.3082, smooth loss = 0.3882, ce loss = 0.3082, contrastive loss = 0.0000, lr = 0.003933945355113252
epoch 76 iter 3040: loss = 0.3966, smooth loss = 0.3780, ce loss = 0.3966, contrastive loss = 0.0000, lr = 0.0038979866667478323
epoch 77 iter 3080: loss = 0.2867, smooth loss = 0.3679, ce loss = 0.2867, contrastive loss = 0.0000, lr = 0.003861602141147218
epoch 78 iter 3120: loss = 0.3258, smooth loss = 0.3738, ce loss = 0.3258, contrastive loss = 0.0000, lr = 0.00382480286139037
epoch 79 iter 3160: loss = 0.2421, smooth loss = 0.3725, ce loss = 0.2421, contrastive loss = 0.0000, lr = 0.0037876000368943868
epoch 80 iter 3200: loss = 0.4730, smooth loss = 0.3837, ce loss = 0.4730, contrastive loss = 0.0000, lr = 0.003750005
epoch 81 iter 3240: loss = 0.4053, smooth loss = 0.3798, ce loss = 0.4053, contrastive loss = 0.0000, lr = 0.0037120292025196408
epoch 82 iter 3280: loss = 0.3255, smooth loss = 0.3643, ce loss = 0.3255, contrastive loss = 0.0000, lr = 0.003673684212249099
epoch 83 iter 3320: loss = 0.2744, smooth loss = 0.3431, ce loss = 0.2744, contrastive loss = 0.0000, lr = 0.0036349817094438693
epoch 84 iter 3360: loss = 0.2609, smooth loss = 0.3450, ce loss = 0.2609, contrastive loss = 0.0000, lr = 0.0035959334832612257
epoch 85 iter 3400: loss = 0.3462, smooth loss = 0.3432, ce loss = 0.3462, contrastive loss = 0.0000, lr = 0.0035565514281691315
epoch 86 iter 3440: loss = 0.2456, smooth loss = 0.3354, ce loss = 0.2456, contrastive loss = 0.0000, lr = 0.00351684754032307
epoch 87 iter 3480: loss = 0.3194, smooth loss = 0.3440, ce loss = 0.3194, contrastive loss = 0.0000, lr = 0.003476833913911899
epoch 88 iter 3520: loss = 0.3038, smooth loss = 0.3462, ce loss = 0.3038, contrastive loss = 0.0000, lr = 0.0034365227374738463
epoch 89 iter 3560: loss = 0.2239, smooth loss = 0.3418, ce loss = 0.2239, contrastive loss = 0.0000, lr = 0.0033959262901837547
epoch 90 iter 3600: loss = 0.2859, smooth loss = 0.3416, ce loss = 0.2859, contrastive loss = 0.0000, lr = 0.003355056938112739
epoch 91 iter 3640: loss = 0.3752, smooth loss = 0.3296, ce loss = 0.3752, contrastive loss = 0.0000, lr = 0.0033139271304613474
epoch 92 iter 3680: loss = 0.4142, smooth loss = 0.3239, ce loss = 0.4142, contrastive loss = 0.0000, lr = 0.003272549395767425
epoch 93 iter 3720: loss = 0.2994, smooth loss = 0.3167, ce loss = 0.2994, contrastive loss = 0.0000, lr = 0.0032309363380897947
epoch 94 iter 3760: loss = 0.3909, smooth loss = 0.3194, ce loss = 0.3909, contrastive loss = 0.0000, lr = 0.0031891006331689394
epoch 95 iter 3800: loss = 0.3904, smooth loss = 0.3176, ce loss = 0.3904, contrastive loss = 0.0000, lr = 0.003147055024565851
epoch 96 iter 3840: loss = 0.3020, smooth loss = 0.3159, ce loss = 0.3020, contrastive loss = 0.0000, lr = 0.003104812319780213
epoch 97 iter 3880: loss = 0.3245, smooth loss = 0.3217, ce loss = 0.3245, contrastive loss = 0.0000, lr = 0.0030623853863491193
epoch 98 iter 3920: loss = 0.2451, smooth loss = 0.3096, ce loss = 0.2451, contrastive loss = 0.0000, lr = 0.0030197871479274896
epoch 99 iter 3960: loss = 0.3381, smooth loss = 0.3108, ce loss = 0.3381, contrastive loss = 0.0000, lr = 0.0029770305803514083
epoch 100 iter 4000: loss = 0.2521, smooth loss = 0.3095, ce loss = 0.2521, contrastive loss = 0.0000, lr = 0.0029341287076855497
Save model train-seed-42-SLPR-P-32-256-bs-128-lr-0.005-d_model-512-epoch-200-decay-cos-grad-clip-20-AdamW-wd-0.1-PA-decoder-max-len-25-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.1_100_4000
epoch 101 iter 4040: loss = 0.3314, smooth loss = 0.3145, ce loss = 0.3314, contrastive loss = 0.0000, lr = 0.002891094598255927
average data time = 0.0629s, average running time = 0.3322s
epoch 101 iter 4040: eval loss = 0.3770,  ccr = 0.8985,  cwr = 0.8146,  ted = 795.0000,  ned = 151.1794,  ted/w = 0.4439, 
Better model found at epoch 101, iter 4040 with accuracy value: 0.8146.
epoch 102 iter 4080: loss = 0.2826, smooth loss = 0.2971, ce loss = 0.2826, contrastive loss = 0.0000, lr = 0.002847941360669154
average data time = 0.0628s, average running time = 0.3335s
epoch 102 iter 4080: eval loss = 0.3677,  ccr = 0.9062,  cwr = 0.8197,  ted = 740.0000,  ned = 141.2012,  ted/w = 0.4132, 
Better model found at epoch 102, iter 4080 with accuracy value: 0.8197.
epoch 103 iter 4120: loss = 0.3309, smooth loss = 0.3016, ce loss = 0.3309, contrastive loss = 0.0000, lr = 0.0028046821398194344
average data time = 0.0628s, average running time = 0.3349s
epoch 103 iter 4120: eval loss = 0.3905,  ccr = 0.8973,  cwr = 0.8018,  ted = 774.0000,  ned = 152.3659,  ted/w = 0.4322, 
epoch 104 iter 4160: loss = 0.3051, smooth loss = 0.2870, ce loss = 0.3051, contrastive loss = 0.0000, lr = 0.002761330112884501
average data time = 0.0627s, average running time = 0.3359s
epoch 104 iter 4160: eval loss = 0.3613,  ccr = 0.9033,  cwr = 0.8185,  ted = 737.0000,  ned = 140.3286,  ted/w = 0.4115, 
epoch 105 iter 4200: loss = 0.3314, smooth loss = 0.2814, ce loss = 0.3314, contrastive loss = 0.0000, lr = 0.0027178984853117186
average data time = 0.0627s, average running time = 0.3369s
epoch 105 iter 4200: eval loss = 0.3723,  ccr = 0.9061,  cwr = 0.8230,  ted = 704.0000,  ned = 133.5016,  ted/w = 0.3931, 
Better model found at epoch 105, iter 4200 with accuracy value: 0.8230.
epoch 106 iter 4240: loss = 0.2988, smooth loss = 0.2868, ce loss = 0.2988, contrastive loss = 0.0000, lr = 0.0026744004867955756
average data time = 0.0627s, average running time = 0.3382s
epoch 106 iter 4240: eval loss = 0.3939,  ccr = 0.9024,  cwr = 0.8174,  ted = 767.0000,  ned = 146.5913,  ted/w = 0.4283, 
epoch 107 iter 4280: loss = 0.2233, smooth loss = 0.2878, ce loss = 0.2233, contrastive loss = 0.0000, lr = 0.0026308493672477975
average data time = 0.0627s, average running time = 0.3390s
epoch 107 iter 4280: eval loss = 0.3390,  ccr = 0.9099,  cwr = 0.8331,  ted = 704.0000,  ned = 136.7095,  ted/w = 0.3931, 
Better model found at epoch 107, iter 4280 with accuracy value: 0.8331.
epoch 108 iter 4320: loss = 0.2274, smooth loss = 0.2875, ce loss = 0.2274, contrastive loss = 0.0000, lr = 0.002587258392761286
average data time = 0.0626s, average running time = 0.3402s
epoch 108 iter 4320: eval loss = 0.3712,  ccr = 0.9079,  cwr = 0.8208,  ted = 696.0000,  ned = 132.6540,  ted/w = 0.3886, 
epoch 109 iter 4360: loss = 0.1321, smooth loss = 0.2841, ce loss = 0.1321, contrastive loss = 0.0000, lr = 0.002543640841569145
average data time = 0.0626s, average running time = 0.3411s
epoch 109 iter 4360: eval loss = 0.3728,  ccr = 0.9018,  cwr = 0.8197,  ted = 738.0000,  ned = 139.6742,  ted/w = 0.4121, 
epoch 110 iter 4400: loss = 0.2713, smooth loss = 0.2779, ce loss = 0.2713, contrastive loss = 0.0000, lr = 0.00250001
average data time = 0.0625s, average running time = 0.3419s
epoch 110 iter 4400: eval loss = 0.3612,  ccr = 0.9080,  cwr = 0.8252,  ted = 697.0000,  ned = 131.9647,  ted/w = 0.3892, 
epoch 111 iter 4440: loss = 0.1882, smooth loss = 0.2778, ce loss = 0.1882, contrastive loss = 0.0000, lr = 0.0024563791584308555
average data time = 0.0625s, average running time = 0.3426s
epoch 111 iter 4440: eval loss = 0.3547,  ccr = 0.9128,  cwr = 0.8314,  ted = 684.0000,  ned = 129.5278,  ted/w = 0.3819, 
epoch 112 iter 4480: loss = 0.2739, smooth loss = 0.2634, ce loss = 0.2739, contrastive loss = 0.0000, lr = 0.0024127616072387153
average data time = 0.0625s, average running time = 0.3434s
epoch 112 iter 4480: eval loss = 0.3499,  ccr = 0.9063,  cwr = 0.8303,  ted = 697.0000,  ned = 129.6913,  ted/w = 0.3892, 
epoch 113 iter 4520: loss = 0.1549, smooth loss = 0.2648, ce loss = 0.1549, contrastive loss = 0.0000, lr = 0.002369170632752203
average data time = 0.0625s, average running time = 0.3442s
epoch 113 iter 4520: eval loss = 0.3487,  ccr = 0.9080,  cwr = 0.8264,  ted = 679.0000,  ned = 131.4250,  ted/w = 0.3791, 
epoch 114 iter 4560: loss = 0.2933, smooth loss = 0.2706, ce loss = 0.2933, contrastive loss = 0.0000, lr = 0.002325619513204424
average data time = 0.0625s, average running time = 0.3450s
epoch 114 iter 4560: eval loss = 0.3555,  ccr = 0.9158,  cwr = 0.8425,  ted = 625.0000,  ned = 118.2690,  ted/w = 0.3490, 
Better model found at epoch 114, iter 4560 with accuracy value: 0.8425.
epoch 115 iter 4600: loss = 0.2028, smooth loss = 0.2539, ce loss = 0.2028, contrastive loss = 0.0000, lr = 0.002282121514688282
average data time = 0.0625s, average running time = 0.3461s
epoch 115 iter 4600: eval loss = 0.3497,  ccr = 0.9116,  cwr = 0.8297,  ted = 683.0000,  ned = 129.2325,  ted/w = 0.3814, 
epoch 116 iter 4640: loss = 0.2289, smooth loss = 0.2541, ce loss = 0.2289, contrastive loss = 0.0000, lr = 0.0022386898871154994
average data time = 0.0626s, average running time = 0.3469s
epoch 116 iter 4640: eval loss = 0.3550,  ccr = 0.9127,  cwr = 0.8241,  ted = 690.0000,  ned = 133.6722,  ted/w = 0.3853, 
epoch 117 iter 4680: loss = 0.2760, smooth loss = 0.2572, ce loss = 0.2760, contrastive loss = 0.0000, lr = 0.0021953378601805656
average data time = 0.0626s, average running time = 0.3475s
epoch 117 iter 4680: eval loss = 0.3465,  ccr = 0.9116,  cwr = 0.8291,  ted = 685.0000,  ned = 131.0849,  ted/w = 0.3825, 
epoch 118 iter 4720: loss = 0.2762, smooth loss = 0.2544, ce loss = 0.2762, contrastive loss = 0.0000, lr = 0.0021520786393308465
average data time = 0.0626s, average running time = 0.3480s
epoch 118 iter 4720: eval loss = 0.3702,  ccr = 0.9100,  cwr = 0.8319,  ted = 682.0000,  ned = 128.4627,  ted/w = 0.3808, 
epoch 119 iter 4760: loss = 0.1614, smooth loss = 0.2432, ce loss = 0.1614, contrastive loss = 0.0000, lr = 0.0021089254017440727
average data time = 0.0625s, average running time = 0.3487s
epoch 119 iter 4760: eval loss = 0.3776,  ccr = 0.9072,  cwr = 0.8275,  ted = 710.0000,  ned = 134.3921,  ted/w = 0.3964, 
epoch 120 iter 4800: loss = 0.2171, smooth loss = 0.2479, ce loss = 0.2171, contrastive loss = 0.0000, lr = 0.0020658912923144507
average data time = 0.0624s, average running time = 0.3494s
epoch 120 iter 4800: eval loss = 0.3726,  ccr = 0.9082,  cwr = 0.8197,  ted = 708.0000,  ned = 137.6075,  ted/w = 0.3953, 
epoch 121 iter 4840: loss = 0.2138, smooth loss = 0.2512, ce loss = 0.2138, contrastive loss = 0.0000, lr = 0.0020229894196485917
average data time = 0.0624s, average running time = 0.3502s
epoch 121 iter 4840: eval loss = 0.3401,  ccr = 0.9171,  cwr = 0.8381,  ted = 643.0000,  ned = 123.0675,  ted/w = 0.3590, 
epoch 122 iter 4880: loss = 0.3415, smooth loss = 0.2392, ce loss = 0.3415, contrastive loss = 0.0000, lr = 0.0019802328520725104
average data time = 0.0624s, average running time = 0.3509s
epoch 122 iter 4880: eval loss = 0.3620,  ccr = 0.9113,  cwr = 0.8381,  ted = 671.0000,  ned = 126.1056,  ted/w = 0.3747, 
epoch 123 iter 4920: loss = 0.2914, smooth loss = 0.2279, ce loss = 0.2914, contrastive loss = 0.0000, lr = 0.0019376346136508816
average data time = 0.0625s, average running time = 0.3516s
epoch 123 iter 4920: eval loss = 0.3514,  ccr = 0.9108,  cwr = 0.8347,  ted = 668.0000,  ned = 128.3810,  ted/w = 0.3730, 
epoch 124 iter 4960: loss = 0.2749, smooth loss = 0.2339, ce loss = 0.2749, contrastive loss = 0.0000, lr = 0.0018952076802197873
average data time = 0.0625s, average running time = 0.3520s
epoch 124 iter 4960: eval loss = 0.3638,  ccr = 0.9134,  cwr = 0.8358,  ted = 676.0000,  ned = 127.7135,  ted/w = 0.3774, 
epoch 125 iter 5000: loss = 0.2795, smooth loss = 0.2336, ce loss = 0.2795, contrastive loss = 0.0000, lr = 0.0018529649754341492
average data time = 0.0625s, average running time = 0.3526s
epoch 125 iter 5000: eval loss = 0.3268,  ccr = 0.9173,  cwr = 0.8465,  ted = 629.0000,  ned = 115.6786,  ted/w = 0.3512, 
Better model found at epoch 125, iter 5000 with accuracy value: 0.8465.
epoch 126 iter 5040: loss = 0.2459, smooth loss = 0.2315, ce loss = 0.2459, contrastive loss = 0.0000, lr = 0.0018109193668310606
average data time = 0.0624s, average running time = 0.3535s
epoch 126 iter 5040: eval loss = 0.3472,  ccr = 0.9166,  cwr = 0.8403,  ted = 646.0000,  ned = 122.2948,  ted/w = 0.3607, 
epoch 127 iter 5080: loss = 0.1021, smooth loss = 0.2273, ce loss = 0.1021, contrastive loss = 0.0000, lr = 0.0017690836619102058
average data time = 0.0624s, average running time = 0.3542s
epoch 127 iter 5080: eval loss = 0.3397,  ccr = 0.9151,  cwr = 0.8336,  ted = 658.0000,  ned = 125.1226,  ted/w = 0.3674, 
epoch 128 iter 5120: loss = 0.2557, smooth loss = 0.2128, ce loss = 0.2557, contrastive loss = 0.0000, lr = 0.0017274706042325755
average data time = 0.0623s, average running time = 0.3548s
epoch 128 iter 5120: eval loss = 0.3430,  ccr = 0.9135,  cwr = 0.8403,  ted = 668.0000,  ned = 124.1488,  ted/w = 0.3730, 
epoch 129 iter 5160: loss = 0.2247, smooth loss = 0.2148, ce loss = 0.2247, contrastive loss = 0.0000, lr = 0.0016860928695386535
average data time = 0.0623s, average running time = 0.3555s
epoch 129 iter 5160: eval loss = 0.3362,  ccr = 0.9157,  cwr = 0.8431,  ted = 634.0000,  ned = 119.2187,  ted/w = 0.3540, 
epoch 130 iter 5200: loss = 0.1845, smooth loss = 0.2076, ce loss = 0.1845, contrastive loss = 0.0000, lr = 0.0016449630618872617
average data time = 0.0623s, average running time = 0.3560s
epoch 130 iter 5200: eval loss = 0.3661,  ccr = 0.9168,  cwr = 0.8386,  ted = 640.0000,  ned = 123.8734,  ted/w = 0.3573, 
epoch 131 iter 5240: loss = 0.1627, smooth loss = 0.2090, ce loss = 0.1627, contrastive loss = 0.0000, lr = 0.0016040937098162449
average data time = 0.0622s, average running time = 0.3566s
epoch 131 iter 5240: eval loss = 0.3570,  ccr = 0.9173,  cwr = 0.8414,  ted = 639.0000,  ned = 124.0623,  ted/w = 0.3568, 
epoch 132 iter 5280: loss = 0.2109, smooth loss = 0.2194, ce loss = 0.2109, contrastive loss = 0.0000, lr = 0.001563497262526154
average data time = 0.0623s, average running time = 0.3572s
epoch 132 iter 5280: eval loss = 0.3526,  ccr = 0.9200,  cwr = 0.8442,  ted = 618.0000,  ned = 120.6913,  ted/w = 0.3451, 
epoch 133 iter 5320: loss = 0.2384, smooth loss = 0.2051, ce loss = 0.2384, contrastive loss = 0.0000, lr = 0.001523186086088101
average data time = 0.0622s, average running time = 0.3577s
epoch 133 iter 5320: eval loss = 0.3560,  ccr = 0.9148,  cwr = 0.8425,  ted = 656.0000,  ned = 125.3889,  ted/w = 0.3663, 
epoch 134 iter 5360: loss = 0.2155, smooth loss = 0.1977, ce loss = 0.2155, contrastive loss = 0.0000, lr = 0.001483172459676931
average data time = 0.0622s, average running time = 0.3583s
epoch 134 iter 5360: eval loss = 0.3425,  ccr = 0.9182,  cwr = 0.8459,  ted = 607.0000,  ned = 116.1976,  ted/w = 0.3389, 
epoch 135 iter 5400: loss = 0.2874, smooth loss = 0.2016, ce loss = 0.2874, contrastive loss = 0.0000, lr = 0.0014434685718308694
average data time = 0.0622s, average running time = 0.3588s
epoch 135 iter 5400: eval loss = 0.3379,  ccr = 0.9172,  cwr = 0.8414,  ted = 629.0000,  ned = 121.2567,  ted/w = 0.3512, 
epoch 136 iter 5440: loss = 0.2205, smooth loss = 0.2007, ce loss = 0.2205, contrastive loss = 0.0000, lr = 0.0014040865167387743
average data time = 0.0622s, average running time = 0.3595s
epoch 136 iter 5440: eval loss = 0.3518,  ccr = 0.9193,  cwr = 0.8409,  ted = 633.0000,  ned = 124.1857,  ted/w = 0.3534, 
epoch 137 iter 5480: loss = 0.2899, smooth loss = 0.1952, ce loss = 0.2899, contrastive loss = 0.0000, lr = 0.0013650382905561306
average data time = 0.0623s, average running time = 0.3601s
epoch 137 iter 5480: eval loss = 0.3473,  ccr = 0.9182,  cwr = 0.8425,  ted = 627.0000,  ned = 121.0647,  ted/w = 0.3501, 
epoch 138 iter 5520: loss = 0.1588, smooth loss = 0.1915, ce loss = 0.1588, contrastive loss = 0.0000, lr = 0.0013263357877509017
average data time = 0.0622s, average running time = 0.3606s
epoch 138 iter 5520: eval loss = 0.3533,  ccr = 0.9183,  cwr = 0.8465,  ted = 615.0000,  ned = 116.8341,  ted/w = 0.3434, 
epoch 139 iter 5560: loss = 0.2079, smooth loss = 0.1874, ce loss = 0.2079, contrastive loss = 0.0000, lr = 0.00128799079748036
average data time = 0.0622s, average running time = 0.3612s
epoch 139 iter 5560: eval loss = 0.3579,  ccr = 0.9181,  cwr = 0.8425,  ted = 645.0000,  ned = 120.2738,  ted/w = 0.3601, 
epoch 140 iter 5600: loss = 0.1528, smooth loss = 0.1854, ce loss = 0.1528, contrastive loss = 0.0000, lr = 0.0012500150000000008
average data time = 0.0622s, average running time = 0.3617s
epoch 140 iter 5600: eval loss = 0.3184,  ccr = 0.9264,  cwr = 0.8504,  ted = 571.0000,  ned = 110.9024,  ted/w = 0.3188, 
Better model found at epoch 140, iter 5600 with accuracy value: 0.8504.
epoch 141 iter 5640: loss = 0.1751, smooth loss = 0.1874, ce loss = 0.1751, contrastive loss = 0.0000, lr = 0.0012124199631056137
average data time = 0.0622s, average running time = 0.3624s
epoch 141 iter 5640: eval loss = 0.3328,  ccr = 0.9231,  cwr = 0.8532,  ted = 581.0000,  ned = 110.2917,  ted/w = 0.3244, 
Better model found at epoch 141, iter 5640 with accuracy value: 0.8532.
epoch 142 iter 5680: loss = 0.2225, smooth loss = 0.1849, ce loss = 0.2225, contrastive loss = 0.0000, lr = 0.0011752171386096306
average data time = 0.0621s, average running time = 0.3631s
epoch 142 iter 5680: eval loss = 0.3369,  ccr = 0.9206,  cwr = 0.8504,  ted = 604.0000,  ned = 114.9183,  ted/w = 0.3372, 
epoch 143 iter 5720: loss = 0.1464, smooth loss = 0.1856, ce loss = 0.1464, contrastive loss = 0.0000, lr = 0.0011384178588527826
average data time = 0.0621s, average running time = 0.3636s
epoch 143 iter 5720: eval loss = 0.3219,  ccr = 0.9252,  cwr = 0.8565,  ted = 563.0000,  ned = 108.6667,  ted/w = 0.3143, 
Better model found at epoch 143, iter 5720 with accuracy value: 0.8565.
epoch 144 iter 5760: loss = 0.1901, smooth loss = 0.1744, ce loss = 0.1901, contrastive loss = 0.0000, lr = 0.0011020333332521681
average data time = 0.0621s, average running time = 0.3644s
epoch 144 iter 5760: eval loss = 0.3350,  ccr = 0.9205,  cwr = 0.8515,  ted = 604.0000,  ned = 115.2504,  ted/w = 0.3372, 
epoch 145 iter 5800: loss = 0.1675, smooth loss = 0.1634, ce loss = 0.1675, contrastive loss = 0.0000, lr = 0.001066074644886749
average data time = 0.0621s, average running time = 0.3649s
epoch 145 iter 5800: eval loss = 0.3425,  ccr = 0.9214,  cwr = 0.8520,  ted = 585.0000,  ned = 111.1151,  ted/w = 0.3266, 
epoch 146 iter 5840: loss = 0.1925, smooth loss = 0.1689, ce loss = 0.1925, contrastive loss = 0.0000, lr = 0.0010305527471213406
average data time = 0.0621s, average running time = 0.3655s
epoch 146 iter 5840: eval loss = 0.3358,  ccr = 0.9188,  cwr = 0.8465,  ted = 612.0000,  ned = 118.4742,  ted/w = 0.3417, 
epoch 147 iter 5880: loss = 0.1538, smooth loss = 0.1669, ce loss = 0.1538, contrastive loss = 0.0000, lr = 0.0009954784602701108
average data time = 0.0620s, average running time = 0.3660s
epoch 147 iter 5880: eval loss = 0.3192,  ccr = 0.9259,  cwr = 0.8554,  ted = 559.0000,  ned = 103.8000,  ted/w = 0.3121, 
epoch 148 iter 5920: loss = 0.1691, smooth loss = 0.1597, ce loss = 0.1691, contrastive loss = 0.0000, lr = 0.0009608624683006075
average data time = 0.0620s, average running time = 0.3665s
epoch 148 iter 5920: eval loss = 0.3379,  ccr = 0.9202,  cwr = 0.8515,  ted = 582.0000,  ned = 109.7810,  ted/w = 0.3250, 
epoch 149 iter 5960: loss = 0.1825, smooth loss = 0.1557, ce loss = 0.1825, contrastive loss = 0.0000, lr = 0.0009267153155793173
average data time = 0.0621s, average running time = 0.3669s
epoch 149 iter 5960: eval loss = 0.3177,  ccr = 0.9244,  cwr = 0.8532,  ted = 568.0000,  ned = 108.5095,  ted/w = 0.3171, 
epoch 150 iter 6000: loss = 0.2073, smooth loss = 0.1550, ce loss = 0.2073, contrastive loss = 0.0000, lr = 0.0008930474036597485
average data time = 0.0620s, average running time = 0.3674s
epoch 150 iter 6000: eval loss = 0.3262,  ccr = 0.9277,  cwr = 0.8565,  ted = 553.0000,  ned = 105.1484,  ted/w = 0.3088, 
epoch 151 iter 6040: loss = 0.1794, smooth loss = 0.1618, ce loss = 0.1794, contrastive loss = 0.0000, lr = 0.000859868988114022
average data time = 0.0619s, average running time = 0.3680s
epoch 151 iter 6040: eval loss = 0.3415,  ccr = 0.9216,  cwr = 0.8470,  ted = 600.0000,  ned = 114.4131,  ted/w = 0.3350, 
epoch 152 iter 6080: loss = 0.1940, smooth loss = 0.1515, ce loss = 0.1940, contrastive loss = 0.0000, lr = 0.0008271901754089188
average data time = 0.0620s, average running time = 0.3685s
epoch 152 iter 6080: eval loss = 0.3338,  ccr = 0.9272,  cwr = 0.8576,  ted = 558.0000,  ned = 106.8179,  ted/w = 0.3116, 
Better model found at epoch 152, iter 6080 with accuracy value: 0.8576.
epoch 153 iter 6120: loss = 0.1573, smooth loss = 0.1485, ce loss = 0.1573, contrastive loss = 0.0000, lr = 0.0007950209198273547
average data time = 0.0620s, average running time = 0.3692s
epoch 153 iter 6120: eval loss = 0.3234,  ccr = 0.9225,  cwr = 0.8520,  ted = 599.0000,  ned = 114.6437,  ted/w = 0.3345, 
epoch 154 iter 6160: loss = 0.1129, smooth loss = 0.1474, ce loss = 0.1129, contrastive loss = 0.0000, lr = 0.0007633710204362112
average data time = 0.0620s, average running time = 0.3697s
epoch 154 iter 6160: eval loss = 0.3411,  ccr = 0.9259,  cwr = 0.8526,  ted = 566.0000,  ned = 111.5829,  ted/w = 0.3160, 
epoch 155 iter 6200: loss = 0.1209, smooth loss = 0.1409, ce loss = 0.1209, contrastive loss = 0.0000, lr = 0.0007322501181014433
average data time = 0.0620s, average running time = 0.3700s
epoch 155 iter 6200: eval loss = 0.3373,  ccr = 0.9241,  cwr = 0.8537,  ted = 577.0000,  ned = 111.7806,  ted/w = 0.3222, 
epoch 156 iter 6240: loss = 0.1019, smooth loss = 0.1458, ce loss = 0.1019, contrastive loss = 0.0000, lr = 0.000701667692551376
average data time = 0.0620s, average running time = 0.3705s
epoch 156 iter 6240: eval loss = 0.3345,  ccr = 0.9255,  cwr = 0.8599,  ted = 562.0000,  ned = 107.3877,  ted/w = 0.3138, 
Better model found at epoch 156, iter 6240 with accuracy value: 0.8599.
epoch 157 iter 6280: loss = 0.1290, smooth loss = 0.1421, ce loss = 0.1290, contrastive loss = 0.0000, lr = 0.00067163305948909
average data time = 0.0620s, average running time = 0.3712s
epoch 157 iter 6280: eval loss = 0.3408,  ccr = 0.9230,  cwr = 0.8548,  ted = 571.0000,  ned = 110.0671,  ted/w = 0.3188, 
epoch 158 iter 6320: loss = 0.1097, smooth loss = 0.1306, ce loss = 0.1097, contrastive loss = 0.0000, lr = 0.0006421553677547689
average data time = 0.0620s, average running time = 0.3716s
epoch 158 iter 6320: eval loss = 0.3526,  ccr = 0.9233,  cwr = 0.8565,  ted = 572.0000,  ned = 110.5571,  ted/w = 0.3194, 
epoch 159 iter 6360: loss = 0.1458, smooth loss = 0.1334, ce loss = 0.1458, contrastive loss = 0.0000, lr = 0.0006132435965388722
average data time = 0.0620s, average running time = 0.3721s
epoch 159 iter 6360: eval loss = 0.3306,  ccr = 0.9233,  cwr = 0.8571,  ted = 578.0000,  ned = 111.1861,  ted/w = 0.3227, 
epoch 160 iter 6400: loss = 0.1656, smooth loss = 0.1398, ce loss = 0.1656, contrastive loss = 0.0000, lr = 0.0005849065526469865
average data time = 0.0620s, average running time = 0.3726s
epoch 160 iter 6400: eval loss = 0.3396,  ccr = 0.9230,  cwr = 0.8610,  ted = 571.0000,  ned = 107.4976,  ted/w = 0.3188, 
Better model found at epoch 160, iter 6400 with accuracy value: 0.8610.
epoch 161 iter 6440: loss = 0.0861, smooth loss = 0.1385, ce loss = 0.0861, contrastive loss = 0.0000, lr = 0.0005571528678171874
average data time = 0.0619s, average running time = 0.3732s
epoch 161 iter 6440: eval loss = 0.3370,  ccr = 0.9248,  cwr = 0.8537,  ted = 578.0000,  ned = 109.6000,  ted/w = 0.3227, 
epoch 162 iter 6480: loss = 0.1500, smooth loss = 0.1333, ce loss = 0.1500, contrastive loss = 0.0000, lr = 0.0005299909960907313
average data time = 0.0619s, average running time = 0.3736s
epoch 162 iter 6480: eval loss = 0.3381,  ccr = 0.9270,  cwr = 0.8610,  ted = 547.0000,  ned = 104.2786,  ted/w = 0.3054, 
epoch 163 iter 6520: loss = 0.2148, smooth loss = 0.1314, ce loss = 0.2148, contrastive loss = 0.0000, lr = 0.0005034292112368689
average data time = 0.0619s, average running time = 0.3742s
epoch 163 iter 6520: eval loss = 0.3407,  ccr = 0.9267,  cwr = 0.8582,  ted = 559.0000,  ned = 105.1123,  ted/w = 0.3121, 
epoch 164 iter 6560: loss = 0.1076, smooth loss = 0.1266, ce loss = 0.1076, contrastive loss = 0.0000, lr = 0.0004774756042325754
average data time = 0.0619s, average running time = 0.3745s
epoch 164 iter 6560: eval loss = 0.3417,  ccr = 0.9281,  cwr = 0.8599,  ted = 553.0000,  ned = 104.7687,  ted/w = 0.3088, 
epoch 165 iter 6600: loss = 0.1460, smooth loss = 0.1249, ce loss = 0.1460, contrastive loss = 0.0000, lr = 0.00045213808079796314
average data time = 0.0619s, average running time = 0.3749s
epoch 165 iter 6600: eval loss = 0.3428,  ccr = 0.9265,  cwr = 0.8599,  ted = 559.0000,  ned = 104.8722,  ted/w = 0.3121, 
epoch 166 iter 6640: loss = 0.0560, smooth loss = 0.1236, ce loss = 0.0560, contrastive loss = 0.0000, lr = 0.0004274243589881215
average data time = 0.0619s, average running time = 0.3753s
epoch 166 iter 6640: eval loss = 0.3318,  ccr = 0.9243,  cwr = 0.8565,  ted = 575.0000,  ned = 109.3964,  ted/w = 0.3210, 
epoch 167 iter 6680: loss = 0.1317, smooth loss = 0.1212, ce loss = 0.1317, contrastive loss = 0.0000, lr = 0.0004033419668421196
average data time = 0.0619s, average running time = 0.3758s
epoch 167 iter 6680: eval loss = 0.3384,  ccr = 0.9269,  cwr = 0.8632,  ted = 549.0000,  ned = 104.3925,  ted/w = 0.3065, 
Better model found at epoch 167, iter 6680 with accuracy value: 0.8632.
epoch 168 iter 6720: loss = 0.1129, smooth loss = 0.1154, ce loss = 0.1129, contrastive loss = 0.0000, lr = 0.0003798982400898967
average data time = 0.0619s, average running time = 0.3764s
epoch 168 iter 6720: eval loss = 0.3548,  ccr = 0.9240,  cwr = 0.8599,  ted = 561.0000,  ned = 108.0337,  ted/w = 0.3132, 
epoch 169 iter 6760: loss = 0.1284, smooth loss = 0.1143, ce loss = 0.1284, contrastive loss = 0.0000, lr = 0.0003571003199177265
average data time = 0.0619s, average running time = 0.3768s
epoch 169 iter 6760: eval loss = 0.3528,  ccr = 0.9236,  cwr = 0.8554,  ted = 576.0000,  ned = 111.8107,  ted/w = 0.3216, 
epoch 170 iter 6800: loss = 0.0663, smooth loss = 0.1117, ce loss = 0.0663, contrastive loss = 0.0000, lr = 0.0003349551507929411
average data time = 0.0619s, average running time = 0.3772s
epoch 170 iter 6800: eval loss = 0.3400,  ccr = 0.9252,  cwr = 0.8593,  ted = 565.0000,  ned = 107.7929,  ted/w = 0.3155, 
epoch 171 iter 6840: loss = 0.0773, smooth loss = 0.1155, ce loss = 0.0773, contrastive loss = 0.0000, lr = 0.000313469478348582
average data time = 0.0619s, average running time = 0.3776s
epoch 171 iter 6840: eval loss = 0.3383,  ccr = 0.9260,  cwr = 0.8621,  ted = 554.0000,  ned = 105.6123,  ted/w = 0.3093, 
epoch 172 iter 6880: loss = 0.1209, smooth loss = 0.1132, ce loss = 0.1209, contrastive loss = 0.0000, lr = 0.0002926498473286111
average data time = 0.0618s, average running time = 0.3780s
epoch 172 iter 6880: eval loss = 0.3469,  ccr = 0.9250,  cwr = 0.8582,  ted = 572.0000,  ned = 108.3905,  ted/w = 0.3194, 
epoch 173 iter 6920: loss = 0.2095, smooth loss = 0.1094, ce loss = 0.2095, contrastive loss = 0.0000, lr = 0.0002725025995943224
average data time = 0.0618s, average running time = 0.3784s
epoch 173 iter 6920: eval loss = 0.3460,  ccr = 0.9245,  cwr = 0.8604,  ted = 582.0000,  ned = 109.0036,  ted/w = 0.3250, 
epoch 174 iter 6960: loss = 0.1286, smooth loss = 0.1051, ce loss = 0.1286, contrastive loss = 0.0000, lr = 0.00025303387219254594
average data time = 0.0618s, average running time = 0.3787s
epoch 174 iter 6960: eval loss = 0.3421,  ccr = 0.9269,  cwr = 0.8615,  ted = 548.0000,  ned = 104.0393,  ted/w = 0.3060, 
epoch 175 iter 7000: loss = 0.1021, smooth loss = 0.1065, ce loss = 0.1021, contrastive loss = 0.0000, lr = 0.00023424959548624553
average data time = 0.0618s, average running time = 0.3791s
epoch 175 iter 7000: eval loss = 0.3340,  ccr = 0.9278,  cwr = 0.8593,  ted = 551.0000,  ned = 105.2940,  ted/w = 0.3076, 
epoch 176 iter 7040: loss = 0.1765, smooth loss = 0.1044, ce loss = 0.1765, contrastive loss = 0.0000, lr = 0.00021615549134807398
average data time = 0.0618s, average running time = 0.3795s
epoch 176 iter 7040: eval loss = 0.3405,  ccr = 0.9292,  cwr = 0.8615,  ted = 542.0000,  ned = 105.2206,  ted/w = 0.3026, 
epoch 177 iter 7080: loss = 0.0911, smooth loss = 0.1003, ce loss = 0.0911, contrastive loss = 0.0000, lr = 0.00019875707141743358
average data time = 0.0618s, average running time = 0.3798s
epoch 177 iter 7080: eval loss = 0.3331,  ccr = 0.9292,  cwr = 0.8660,  ted = 538.0000,  ned = 103.1567,  ted/w = 0.3004, 
Better model found at epoch 177, iter 7080 with accuracy value: 0.8660.
epoch 178 iter 7120: loss = 0.0832, smooth loss = 0.1020, ce loss = 0.0832, contrastive loss = 0.0000, lr = 0.00018205963542157737
average data time = 0.0618s, average running time = 0.3803s
epoch 178 iter 7120: eval loss = 0.3386,  ccr = 0.9286,  cwr = 0.8643,  ted = 556.0000,  ned = 106.7159,  ted/w = 0.3104, 
epoch 179 iter 7160: loss = 0.0649, smooth loss = 0.1036, ce loss = 0.0649, contrastive loss = 0.0000, lr = 0.00016606826956126088
average data time = 0.0618s, average running time = 0.3807s
epoch 179 iter 7160: eval loss = 0.3390,  ccr = 0.9262,  cwr = 0.8654,  ted = 568.0000,  ned = 108.7123,  ted/w = 0.3171, 
epoch 180 iter 7200: loss = 0.1483, smooth loss = 0.1089, ce loss = 0.1483, contrastive loss = 0.0000, lr = 0.00015078784496143707
average data time = 0.0617s, average running time = 0.3810s
epoch 180 iter 7200: eval loss = 0.3443,  ccr = 0.9259,  cwr = 0.8626,  ted = 554.0000,  ned = 105.7222,  ted/w = 0.3093, 
epoch 181 iter 7240: loss = 0.1413, smooth loss = 0.1024, ce loss = 0.1413, contrastive loss = 0.0000, lr = 0.00013622301618746387
average data time = 0.0618s, average running time = 0.3813s
epoch 181 iter 7240: eval loss = 0.3437,  ccr = 0.9265,  cwr = 0.8593,  ted = 559.0000,  ned = 106.1714,  ted/w = 0.3121, 
epoch 182 iter 7280: loss = 0.0748, smooth loss = 0.0978, ce loss = 0.0748, contrastive loss = 0.0000, lr = 0.00012237821982727913
average data time = 0.0617s, average running time = 0.3817s
epoch 182 iter 7280: eval loss = 0.3422,  ccr = 0.9278,  cwr = 0.8626,  ted = 548.0000,  ned = 104.7647,  ted/w = 0.3060, 
epoch 183 iter 7320: loss = 0.1581, smooth loss = 0.0953, ce loss = 0.1581, contrastive loss = 0.0000, lr = 0.00010925767313997105
average data time = 0.0618s, average running time = 0.3820s
epoch 183 iter 7320: eval loss = 0.3434,  ccr = 0.9274,  cwr = 0.8621,  ted = 543.0000,  ned = 103.8512,  ted/w = 0.3032, 
epoch 184 iter 7360: loss = 0.1352, smooth loss = 0.0944, ce loss = 0.1352, contrastive loss = 0.0000, lr = 9.686537277116215e-05
average data time = 0.0618s, average running time = 0.3823s
epoch 184 iter 7360: eval loss = 0.3516,  ccr = 0.9268,  cwr = 0.8615,  ted = 554.0000,  ned = 106.6802,  ted/w = 0.3093, 
epoch 185 iter 7400: loss = 0.1060, smooth loss = 0.0924, ce loss = 0.1060, contrastive loss = 0.0000, lr = 8.520509353559239e-05
average data time = 0.0618s, average running time = 0.3826s
epoch 185 iter 7400: eval loss = 0.3490,  ccr = 0.9264,  cwr = 0.8615,  ted = 551.0000,  ned = 105.2218,  ted/w = 0.3076, 
epoch 186 iter 7440: loss = 0.0911, smooth loss = 0.0973, ce loss = 0.0911, contrastive loss = 0.0000, lr = 7.428038726727158e-05
average data time = 0.0618s, average running time = 0.3828s
epoch 186 iter 7440: eval loss = 0.3478,  ccr = 0.9268,  cwr = 0.8610,  ted = 553.0000,  ned = 105.3179,  ted/w = 0.3088, 
epoch 187 iter 7480: loss = 0.1382, smooth loss = 0.0944, ce loss = 0.1382, contrastive loss = 0.0000, lr = 6.409458173756002e-05
average data time = 0.0618s, average running time = 0.3832s
epoch 187 iter 7480: eval loss = 0.3475,  ccr = 0.9276,  cwr = 0.8599,  ted = 552.0000,  ned = 105.9373,  ted/w = 0.3082, 
epoch 188 iter 7520: loss = 0.0526, smooth loss = 0.0927, ce loss = 0.0526, contrastive loss = 0.0000, lr = 5.465077964149312e-05
average data time = 0.0618s, average running time = 0.3835s
epoch 188 iter 7520: eval loss = 0.3483,  ccr = 0.9278,  cwr = 0.8615,  ted = 551.0000,  ned = 105.9167,  ted/w = 0.3076, 
epoch 189 iter 7560: loss = 0.0788, smooth loss = 0.0949, ce loss = 0.0788, contrastive loss = 0.0000, lr = 4.595185765267454e-05
average data time = 0.0619s, average running time = 0.3838s
epoch 189 iter 7560: eval loss = 0.3454,  ccr = 0.9281,  cwr = 0.8610,  ted = 548.0000,  ned = 105.6365,  ted/w = 0.3060, 
epoch 190 iter 7600: loss = 0.0984, smooth loss = 0.0877, ce loss = 0.0984, contrastive loss = 0.0000, lr = 3.8000465547010077e-05
average data time = 0.0619s, average running time = 0.3841s
epoch 190 iter 7600: eval loss = 0.3448,  ccr = 0.9276,  cwr = 0.8610,  ted = 552.0000,  ned = 105.7556,  ted/w = 0.3082, 
epoch 191 iter 7640: loss = 0.0861, smooth loss = 0.0902, ce loss = 0.0861, contrastive loss = 0.0000, lr = 3.079902539556181e-05
average data time = 0.0619s, average running time = 0.3844s
epoch 191 iter 7640: eval loss = 0.3432,  ccr = 0.9270,  cwr = 0.8610,  ted = 555.0000,  ned = 106.0984,  ted/w = 0.3099, 
epoch 192 iter 7680: loss = 0.0735, smooth loss = 0.1003, ce loss = 0.0735, contrastive loss = 0.0000, lr = 2.434973082676151e-05
average data time = 0.0619s, average running time = 0.3847s
epoch 192 iter 7680: eval loss = 0.3435,  ccr = 0.9273,  cwr = 0.8626,  ted = 553.0000,  ned = 105.8944,  ted/w = 0.3088, 
epoch 193 iter 7720: loss = 0.0547, smooth loss = 0.0904, ce loss = 0.0547, contrastive loss = 0.0000, lr = 1.8654546358211455e-05
average data time = 0.0619s, average running time = 0.3849s
epoch 193 iter 7720: eval loss = 0.3446,  ccr = 0.9262,  cwr = 0.8615,  ted = 555.0000,  ned = 106.5349,  ted/w = 0.3099, 
epoch 194 iter 7760: loss = 0.0986, smooth loss = 0.0882, ce loss = 0.0986, contrastive loss = 0.0000, lr = 1.3715206798270458e-05
average data time = 0.0619s, average running time = 0.3852s
epoch 194 iter 7760: eval loss = 0.3487,  ccr = 0.9265,  cwr = 0.8615,  ted = 554.0000,  ned = 106.0766,  ted/w = 0.3093, 
epoch 195 iter 7800: loss = 0.1031, smooth loss = 0.0958, ce loss = 0.1031, contrastive loss = 0.0000, lr = 9.533216717617054e-06
average data time = 0.0619s, average running time = 0.3855s
epoch 195 iter 7800: eval loss = 0.3471,  ccr = 0.9265,  cwr = 0.8615,  ted = 551.0000,  ned = 105.6460,  ted/w = 0.3076, 
epoch 196 iter 7840: loss = 0.1101, smooth loss = 0.0952, ce loss = 0.1101, contrastive loss = 0.0000, lr = 6.1098499909421046e-06
average data time = 0.0619s, average running time = 0.3858s
epoch 196 iter 7840: eval loss = 0.3466,  ccr = 0.9265,  cwr = 0.8621,  ted = 551.0000,  ned = 105.7873,  ted/w = 0.3076, 
epoch 197 iter 7880: loss = 0.0984, smooth loss = 0.0934, ce loss = 0.0984, contrastive loss = 0.0000, lr = 3.4461494089129626e-06
average data time = 0.0620s, average running time = 0.3860s
epoch 197 iter 7880: eval loss = 0.3479,  ccr = 0.9264,  cwr = 0.8610,  ted = 552.0000,  ned = 105.4377,  ted/w = 0.3082, 
epoch 198 iter 7920: loss = 0.1464, smooth loss = 0.0923, ce loss = 0.1464, contrastive loss = 0.0000, lr = 1.5429263605307858e-06
average data time = 0.0620s, average running time = 0.3863s
epoch 198 iter 7920: eval loss = 0.3476,  ccr = 0.9268,  cwr = 0.8632,  ted = 550.0000,  ned = 105.1683,  ted/w = 0.3071, 
epoch 199 iter 7960: loss = 0.0447, smooth loss = 0.0946, ce loss = 0.0447, contrastive loss = 0.0000, lr = 4.0076058597339e-07
average data time = 0.0620s, average running time = 0.3865s
epoch 199 iter 7960: eval loss = 0.3485,  ccr = 0.9264,  cwr = 0.8610,  ted = 551.0000,  ned = 105.7056,  ted/w = 0.3076, 
Save model train-seed-42-SLPR-P-32-256-bs-128-lr-0.005-d_model-512-epoch-200-decay-cos-grad-clip-20-AdamW-wd-0.1-PA-decoder-max-len-25-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.1_199_7960
