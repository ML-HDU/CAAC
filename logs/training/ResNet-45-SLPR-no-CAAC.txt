You have chosen to seed training. This will slow down your training!
Construct dataset.
5131 training items found.
1791 valid items found.
Construct model.
Model(
  (backbone): ResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (decoder): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(size=(4, 32), mode=nearest)
        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=6068, bias=True)
)
The parameters size of model is 18.143252 MB
Construct learner.
Start training.
epoch 1 iter 40: loss = 3.4412, smooth loss = 4.2642, ce loss = 3.4412, contrastive loss = 0.0000, lr = 0.00022954798257166908
epoch 2 iter 80: loss = 3.0352, smooth loss = 3.5007, ce loss = 3.0352, contrastive loss = 0.0000, lr = 0.00031746436089163076
epoch 3 iter 120: loss = 2.8415, smooth loss = 3.1451, ce loss = 2.8415, contrastive loss = 0.0000, lr = 0.00046158434194791716
epoch 4 iter 160: loss = 2.6868, smooth loss = 2.9313, ce loss = 2.6868, contrastive loss = 0.0000, lr = 0.0006583592135001254
epoch 5 iter 200: loss = 2.5594, smooth loss = 2.7679, ce loss = 2.5594, contrastive loss = 0.0000, lr = 0.0009029437251522861
epoch 6 iter 240: loss = 2.4838, smooth loss = 2.6185, ce loss = 2.4838, contrastive loss = 0.0000, lr = 0.0011893153944980642
epoch 7 iter 280: loss = 2.3929, smooth loss = 2.5127, ce loss = 2.3929, contrastive loss = 0.0000, lr = 0.0015104228006250875
epoch 8 iter 320: loss = 2.4746, smooth loss = 2.4162, ce loss = 2.4746, contrastive loss = 0.0000, lr = 0.001858359213500126
epoch 9 iter 360: loss = 2.1037, smooth loss = 2.3067, ce loss = 2.1037, contrastive loss = 0.0000, lr = 0.0022245572839034457
epoch 10 iter 400: loss = 2.0944, smooth loss = 2.1790, ce loss = 2.0944, contrastive loss = 0.0000, lr = 0.0026
epoch 11 iter 440: loss = 1.9623, smooth loss = 2.0103, ce loss = 1.9623, contrastive loss = 0.0000, lr = 0.0029754427160965545
epoch 12 iter 480: loss = 1.5226, smooth loss = 1.8380, ce loss = 1.5226, contrastive loss = 0.0000, lr = 0.003341640786499874
epoch 13 iter 520: loss = 1.3851, smooth loss = 1.6573, ce loss = 1.3851, contrastive loss = 0.0000, lr = 0.0036895771993749123
epoch 14 iter 560: loss = 1.0855, smooth loss = 1.4909, ce loss = 1.0855, contrastive loss = 0.0000, lr = 0.0040106846055019355
epoch 15 iter 600: loss = 1.1393, smooth loss = 1.3507, ce loss = 1.1393, contrastive loss = 0.0000, lr = 0.004297056274847714
epoch 16 iter 640: loss = 1.0888, smooth loss = 1.2411, ce loss = 1.0888, contrastive loss = 0.0000, lr = 0.0045416407864998735
epoch 17 iter 680: loss = 0.9980, smooth loss = 1.1394, ce loss = 0.9980, contrastive loss = 0.0000, lr = 0.004738415658052083
epoch 18 iter 720: loss = 0.7822, smooth loss = 1.0456, ce loss = 0.7822, contrastive loss = 0.0000, lr = 0.004882535639108369
epoch 19 iter 760: loss = 1.0837, smooth loss = 0.9787, ce loss = 1.0837, contrastive loss = 0.0000, lr = 0.004970452017428331
epoch 20 iter 800: loss = 0.8301, smooth loss = 0.8927, ce loss = 0.8301, contrastive loss = 0.0000, lr = 0.005
epoch 21 iter 840: loss = 0.8387, smooth loss = 0.8420, ce loss = 0.8387, contrastive loss = 0.0000, lr = 0.004999619239414027
epoch 22 iter 880: loss = 0.5773, smooth loss = 0.7841, ce loss = 0.5773, contrastive loss = 0.0000, lr = 0.00499847707363947
epoch 23 iter 920: loss = 0.5976, smooth loss = 0.7549, ce loss = 0.5976, contrastive loss = 0.0000, lr = 0.004996573850591087
epoch 24 iter 960: loss = 0.6812, smooth loss = 0.7259, ce loss = 0.6812, contrastive loss = 0.0000, lr = 0.004993910150009058
epoch 25 iter 1000: loss = 0.5362, smooth loss = 0.6941, ce loss = 0.5362, contrastive loss = 0.0000, lr = 0.004990486783282383
epoch 26 iter 1040: loss = 0.4460, smooth loss = 0.6496, ce loss = 0.4460, contrastive loss = 0.0000, lr = 0.0049863047932017296
epoch 27 iter 1080: loss = 0.6371, smooth loss = 0.6296, ce loss = 0.6371, contrastive loss = 0.0000, lr = 0.004981365453641789
epoch 28 iter 1120: loss = 0.5009, smooth loss = 0.6175, ce loss = 0.5009, contrastive loss = 0.0000, lr = 0.004975670269173239
epoch 29 iter 1160: loss = 0.5303, smooth loss = 0.5816, ce loss = 0.5303, contrastive loss = 0.0000, lr = 0.004969220974604439
epoch 30 iter 1200: loss = 0.6365, smooth loss = 0.5709, ce loss = 0.6365, contrastive loss = 0.0000, lr = 0.00496201953445299
epoch 31 iter 1240: loss = 0.4176, smooth loss = 0.5476, ce loss = 0.4176, contrastive loss = 0.0000, lr = 0.004954068142347326
epoch 32 iter 1280: loss = 0.4750, smooth loss = 0.5295, ce loss = 0.4750, contrastive loss = 0.0000, lr = 0.004945369220358507
epoch 33 iter 1320: loss = 0.3988, smooth loss = 0.5141, ce loss = 0.3988, contrastive loss = 0.0000, lr = 0.004935925418262441
epoch 34 iter 1360: loss = 0.4770, smooth loss = 0.4963, ce loss = 0.4770, contrastive loss = 0.0000, lr = 0.004925739612732728
epoch 35 iter 1400: loss = 0.5136, smooth loss = 0.4919, ce loss = 0.5136, contrastive loss = 0.0000, lr = 0.004914814906464408
epoch 36 iter 1440: loss = 0.4111, smooth loss = 0.4829, ce loss = 0.4111, contrastive loss = 0.0000, lr = 0.004903154627228838
epoch 37 iter 1480: loss = 0.4563, smooth loss = 0.4673, ce loss = 0.4563, contrastive loss = 0.0000, lr = 0.004890762326860029
epoch 38 iter 1520: loss = 0.3305, smooth loss = 0.4518, ce loss = 0.3305, contrastive loss = 0.0000, lr = 0.004877641780172721
epoch 39 iter 1560: loss = 0.6519, smooth loss = 0.4585, ce loss = 0.6519, contrastive loss = 0.0000, lr = 0.004863796983812537
epoch 40 iter 1600: loss = 0.3566, smooth loss = 0.4517, ce loss = 0.3566, contrastive loss = 0.0000, lr = 0.004849232155038563
epoch 41 iter 1640: loss = 0.4499, smooth loss = 0.4459, ce loss = 0.4499, contrastive loss = 0.0000, lr = 0.004833951730438739
epoch 42 iter 1680: loss = 0.3523, smooth loss = 0.4346, ce loss = 0.3523, contrastive loss = 0.0000, lr = 0.004817960364578423
epoch 43 iter 1720: loss = 0.4087, smooth loss = 0.4344, ce loss = 0.4087, contrastive loss = 0.0000, lr = 0.0048012629285825665
epoch 44 iter 1760: loss = 0.4450, smooth loss = 0.4292, ce loss = 0.4450, contrastive loss = 0.0000, lr = 0.004783864508651926
epoch 45 iter 1800: loss = 0.3076, smooth loss = 0.4272, ce loss = 0.3076, contrastive loss = 0.0000, lr = 0.004765770404513755
epoch 46 iter 1840: loss = 0.3799, smooth loss = 0.4141, ce loss = 0.3799, contrastive loss = 0.0000, lr = 0.004746986127807455
epoch 47 iter 1880: loss = 0.4569, smooth loss = 0.4203, ce loss = 0.4569, contrastive loss = 0.0000, lr = 0.004727517400405678
epoch 48 iter 1920: loss = 0.4313, smooth loss = 0.4043, ce loss = 0.4313, contrastive loss = 0.0000, lr = 0.004707370152671389
epoch 49 iter 1960: loss = 0.3424, smooth loss = 0.3891, ce loss = 0.3424, contrastive loss = 0.0000, lr = 0.004686550521651418
epoch 50 iter 2000: loss = 0.3580, smooth loss = 0.3806, ce loss = 0.3580, contrastive loss = 0.0000, lr = 0.00466506484920706
epoch 51 iter 2040: loss = 0.3888, smooth loss = 0.3830, ce loss = 0.3888, contrastive loss = 0.0000, lr = 0.004642919680082274
epoch 52 iter 2080: loss = 0.3285, smooth loss = 0.3793, ce loss = 0.3285, contrastive loss = 0.0000, lr = 0.004620121759910103
epoch 53 iter 2120: loss = 0.2680, smooth loss = 0.3898, ce loss = 0.2680, contrastive loss = 0.0000, lr = 0.004596678033157881
epoch 54 iter 2160: loss = 0.3542, smooth loss = 0.3815, ce loss = 0.3542, contrastive loss = 0.0000, lr = 0.004572595641011879
epoch 55 iter 2200: loss = 0.4265, smooth loss = 0.3781, ce loss = 0.4265, contrastive loss = 0.0000, lr = 0.004547881919202037
epoch 56 iter 2240: loss = 0.4081, smooth loss = 0.3825, ce loss = 0.4081, contrastive loss = 0.0000, lr = 0.004522544395767425
epoch 57 iter 2280: loss = 0.2991, smooth loss = 0.3607, ce loss = 0.2991, contrastive loss = 0.0000, lr = 0.004496590788763132
epoch 58 iter 2320: loss = 0.3330, smooth loss = 0.3603, ce loss = 0.3330, contrastive loss = 0.0000, lr = 0.004470029003909268
epoch 59 iter 2360: loss = 0.4490, smooth loss = 0.3715, ce loss = 0.4490, contrastive loss = 0.0000, lr = 0.004442867132182813
epoch 60 iter 2400: loss = 0.2952, smooth loss = 0.3543, ce loss = 0.2952, contrastive loss = 0.0000, lr = 0.004415113447353014
epoch 61 iter 2440: loss = 0.3632, smooth loss = 0.3491, ce loss = 0.3632, contrastive loss = 0.0000, lr = 0.0043867764034611284
epoch 62 iter 2480: loss = 0.2609, smooth loss = 0.3504, ce loss = 0.2609, contrastive loss = 0.0000, lr = 0.0043578646322452305
epoch 63 iter 2520: loss = 0.2784, smooth loss = 0.3523, ce loss = 0.2784, contrastive loss = 0.0000, lr = 0.00432838694051091
epoch 64 iter 2560: loss = 0.3460, smooth loss = 0.3506, ce loss = 0.3460, contrastive loss = 0.0000, lr = 0.004298352307448625
epoch 65 iter 2600: loss = 0.4075, smooth loss = 0.3493, ce loss = 0.4075, contrastive loss = 0.0000, lr = 0.004267769881898557
epoch 66 iter 2640: loss = 0.3238, smooth loss = 0.3340, ce loss = 0.3238, contrastive loss = 0.0000, lr = 0.004236648979563789
epoch 67 iter 2680: loss = 0.2845, smooth loss = 0.3311, ce loss = 0.2845, contrastive loss = 0.0000, lr = 0.0042049990801726455
epoch 68 iter 2720: loss = 0.2503, smooth loss = 0.3270, ce loss = 0.2503, contrastive loss = 0.0000, lr = 0.004172829824591082
epoch 69 iter 2760: loss = 0.4176, smooth loss = 0.3306, ce loss = 0.4176, contrastive loss = 0.0000, lr = 0.004140151011885978
epoch 70 iter 2800: loss = 0.3007, smooth loss = 0.3273, ce loss = 0.3007, contrastive loss = 0.0000, lr = 0.004106972596340251
epoch 71 iter 2840: loss = 0.3742, smooth loss = 0.3176, ce loss = 0.3742, contrastive loss = 0.0000, lr = 0.004073304684420683
epoch 72 iter 2880: loss = 0.3238, smooth loss = 0.3103, ce loss = 0.3238, contrastive loss = 0.0000, lr = 0.004039157531699393
epoch 73 iter 2920: loss = 0.4508, smooth loss = 0.3093, ce loss = 0.4508, contrastive loss = 0.0000, lr = 0.00400454153972989
epoch 74 iter 2960: loss = 0.2027, smooth loss = 0.3003, ce loss = 0.2027, contrastive loss = 0.0000, lr = 0.00396946725287866
epoch 75 iter 3000: loss = 0.2980, smooth loss = 0.3051, ce loss = 0.2980, contrastive loss = 0.0000, lr = 0.003933945355113252
epoch 76 iter 3040: loss = 0.2040, smooth loss = 0.3153, ce loss = 0.2040, contrastive loss = 0.0000, lr = 0.0038979866667478323
epoch 77 iter 3080: loss = 0.3307, smooth loss = 0.3056, ce loss = 0.3307, contrastive loss = 0.0000, lr = 0.003861602141147218
epoch 78 iter 3120: loss = 0.4206, smooth loss = 0.3075, ce loss = 0.4206, contrastive loss = 0.0000, lr = 0.00382480286139037
epoch 79 iter 3160: loss = 0.2550, smooth loss = 0.3154, ce loss = 0.2550, contrastive loss = 0.0000, lr = 0.0037876000368943868
epoch 80 iter 3200: loss = 0.3914, smooth loss = 0.3074, ce loss = 0.3914, contrastive loss = 0.0000, lr = 0.003750005
epoch 81 iter 3240: loss = 0.2505, smooth loss = 0.3029, ce loss = 0.2505, contrastive loss = 0.0000, lr = 0.0037120292025196408
epoch 82 iter 3280: loss = 0.2943, smooth loss = 0.3012, ce loss = 0.2943, contrastive loss = 0.0000, lr = 0.003673684212249099
epoch 83 iter 3320: loss = 0.2393, smooth loss = 0.3011, ce loss = 0.2393, contrastive loss = 0.0000, lr = 0.0036349817094438693
epoch 84 iter 3360: loss = 0.2830, smooth loss = 0.3012, ce loss = 0.2830, contrastive loss = 0.0000, lr = 0.0035959334832612257
epoch 85 iter 3400: loss = 0.4289, smooth loss = 0.3063, ce loss = 0.4289, contrastive loss = 0.0000, lr = 0.0035565514281691315
epoch 86 iter 3440: loss = 0.3424, smooth loss = 0.2953, ce loss = 0.3424, contrastive loss = 0.0000, lr = 0.00351684754032307
epoch 87 iter 3480: loss = 0.2109, smooth loss = 0.2820, ce loss = 0.2109, contrastive loss = 0.0000, lr = 0.003476833913911899
epoch 88 iter 3520: loss = 0.3414, smooth loss = 0.2736, ce loss = 0.3414, contrastive loss = 0.0000, lr = 0.0034365227374738463
epoch 89 iter 3560: loss = 0.2272, smooth loss = 0.2780, ce loss = 0.2272, contrastive loss = 0.0000, lr = 0.0033959262901837547
epoch 90 iter 3600: loss = 0.1771, smooth loss = 0.2790, ce loss = 0.1771, contrastive loss = 0.0000, lr = 0.003355056938112739
epoch 91 iter 3640: loss = 0.2673, smooth loss = 0.2886, ce loss = 0.2673, contrastive loss = 0.0000, lr = 0.0033139271304613474
epoch 92 iter 3680: loss = 0.1871, smooth loss = 0.2789, ce loss = 0.1871, contrastive loss = 0.0000, lr = 0.003272549395767425
epoch 93 iter 3720: loss = 0.1846, smooth loss = 0.2701, ce loss = 0.1846, contrastive loss = 0.0000, lr = 0.0032309363380897947
epoch 94 iter 3760: loss = 0.2208, smooth loss = 0.2630, ce loss = 0.2208, contrastive loss = 0.0000, lr = 0.0031891006331689394
epoch 95 iter 3800: loss = 0.3018, smooth loss = 0.2585, ce loss = 0.3018, contrastive loss = 0.0000, lr = 0.003147055024565851
epoch 96 iter 3840: loss = 0.1842, smooth loss = 0.2632, ce loss = 0.1842, contrastive loss = 0.0000, lr = 0.003104812319780213
epoch 97 iter 3880: loss = 0.2646, smooth loss = 0.2665, ce loss = 0.2646, contrastive loss = 0.0000, lr = 0.0030623853863491193
epoch 98 iter 3920: loss = 0.2428, smooth loss = 0.2693, ce loss = 0.2428, contrastive loss = 0.0000, lr = 0.0030197871479274896
epoch 99 iter 3960: loss = 0.3346, smooth loss = 0.2607, ce loss = 0.3346, contrastive loss = 0.0000, lr = 0.0029770305803514083
epoch 100 iter 4000: loss = 0.1885, smooth loss = 0.2513, ce loss = 0.1885, contrastive loss = 0.0000, lr = 0.0029341287076855497
Save model train-seed-42-SLPR-32-256-bs-128-lr-0.005-d_model-512-epoch-200-decay-cos-grad-clip-20-AdamW-wd-0.1-PA-decoder-max-len-25-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.1_100_4000
epoch 101 iter 4040: loss = 0.4236, smooth loss = 0.2615, ce loss = 0.4236, contrastive loss = 0.0000, lr = 0.002891094598255927
average data time = 0.0425s, average running time = 0.2902s
epoch 101 iter 4040: eval loss = 0.2105,  ccr = 0.9525,  cwr = 0.8839,  ted = 355.0000,  ned = 75.1437,  ted/w = 0.1982, 
Better model found at epoch 101, iter 4040 with accuracy value: 0.8839.
epoch 102 iter 4080: loss = 0.1714, smooth loss = 0.2524, ce loss = 0.1714, contrastive loss = 0.0000, lr = 0.002847941360669154
average data time = 0.0425s, average running time = 0.2916s
epoch 102 iter 4080: eval loss = 0.1930,  ccr = 0.9563,  cwr = 0.8950,  ted = 305.0000,  ned = 60.8135,  ted/w = 0.1703, 
Better model found at epoch 102, iter 4080 with accuracy value: 0.8950.
epoch 103 iter 4120: loss = 0.2046, smooth loss = 0.2510, ce loss = 0.2046, contrastive loss = 0.0000, lr = 0.0028046821398194344
average data time = 0.0425s, average running time = 0.2929s
epoch 103 iter 4120: eval loss = 0.1933,  ccr = 0.9590,  cwr = 0.9062,  ted = 283.0000,  ned = 57.1004,  ted/w = 0.1580, 
Better model found at epoch 103, iter 4120 with accuracy value: 0.9062.
epoch 104 iter 4160: loss = 0.1761, smooth loss = 0.2406, ce loss = 0.1761, contrastive loss = 0.0000, lr = 0.002761330112884501
average data time = 0.0425s, average running time = 0.2942s
epoch 104 iter 4160: eval loss = 0.2105,  ccr = 0.9543,  cwr = 0.8883,  ted = 333.0000,  ned = 72.9369,  ted/w = 0.1859, 
epoch 105 iter 4200: loss = 0.2096, smooth loss = 0.2436, ce loss = 0.2096, contrastive loss = 0.0000, lr = 0.0027178984853117186
average data time = 0.0424s, average running time = 0.2952s
epoch 105 iter 4200: eval loss = 0.2082,  ccr = 0.9525,  cwr = 0.8827,  ted = 343.0000,  ned = 71.5944,  ted/w = 0.1915, 
epoch 106 iter 4240: loss = 0.2078, smooth loss = 0.2359, ce loss = 0.2078, contrastive loss = 0.0000, lr = 0.0026744004867955756
average data time = 0.0424s, average running time = 0.2962s
epoch 106 iter 4240: eval loss = 0.1942,  ccr = 0.9560,  cwr = 0.8900,  ted = 320.0000,  ned = 67.0258,  ted/w = 0.1787, 
epoch 107 iter 4280: loss = 0.1878, smooth loss = 0.2406, ce loss = 0.1878, contrastive loss = 0.0000, lr = 0.0026308493672477975
average data time = 0.0423s, average running time = 0.2972s
epoch 107 iter 4280: eval loss = 0.1950,  ccr = 0.9586,  cwr = 0.8973,  ted = 306.0000,  ned = 62.0845,  ted/w = 0.1709, 
epoch 108 iter 4320: loss = 0.1478, smooth loss = 0.2288, ce loss = 0.1478, contrastive loss = 0.0000, lr = 0.002587258392761286
average data time = 0.0423s, average running time = 0.2981s
epoch 108 iter 4320: eval loss = 0.2042,  ccr = 0.9552,  cwr = 0.8867,  ted = 338.0000,  ned = 67.3905,  ted/w = 0.1887, 
epoch 109 iter 4360: loss = 0.1385, smooth loss = 0.2166, ce loss = 0.1385, contrastive loss = 0.0000, lr = 0.002543640841569145
average data time = 0.0423s, average running time = 0.2990s
epoch 109 iter 4360: eval loss = 0.1925,  ccr = 0.9595,  cwr = 0.9056,  ted = 298.0000,  ned = 60.3500,  ted/w = 0.1664, 
epoch 110 iter 4400: loss = 0.1502, smooth loss = 0.2161, ce loss = 0.1502, contrastive loss = 0.0000, lr = 0.00250001
average data time = 0.0423s, average running time = 0.2999s
epoch 110 iter 4400: eval loss = 0.2000,  ccr = 0.9544,  cwr = 0.8872,  ted = 329.0000,  ned = 64.9730,  ted/w = 0.1837, 
epoch 111 iter 4440: loss = 0.2255, smooth loss = 0.2222, ce loss = 0.2255, contrastive loss = 0.0000, lr = 0.0024563791584308555
average data time = 0.0423s, average running time = 0.3007s
epoch 111 iter 4440: eval loss = 0.2104,  ccr = 0.9551,  cwr = 0.8934,  ted = 329.0000,  ned = 66.9694,  ted/w = 0.1837, 
epoch 112 iter 4480: loss = 0.2306, smooth loss = 0.2147, ce loss = 0.2306, contrastive loss = 0.0000, lr = 0.0024127616072387153
average data time = 0.0422s, average running time = 0.3016s
epoch 112 iter 4480: eval loss = 0.2163,  ccr = 0.9530,  cwr = 0.8688,  ted = 359.0000,  ned = 78.5845,  ted/w = 0.2004, 
epoch 113 iter 4520: loss = 0.2558, smooth loss = 0.2174, ce loss = 0.2558, contrastive loss = 0.0000, lr = 0.002369170632752203
average data time = 0.0422s, average running time = 0.3024s
epoch 113 iter 4520: eval loss = 0.1973,  ccr = 0.9600,  cwr = 0.9001,  ted = 301.0000,  ned = 61.7667,  ted/w = 0.1681, 
epoch 114 iter 4560: loss = 0.1289, smooth loss = 0.2179, ce loss = 0.1289, contrastive loss = 0.0000, lr = 0.002325619513204424
average data time = 0.0422s, average running time = 0.3032s
epoch 114 iter 4560: eval loss = 0.2139,  ccr = 0.9537,  cwr = 0.8855,  ted = 338.0000,  ned = 68.9179,  ted/w = 0.1887, 
epoch 115 iter 4600: loss = 0.2516, smooth loss = 0.2182, ce loss = 0.2516, contrastive loss = 0.0000, lr = 0.002282121514688282
average data time = 0.0422s, average running time = 0.3040s
epoch 115 iter 4600: eval loss = 0.2030,  ccr = 0.9523,  cwr = 0.8906,  ted = 347.0000,  ned = 67.2175,  ted/w = 0.1937, 
epoch 116 iter 4640: loss = 0.2218, smooth loss = 0.2185, ce loss = 0.2218, contrastive loss = 0.0000, lr = 0.0022386898871154994
average data time = 0.0422s, average running time = 0.3048s
epoch 116 iter 4640: eval loss = 0.1921,  ccr = 0.9571,  cwr = 0.8945,  ted = 303.0000,  ned = 62.2845,  ted/w = 0.1692, 
epoch 117 iter 4680: loss = 0.1828, smooth loss = 0.2080, ce loss = 0.1828, contrastive loss = 0.0000, lr = 0.0021953378601805656
average data time = 0.0422s, average running time = 0.3056s
epoch 117 iter 4680: eval loss = 0.2033,  ccr = 0.9552,  cwr = 0.9001,  ted = 321.0000,  ned = 68.1278,  ted/w = 0.1792, 
epoch 118 iter 4720: loss = 0.2162, smooth loss = 0.1968, ce loss = 0.2162, contrastive loss = 0.0000, lr = 0.0021520786393308465
average data time = 0.0422s, average running time = 0.3064s
epoch 118 iter 4720: eval loss = 0.1987,  ccr = 0.9596,  cwr = 0.9012,  ted = 298.0000,  ned = 64.7361,  ted/w = 0.1664, 
epoch 119 iter 4760: loss = 0.1854, smooth loss = 0.1963, ce loss = 0.1854, contrastive loss = 0.0000, lr = 0.0021089254017440727
average data time = 0.0422s, average running time = 0.3071s
epoch 119 iter 4760: eval loss = 0.1947,  ccr = 0.9565,  cwr = 0.8978,  ted = 316.0000,  ned = 66.6004,  ted/w = 0.1764, 
epoch 120 iter 4800: loss = 0.1594, smooth loss = 0.2106, ce loss = 0.1594, contrastive loss = 0.0000, lr = 0.0020658912923144507
average data time = 0.0421s, average running time = 0.3078s
epoch 120 iter 4800: eval loss = 0.2029,  ccr = 0.9558,  cwr = 0.8956,  ted = 317.0000,  ned = 63.3476,  ted/w = 0.1770, 
epoch 121 iter 4840: loss = 0.3226, smooth loss = 0.2031, ce loss = 0.3226, contrastive loss = 0.0000, lr = 0.0020229894196485917
average data time = 0.0421s, average running time = 0.3085s
epoch 121 iter 4840: eval loss = 0.1980,  ccr = 0.9590,  cwr = 0.9012,  ted = 285.0000,  ned = 58.3329,  ted/w = 0.1591, 
epoch 122 iter 4880: loss = 0.1492, smooth loss = 0.1938, ce loss = 0.1492, contrastive loss = 0.0000, lr = 0.0019802328520725104
average data time = 0.0421s, average running time = 0.3093s
epoch 122 iter 4880: eval loss = 0.2032,  ccr = 0.9583,  cwr = 0.9006,  ted = 300.0000,  ned = 58.9917,  ted/w = 0.1675, 
epoch 123 iter 4920: loss = 0.2327, smooth loss = 0.1945, ce loss = 0.2327, contrastive loss = 0.0000, lr = 0.0019376346136508816
average data time = 0.0421s, average running time = 0.3101s
epoch 123 iter 4920: eval loss = 0.2026,  ccr = 0.9582,  cwr = 0.8978,  ted = 311.0000,  ned = 61.8643,  ted/w = 0.1736, 
epoch 124 iter 4960: loss = 0.1709, smooth loss = 0.1854, ce loss = 0.1709, contrastive loss = 0.0000, lr = 0.0018952076802197873
average data time = 0.0421s, average running time = 0.3108s
epoch 124 iter 4960: eval loss = 0.1967,  ccr = 0.9607,  cwr = 0.9112,  ted = 282.0000,  ned = 59.4171,  ted/w = 0.1575, 
Better model found at epoch 124, iter 4960 with accuracy value: 0.9112.
epoch 125 iter 5000: loss = 0.1963, smooth loss = 0.1835, ce loss = 0.1963, contrastive loss = 0.0000, lr = 0.0018529649754341492
average data time = 0.0421s, average running time = 0.3118s
epoch 125 iter 5000: eval loss = 0.2024,  ccr = 0.9541,  cwr = 0.8956,  ted = 322.0000,  ned = 66.2107,  ted/w = 0.1798, 
epoch 126 iter 5040: loss = 0.1316, smooth loss = 0.1891, ce loss = 0.1316, contrastive loss = 0.0000, lr = 0.0018109193668310606
average data time = 0.0421s, average running time = 0.3125s
epoch 126 iter 5040: eval loss = 0.1886,  ccr = 0.9599,  cwr = 0.9051,  ted = 273.0000,  ned = 55.1548,  ted/w = 0.1524, 
epoch 127 iter 5080: loss = 0.1164, smooth loss = 0.1825, ce loss = 0.1164, contrastive loss = 0.0000, lr = 0.0017690836619102058
average data time = 0.0422s, average running time = 0.3132s
epoch 127 iter 5080: eval loss = 0.1861,  ccr = 0.9640,  cwr = 0.9084,  ted = 260.0000,  ned = 54.3714,  ted/w = 0.1452, 
epoch 128 iter 5120: loss = 0.2501, smooth loss = 0.1795, ce loss = 0.2501, contrastive loss = 0.0000, lr = 0.0017274706042325755
average data time = 0.0422s, average running time = 0.3138s
epoch 128 iter 5120: eval loss = 0.1995,  ccr = 0.9595,  cwr = 0.9028,  ted = 295.0000,  ned = 60.5706,  ted/w = 0.1647, 
epoch 129 iter 5160: loss = 0.1644, smooth loss = 0.1782, ce loss = 0.1644, contrastive loss = 0.0000, lr = 0.0016860928695386535
average data time = 0.0422s, average running time = 0.3145s
epoch 129 iter 5160: eval loss = 0.1927,  ccr = 0.9609,  cwr = 0.9045,  ted = 280.0000,  ned = 59.1813,  ted/w = 0.1563, 
epoch 130 iter 5200: loss = 0.1686, smooth loss = 0.1780, ce loss = 0.1686, contrastive loss = 0.0000, lr = 0.0016449630618872617
average data time = 0.0422s, average running time = 0.3151s
epoch 130 iter 5200: eval loss = 0.2023,  ccr = 0.9611,  cwr = 0.9056,  ted = 290.0000,  ned = 59.4798,  ted/w = 0.1619, 
epoch 131 iter 5240: loss = 0.1616, smooth loss = 0.1806, ce loss = 0.1616, contrastive loss = 0.0000, lr = 0.0016040937098162449
average data time = 0.0422s, average running time = 0.3157s
epoch 131 iter 5240: eval loss = 0.1869,  ccr = 0.9616,  cwr = 0.9090,  ted = 279.0000,  ned = 55.4794,  ted/w = 0.1558, 
epoch 132 iter 5280: loss = 0.1939, smooth loss = 0.1839, ce loss = 0.1939, contrastive loss = 0.0000, lr = 0.001563497262526154
average data time = 0.0422s, average running time = 0.3163s
epoch 132 iter 5280: eval loss = 0.1828,  ccr = 0.9618,  cwr = 0.9123,  ted = 270.0000,  ned = 53.4286,  ted/w = 0.1508, 
Better model found at epoch 132, iter 5280 with accuracy value: 0.9123.
epoch 133 iter 5320: loss = 0.2102, smooth loss = 0.1772, ce loss = 0.2102, contrastive loss = 0.0000, lr = 0.001523186086088101
average data time = 0.0422s, average running time = 0.3172s
epoch 133 iter 5320: eval loss = 0.2067,  ccr = 0.9572,  cwr = 0.9006,  ted = 309.0000,  ned = 62.5282,  ted/w = 0.1725, 
epoch 134 iter 5360: loss = 0.2085, smooth loss = 0.1720, ce loss = 0.2085, contrastive loss = 0.0000, lr = 0.001483172459676931
average data time = 0.0422s, average running time = 0.3178s
epoch 134 iter 5360: eval loss = 0.1828,  ccr = 0.9613,  cwr = 0.9095,  ted = 266.0000,  ned = 52.3000,  ted/w = 0.1485, 
epoch 135 iter 5400: loss = 0.1792, smooth loss = 0.1680, ce loss = 0.1792, contrastive loss = 0.0000, lr = 0.0014434685718308694
average data time = 0.0422s, average running time = 0.3184s
epoch 135 iter 5400: eval loss = 0.1886,  ccr = 0.9631,  cwr = 0.9123,  ted = 259.0000,  ned = 51.7139,  ted/w = 0.1446, 
epoch 136 iter 5440: loss = 0.1344, smooth loss = 0.1645, ce loss = 0.1344, contrastive loss = 0.0000, lr = 0.0014040865167387743
average data time = 0.0422s, average running time = 0.3190s
epoch 136 iter 5440: eval loss = 0.1952,  ccr = 0.9609,  cwr = 0.9090,  ted = 279.0000,  ned = 57.0798,  ted/w = 0.1558, 
epoch 137 iter 5480: loss = 0.1525, smooth loss = 0.1606, ce loss = 0.1525, contrastive loss = 0.0000, lr = 0.0013650382905561306
average data time = 0.0422s, average running time = 0.3195s
epoch 137 iter 5480: eval loss = 0.1875,  ccr = 0.9618,  cwr = 0.9123,  ted = 272.0000,  ned = 55.0857,  ted/w = 0.1519, 
epoch 138 iter 5520: loss = 0.1207, smooth loss = 0.1594, ce loss = 0.1207, contrastive loss = 0.0000, lr = 0.0013263357877509017
average data time = 0.0422s, average running time = 0.3200s
epoch 138 iter 5520: eval loss = 0.1874,  ccr = 0.9626,  cwr = 0.9135,  ted = 260.0000,  ned = 51.0734,  ted/w = 0.1452, 
Better model found at epoch 138, iter 5520 with accuracy value: 0.9135.
epoch 139 iter 5560: loss = 0.1932, smooth loss = 0.1534, ce loss = 0.1932, contrastive loss = 0.0000, lr = 0.00128799079748036
average data time = 0.0422s, average running time = 0.3209s
epoch 139 iter 5560: eval loss = 0.1896,  ccr = 0.9597,  cwr = 0.9107,  ted = 272.0000,  ned = 54.5968,  ted/w = 0.1519, 
epoch 140 iter 5600: loss = 0.1099, smooth loss = 0.1555, ce loss = 0.1099, contrastive loss = 0.0000, lr = 0.0012500150000000008
average data time = 0.0422s, average running time = 0.3214s
epoch 140 iter 5600: eval loss = 0.1995,  ccr = 0.9591,  cwr = 0.9095,  ted = 280.0000,  ned = 57.4123,  ted/w = 0.1563, 
epoch 141 iter 5640: loss = 0.0759, smooth loss = 0.1608, ce loss = 0.0759, contrastive loss = 0.0000, lr = 0.0012124199631056137
average data time = 0.0422s, average running time = 0.3220s
epoch 141 iter 5640: eval loss = 0.1968,  ccr = 0.9602,  cwr = 0.9095,  ted = 280.0000,  ned = 57.2413,  ted/w = 0.1563, 
epoch 142 iter 5680: loss = 0.1222, smooth loss = 0.1673, ce loss = 0.1222, contrastive loss = 0.0000, lr = 0.0011752171386096306
average data time = 0.0422s, average running time = 0.3225s
epoch 142 iter 5680: eval loss = 0.1864,  ccr = 0.9620,  cwr = 0.9107,  ted = 281.0000,  ned = 58.4754,  ted/w = 0.1569, 
epoch 143 iter 5720: loss = 0.1622, smooth loss = 0.1622, ce loss = 0.1622, contrastive loss = 0.0000, lr = 0.0011384178588527826
average data time = 0.0422s, average running time = 0.3230s
epoch 143 iter 5720: eval loss = 0.1805,  ccr = 0.9633,  cwr = 0.9146,  ted = 254.0000,  ned = 51.7655,  ted/w = 0.1418, 
Better model found at epoch 143, iter 5720 with accuracy value: 0.9146.
epoch 144 iter 5760: loss = 0.1078, smooth loss = 0.1490, ce loss = 0.1078, contrastive loss = 0.0000, lr = 0.0011020333332521681
average data time = 0.0422s, average running time = 0.3239s
epoch 144 iter 5760: eval loss = 0.1939,  ccr = 0.9625,  cwr = 0.9185,  ted = 258.0000,  ned = 52.3456,  ted/w = 0.1441, 
Better model found at epoch 144, iter 5760 with accuracy value: 0.9185.
epoch 145 iter 5800: loss = 0.1850, smooth loss = 0.1466, ce loss = 0.1850, contrastive loss = 0.0000, lr = 0.001066074644886749
average data time = 0.0422s, average running time = 0.3247s
epoch 145 iter 5800: eval loss = 0.1953,  ccr = 0.9616,  cwr = 0.9112,  ted = 270.0000,  ned = 55.6480,  ted/w = 0.1508, 
epoch 146 iter 5840: loss = 0.1035, smooth loss = 0.1445, ce loss = 0.1035, contrastive loss = 0.0000, lr = 0.0010305527471213406
average data time = 0.0422s, average running time = 0.3251s
epoch 146 iter 5840: eval loss = 0.1857,  ccr = 0.9647,  cwr = 0.9185,  ted = 251.0000,  ned = 53.1198,  ted/w = 0.1401, 
epoch 147 iter 5880: loss = 0.1130, smooth loss = 0.1426, ce loss = 0.1130, contrastive loss = 0.0000, lr = 0.0009954784602701108
average data time = 0.0422s, average running time = 0.3257s
epoch 147 iter 5880: eval loss = 0.1892,  ccr = 0.9649,  cwr = 0.9185,  ted = 251.0000,  ned = 52.9687,  ted/w = 0.1401, 
epoch 148 iter 5920: loss = 0.1233, smooth loss = 0.1431, ce loss = 0.1233, contrastive loss = 0.0000, lr = 0.0009608624683006075
average data time = 0.0422s, average running time = 0.3261s
epoch 148 iter 5920: eval loss = 0.1794,  ccr = 0.9657,  cwr = 0.9151,  ted = 245.0000,  ned = 52.1917,  ted/w = 0.1368, 
epoch 149 iter 5960: loss = 0.1852, smooth loss = 0.1424, ce loss = 0.1852, contrastive loss = 0.0000, lr = 0.0009267153155793173
average data time = 0.0422s, average running time = 0.3266s
epoch 149 iter 5960: eval loss = 0.1774,  ccr = 0.9653,  cwr = 0.9246,  ted = 244.0000,  ned = 47.6377,  ted/w = 0.1362, 
Better model found at epoch 149, iter 5960 with accuracy value: 0.9246.
epoch 150 iter 6000: loss = 0.1649, smooth loss = 0.1403, ce loss = 0.1649, contrastive loss = 0.0000, lr = 0.0008930474036597485
average data time = 0.0422s, average running time = 0.3272s
epoch 150 iter 6000: eval loss = 0.1762,  ccr = 0.9666,  cwr = 0.9218,  ted = 237.0000,  ned = 47.4135,  ted/w = 0.1323, 
epoch 151 iter 6040: loss = 0.1228, smooth loss = 0.1334, ce loss = 0.1228, contrastive loss = 0.0000, lr = 0.000859868988114022
average data time = 0.0422s, average running time = 0.3277s
epoch 151 iter 6040: eval loss = 0.1757,  ccr = 0.9674,  cwr = 0.9213,  ted = 232.0000,  ned = 46.9163,  ted/w = 0.1295, 
epoch 152 iter 6080: loss = 0.1346, smooth loss = 0.1256, ce loss = 0.1346, contrastive loss = 0.0000, lr = 0.0008271901754089188
average data time = 0.0423s, average running time = 0.3281s
epoch 152 iter 6080: eval loss = 0.1804,  ccr = 0.9666,  cwr = 0.9224,  ted = 236.0000,  ned = 47.8528,  ted/w = 0.1318, 
epoch 153 iter 6120: loss = 0.1319, smooth loss = 0.1276, ce loss = 0.1319, contrastive loss = 0.0000, lr = 0.0007950209198273547
average data time = 0.0423s, average running time = 0.3286s
epoch 153 iter 6120: eval loss = 0.1850,  ccr = 0.9658,  cwr = 0.9202,  ted = 244.0000,  ned = 51.5373,  ted/w = 0.1362, 
epoch 154 iter 6160: loss = 0.1120, smooth loss = 0.1277, ce loss = 0.1120, contrastive loss = 0.0000, lr = 0.0007633710204362112
average data time = 0.0423s, average running time = 0.3290s
epoch 154 iter 6160: eval loss = 0.1868,  ccr = 0.9644,  cwr = 0.9168,  ted = 247.0000,  ned = 51.0956,  ted/w = 0.1379, 
epoch 155 iter 6200: loss = 0.0870, smooth loss = 0.1307, ce loss = 0.0870, contrastive loss = 0.0000, lr = 0.0007322501181014433
average data time = 0.0423s, average running time = 0.3294s
epoch 155 iter 6200: eval loss = 0.1887,  ccr = 0.9634,  cwr = 0.9168,  ted = 257.0000,  ned = 52.0020,  ted/w = 0.1435, 
epoch 156 iter 6240: loss = 0.1152, smooth loss = 0.1252, ce loss = 0.1152, contrastive loss = 0.0000, lr = 0.000701667692551376
average data time = 0.0423s, average running time = 0.3298s
epoch 156 iter 6240: eval loss = 0.1948,  ccr = 0.9649,  cwr = 0.9235,  ted = 237.0000,  ned = 47.0452,  ted/w = 0.1323, 
epoch 157 iter 6280: loss = 0.1617, smooth loss = 0.1231, ce loss = 0.1617, contrastive loss = 0.0000, lr = 0.00067163305948909
average data time = 0.0423s, average running time = 0.3302s
epoch 157 iter 6280: eval loss = 0.1879,  ccr = 0.9648,  cwr = 0.9196,  ted = 240.0000,  ned = 49.1976,  ted/w = 0.1340, 
epoch 158 iter 6320: loss = 0.1127, smooth loss = 0.1200, ce loss = 0.1127, contrastive loss = 0.0000, lr = 0.0006421553677547689
average data time = 0.0423s, average running time = 0.3307s
epoch 158 iter 6320: eval loss = 0.1904,  ccr = 0.9655,  cwr = 0.9213,  ted = 239.0000,  ned = 48.0952,  ted/w = 0.1334, 
epoch 159 iter 6360: loss = 0.1522, smooth loss = 0.1199, ce loss = 0.1522, contrastive loss = 0.0000, lr = 0.0006132435965388722
average data time = 0.0423s, average running time = 0.3311s
epoch 159 iter 6360: eval loss = 0.1856,  ccr = 0.9662,  cwr = 0.9196,  ted = 235.0000,  ned = 50.7869,  ted/w = 0.1312, 
epoch 160 iter 6400: loss = 0.0911, smooth loss = 0.1177, ce loss = 0.0911, contrastive loss = 0.0000, lr = 0.0005849065526469865
average data time = 0.0423s, average running time = 0.3315s
epoch 160 iter 6400: eval loss = 0.1789,  ccr = 0.9690,  cwr = 0.9274,  ted = 215.0000,  ned = 43.5440,  ted/w = 0.1200, 
Better model found at epoch 160, iter 6400 with accuracy value: 0.9274.
epoch 161 iter 6440: loss = 0.0971, smooth loss = 0.1106, ce loss = 0.0971, contrastive loss = 0.0000, lr = 0.0005571528678171874
average data time = 0.0423s, average running time = 0.3322s
epoch 161 iter 6440: eval loss = 0.1823,  ccr = 0.9664,  cwr = 0.9235,  ted = 234.0000,  ned = 48.4540,  ted/w = 0.1307, 
epoch 162 iter 6480: loss = 0.1391, smooth loss = 0.1179, ce loss = 0.1391, contrastive loss = 0.0000, lr = 0.0005299909960907313
average data time = 0.0423s, average running time = 0.3326s
epoch 162 iter 6480: eval loss = 0.1779,  ccr = 0.9676,  cwr = 0.9252,  ted = 228.0000,  ned = 45.4472,  ted/w = 0.1273, 
epoch 163 iter 6520: loss = 0.1542, smooth loss = 0.1180, ce loss = 0.1542, contrastive loss = 0.0000, lr = 0.0005034292112368689
average data time = 0.0423s, average running time = 0.3330s
epoch 163 iter 6520: eval loss = 0.1829,  ccr = 0.9673,  cwr = 0.9269,  ted = 225.0000,  ned = 44.5794,  ted/w = 0.1256, 
epoch 164 iter 6560: loss = 0.0572, smooth loss = 0.1173, ce loss = 0.0572, contrastive loss = 0.0000, lr = 0.0004774756042325754
average data time = 0.0423s, average running time = 0.3334s
epoch 164 iter 6560: eval loss = 0.1862,  ccr = 0.9662,  cwr = 0.9246,  ted = 234.0000,  ned = 46.4520,  ted/w = 0.1307, 
epoch 165 iter 6600: loss = 0.0631, smooth loss = 0.1048, ce loss = 0.0631, contrastive loss = 0.0000, lr = 0.00045213808079796314
average data time = 0.0423s, average running time = 0.3338s
epoch 165 iter 6600: eval loss = 0.1872,  ccr = 0.9671,  cwr = 0.9241,  ted = 233.0000,  ned = 45.9198,  ted/w = 0.1301, 
epoch 166 iter 6640: loss = 0.0502, smooth loss = 0.1050, ce loss = 0.0502, contrastive loss = 0.0000, lr = 0.0004274243589881215
average data time = 0.0423s, average running time = 0.3341s
epoch 166 iter 6640: eval loss = 0.1879,  ccr = 0.9657,  cwr = 0.9207,  ted = 235.0000,  ned = 46.6202,  ted/w = 0.1312, 
epoch 167 iter 6680: loss = 0.1442, smooth loss = 0.1024, ce loss = 0.1442, contrastive loss = 0.0000, lr = 0.0004033419668421196
average data time = 0.0423s, average running time = 0.3346s
epoch 167 iter 6680: eval loss = 0.1890,  ccr = 0.9652,  cwr = 0.9224,  ted = 235.0000,  ned = 47.6754,  ted/w = 0.1312, 
epoch 168 iter 6720: loss = 0.1457, smooth loss = 0.1086, ce loss = 0.1457, contrastive loss = 0.0000, lr = 0.0003798982400898967
average data time = 0.0423s, average running time = 0.3350s
epoch 168 iter 6720: eval loss = 0.1855,  ccr = 0.9659,  cwr = 0.9229,  ted = 232.0000,  ned = 46.3060,  ted/w = 0.1295, 
epoch 169 iter 6760: loss = 0.1472, smooth loss = 0.1064, ce loss = 0.1472, contrastive loss = 0.0000, lr = 0.0003571003199177265
average data time = 0.0423s, average running time = 0.3353s
epoch 169 iter 6760: eval loss = 0.1861,  ccr = 0.9653,  cwr = 0.9207,  ted = 240.0000,  ned = 46.7560,  ted/w = 0.1340, 
epoch 170 iter 6800: loss = 0.0843, smooth loss = 0.1046, ce loss = 0.0843, contrastive loss = 0.0000, lr = 0.0003349551507929411
average data time = 0.0422s, average running time = 0.3357s
epoch 170 iter 6800: eval loss = 0.1858,  ccr = 0.9662,  cwr = 0.9229,  ted = 230.0000,  ned = 45.1929,  ted/w = 0.1284, 
epoch 171 iter 6840: loss = 0.0779, smooth loss = 0.0995, ce loss = 0.0779, contrastive loss = 0.0000, lr = 0.000313469478348582
average data time = 0.0422s, average running time = 0.3361s
epoch 171 iter 6840: eval loss = 0.1905,  ccr = 0.9668,  cwr = 0.9257,  ted = 229.0000,  ned = 45.2802,  ted/w = 0.1279, 
epoch 172 iter 6880: loss = 0.0740, smooth loss = 0.0987, ce loss = 0.0740, contrastive loss = 0.0000, lr = 0.0002926498473286111
average data time = 0.0423s, average running time = 0.3364s
epoch 172 iter 6880: eval loss = 0.1873,  ccr = 0.9666,  cwr = 0.9224,  ted = 228.0000,  ned = 45.5956,  ted/w = 0.1273, 
epoch 173 iter 6920: loss = 0.1143, smooth loss = 0.1015, ce loss = 0.1143, contrastive loss = 0.0000, lr = 0.0002725025995943224
average data time = 0.0423s, average running time = 0.3367s
epoch 173 iter 6920: eval loss = 0.1880,  ccr = 0.9663,  cwr = 0.9241,  ted = 236.0000,  ned = 45.9964,  ted/w = 0.1318, 
epoch 174 iter 6960: loss = 0.0464, smooth loss = 0.0979, ce loss = 0.0464, contrastive loss = 0.0000, lr = 0.00025303387219254594
average data time = 0.0422s, average running time = 0.3370s
epoch 174 iter 6960: eval loss = 0.1861,  ccr = 0.9677,  cwr = 0.9257,  ted = 226.0000,  ned = 44.5270,  ted/w = 0.1262, 
epoch 175 iter 7000: loss = 0.1416, smooth loss = 0.0962, ce loss = 0.1416, contrastive loss = 0.0000, lr = 0.00023424959548624553
average data time = 0.0423s, average running time = 0.3373s
epoch 175 iter 7000: eval loss = 0.1864,  ccr = 0.9674,  cwr = 0.9246,  ted = 226.0000,  ned = 44.2690,  ted/w = 0.1262, 
epoch 176 iter 7040: loss = 0.1455, smooth loss = 0.0956, ce loss = 0.1455, contrastive loss = 0.0000, lr = 0.00021615549134807398
average data time = 0.0423s, average running time = 0.3377s
epoch 176 iter 7040: eval loss = 0.1893,  ccr = 0.9664,  cwr = 0.9252,  ted = 229.0000,  ned = 44.6036,  ted/w = 0.1279, 
epoch 177 iter 7080: loss = 0.0735, smooth loss = 0.0944, ce loss = 0.0735, contrastive loss = 0.0000, lr = 0.00019875707141743358
average data time = 0.0423s, average running time = 0.3380s
epoch 177 iter 7080: eval loss = 0.1865,  ccr = 0.9673,  cwr = 0.9257,  ted = 227.0000,  ned = 43.7897,  ted/w = 0.1267, 
epoch 178 iter 7120: loss = 0.0599, smooth loss = 0.0997, ce loss = 0.0599, contrastive loss = 0.0000, lr = 0.00018205963542157737
average data time = 0.0423s, average running time = 0.3384s
epoch 178 iter 7120: eval loss = 0.1870,  ccr = 0.9679,  cwr = 0.9285,  ted = 220.0000,  ned = 42.5361,  ted/w = 0.1228, 
Better model found at epoch 178, iter 7120 with accuracy value: 0.9285.
epoch 179 iter 7160: loss = 0.0822, smooth loss = 0.0974, ce loss = 0.0822, contrastive loss = 0.0000, lr = 0.00016606826956126088
average data time = 0.0423s, average running time = 0.3391s
epoch 179 iter 7160: eval loss = 0.1824,  ccr = 0.9672,  cwr = 0.9263,  ted = 225.0000,  ned = 44.3853,  ted/w = 0.1256, 
epoch 180 iter 7200: loss = 0.0388, smooth loss = 0.0913, ce loss = 0.0388, contrastive loss = 0.0000, lr = 0.00015078784496143707
average data time = 0.0423s, average running time = 0.3394s
epoch 180 iter 7200: eval loss = 0.1851,  ccr = 0.9666,  cwr = 0.9224,  ted = 231.0000,  ned = 46.0393,  ted/w = 0.1290, 
epoch 181 iter 7240: loss = 0.0314, smooth loss = 0.0907, ce loss = 0.0314, contrastive loss = 0.0000, lr = 0.00013622301618746387
average data time = 0.0423s, average running time = 0.3397s
epoch 181 iter 7240: eval loss = 0.1836,  ccr = 0.9666,  cwr = 0.9246,  ted = 228.0000,  ned = 44.6750,  ted/w = 0.1273, 
epoch 182 iter 7280: loss = 0.0731, smooth loss = 0.0866, ce loss = 0.0731, contrastive loss = 0.0000, lr = 0.00012237821982727913
average data time = 0.0423s, average running time = 0.3401s
epoch 182 iter 7280: eval loss = 0.1820,  ccr = 0.9679,  cwr = 0.9269,  ted = 222.0000,  ned = 43.8218,  ted/w = 0.1240, 
epoch 183 iter 7320: loss = 0.0922, smooth loss = 0.0872, ce loss = 0.0922, contrastive loss = 0.0000, lr = 0.00010925767313997105
average data time = 0.0423s, average running time = 0.3404s
epoch 183 iter 7320: eval loss = 0.1851,  ccr = 0.9664,  cwr = 0.9241,  ted = 231.0000,  ned = 45.8972,  ted/w = 0.1290, 
epoch 184 iter 7360: loss = 0.0717, smooth loss = 0.0868, ce loss = 0.0717, contrastive loss = 0.0000, lr = 9.686537277116215e-05
average data time = 0.0423s, average running time = 0.3407s
epoch 184 iter 7360: eval loss = 0.1819,  ccr = 0.9678,  cwr = 0.9257,  ted = 223.0000,  ned = 44.7167,  ted/w = 0.1245, 
epoch 185 iter 7400: loss = 0.1116, smooth loss = 0.0858, ce loss = 0.1116, contrastive loss = 0.0000, lr = 8.520509353559239e-05
average data time = 0.0423s, average running time = 0.3410s
epoch 185 iter 7400: eval loss = 0.1810,  ccr = 0.9678,  cwr = 0.9269,  ted = 218.0000,  ned = 43.8262,  ted/w = 0.1217, 
epoch 186 iter 7440: loss = 0.0265, smooth loss = 0.0826, ce loss = 0.0265, contrastive loss = 0.0000, lr = 7.428038726727158e-05
average data time = 0.0423s, average running time = 0.3413s
epoch 186 iter 7440: eval loss = 0.1816,  ccr = 0.9681,  cwr = 0.9269,  ted = 217.0000,  ned = 43.5452,  ted/w = 0.1212, 
epoch 187 iter 7480: loss = 0.0829, smooth loss = 0.0864, ce loss = 0.0829, contrastive loss = 0.0000, lr = 6.409458173756002e-05
average data time = 0.0423s, average running time = 0.3417s
epoch 187 iter 7480: eval loss = 0.1807,  ccr = 0.9678,  cwr = 0.9257,  ted = 224.0000,  ned = 44.0230,  ted/w = 0.1251, 
epoch 188 iter 7520: loss = 0.1007, smooth loss = 0.0841, ce loss = 0.1007, contrastive loss = 0.0000, lr = 5.465077964149312e-05
average data time = 0.0423s, average running time = 0.3420s
epoch 188 iter 7520: eval loss = 0.1804,  ccr = 0.9678,  cwr = 0.9257,  ted = 224.0000,  ned = 43.7813,  ted/w = 0.1251, 
epoch 189 iter 7560: loss = 0.0935, smooth loss = 0.0902, ce loss = 0.0935, contrastive loss = 0.0000, lr = 4.595185765267454e-05
average data time = 0.0423s, average running time = 0.3423s
epoch 189 iter 7560: eval loss = 0.1814,  ccr = 0.9681,  cwr = 0.9269,  ted = 221.0000,  ned = 43.3607,  ted/w = 0.1234, 
epoch 190 iter 7600: loss = 0.0763, smooth loss = 0.0824, ce loss = 0.0763, contrastive loss = 0.0000, lr = 3.8000465547010077e-05
average data time = 0.0423s, average running time = 0.3425s
epoch 190 iter 7600: eval loss = 0.1823,  ccr = 0.9677,  cwr = 0.9252,  ted = 225.0000,  ned = 44.1222,  ted/w = 0.1256, 
epoch 191 iter 7640: loss = 0.0198, smooth loss = 0.0820, ce loss = 0.0198, contrastive loss = 0.0000, lr = 3.079902539556181e-05
average data time = 0.0423s, average running time = 0.3428s
epoch 191 iter 7640: eval loss = 0.1823,  ccr = 0.9676,  cwr = 0.9257,  ted = 224.0000,  ned = 44.1083,  ted/w = 0.1251, 
epoch 192 iter 7680: loss = 0.0816, smooth loss = 0.0811, ce loss = 0.0816, contrastive loss = 0.0000, lr = 2.434973082676151e-05
average data time = 0.0423s, average running time = 0.3431s
epoch 192 iter 7680: eval loss = 0.1831,  ccr = 0.9674,  cwr = 0.9257,  ted = 227.0000,  ned = 44.4972,  ted/w = 0.1267, 
epoch 193 iter 7720: loss = 0.0570, smooth loss = 0.0829, ce loss = 0.0570, contrastive loss = 0.0000, lr = 1.8654546358211455e-05
average data time = 0.0423s, average running time = 0.3434s
epoch 193 iter 7720: eval loss = 0.1824,  ccr = 0.9672,  cwr = 0.9246,  ted = 227.0000,  ned = 44.2929,  ted/w = 0.1267, 
epoch 194 iter 7760: loss = 0.0777, smooth loss = 0.0837, ce loss = 0.0777, contrastive loss = 0.0000, lr = 1.3715206798270458e-05
average data time = 0.0423s, average running time = 0.3438s
epoch 194 iter 7760: eval loss = 0.1835,  ccr = 0.9676,  cwr = 0.9257,  ted = 225.0000,  ned = 43.7774,  ted/w = 0.1256, 
epoch 195 iter 7800: loss = 0.0678, smooth loss = 0.0802, ce loss = 0.0678, contrastive loss = 0.0000, lr = 9.533216717617054e-06
average data time = 0.0423s, average running time = 0.3441s
epoch 195 iter 7800: eval loss = 0.1830,  ccr = 0.9674,  cwr = 0.9263,  ted = 224.0000,  ned = 43.7873,  ted/w = 0.1251, 
epoch 196 iter 7840: loss = 0.1095, smooth loss = 0.0781, ce loss = 0.1095, contrastive loss = 0.0000, lr = 6.1098499909421046e-06
average data time = 0.0423s, average running time = 0.3444s
epoch 196 iter 7840: eval loss = 0.1832,  ccr = 0.9677,  cwr = 0.9263,  ted = 223.0000,  ned = 43.8429,  ted/w = 0.1245, 
epoch 197 iter 7880: loss = 0.1079, smooth loss = 0.0807, ce loss = 0.1079, contrastive loss = 0.0000, lr = 3.4461494089129626e-06
average data time = 0.0423s, average running time = 0.3447s
epoch 197 iter 7880: eval loss = 0.1824,  ccr = 0.9678,  cwr = 0.9252,  ted = 220.0000,  ned = 43.4337,  ted/w = 0.1228, 
epoch 198 iter 7920: loss = 0.1675, smooth loss = 0.0860, ce loss = 0.1675, contrastive loss = 0.0000, lr = 1.5429263605307858e-06
average data time = 0.0423s, average running time = 0.3449s
epoch 198 iter 7920: eval loss = 0.1821,  ccr = 0.9678,  cwr = 0.9263,  ted = 222.0000,  ned = 43.6500,  ted/w = 0.1240, 
epoch 199 iter 7960: loss = 0.0974, smooth loss = 0.0815, ce loss = 0.0974, contrastive loss = 0.0000, lr = 4.0076058597339e-07
average data time = 0.0423s, average running time = 0.3452s
epoch 199 iter 7960: eval loss = 0.1812,  ccr = 0.9678,  cwr = 0.9257,  ted = 222.0000,  ned = 43.6623,  ted/w = 0.1240, 
Save model train-seed-42-SLPR-32-256-bs-128-lr-0.005-d_model-512-epoch-200-decay-cos-grad-clip-20-AdamW-wd-0.1-PA-decoder-max-len-25-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.1_199_7960
