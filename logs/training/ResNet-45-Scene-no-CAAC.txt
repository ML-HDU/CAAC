You have chosen to seed training. This will slow down your training!
Construct dataset.
509164 training items found.
63645 valid items found.
Construct model.
Model(
  (backbone): ResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (decoder): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(size=(4, 32), mode=nearest)
        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=7935, bias=True)
)
The parameters size of model is 19.101023 MB
Construct learner.
Use 4 GPUs.
Start training.
epoch 1 iter 1325: loss = 5.3873, smooth loss = 5.3574, ce loss = 5.3873, contrastive loss = 0.0000, lr = 0.00022400000000000002
epoch 2 iter 2650: loss = 4.0352, smooth loss = 4.0276, ce loss = 4.0352, contrastive loss = 0.0000, lr = 0.0006079999999999999
epoch 3 iter 3975: loss = 2.9294, smooth loss = 3.2759, ce loss = 2.9294, contrastive loss = 0.0000, lr = 0.0008
epoch 4 iter 5300: loss = 3.1265, smooth loss = 2.9864, ce loss = 3.1265, contrastive loss = 0.0000, lr = 0.0007998558116451099
epoch 5 iter 6625: loss = 2.9643, smooth loss = 2.7833, ce loss = 2.9643, contrastive loss = 0.0000, lr = 0.0007994233505322638
epoch 6 iter 7950: loss = 2.4966, smooth loss = 2.6447, ce loss = 2.4966, contrastive loss = 0.0000, lr = 0.000798702928441991
epoch 7 iter 9275: loss = 2.6036, smooth loss = 2.5497, ce loss = 2.6036, contrastive loss = 0.0000, lr = 0.000797695064758749
epoch 8 iter 10600: loss = 2.1190, smooth loss = 2.4547, ce loss = 2.1190, contrastive loss = 0.0000, lr = 0.0007964004860964767
epoch 9 iter 11925: loss = 2.1914, smooth loss = 2.3848, ce loss = 2.1914, contrastive loss = 0.0000, lr = 0.0007948201257747449
epoch 10 iter 13250: loss = 2.3348, smooth loss = 2.3072, ce loss = 2.3348, contrastive loss = 0.0000, lr = 0.000792955123145886
epoch 11 iter 14575: loss = 2.3585, smooth loss = 2.2567, ce loss = 2.3585, contrastive loss = 0.0000, lr = 0.0007908068227735828
epoch 12 iter 15900: loss = 2.4326, smooth loss = 2.2146, ce loss = 2.4326, contrastive loss = 0.0000, lr = 0.000788376773463513
epoch 13 iter 17225: loss = 2.0374, smooth loss = 2.1871, ce loss = 2.0374, contrastive loss = 0.0000, lr = 0.0007856667271467458
epoch 14 iter 18550: loss = 2.1325, smooth loss = 2.1377, ce loss = 2.1325, contrastive loss = 0.0000, lr = 0.0007826786376166968
epoch 15 iter 19875: loss = 1.9994, smooth loss = 2.1168, ce loss = 1.9994, contrastive loss = 0.0000, lr = 0.0007794146591205511
epoch 16 iter 21200: loss = 2.0771, smooth loss = 2.0611, ce loss = 2.0771, contrastive loss = 0.0000, lr = 0.0007758771448061701
epoch 17 iter 22525: loss = 1.8190, smooth loss = 2.0773, ce loss = 1.8190, contrastive loss = 0.0000, lr = 0.0007720686450256023
epoch 18 iter 23850: loss = 1.9655, smooth loss = 2.0091, ce loss = 1.9655, contrastive loss = 0.0000, lr = 0.0007679919054964199
epoch 19 iter 25175: loss = 1.9871, smooth loss = 1.9949, ce loss = 1.9871, contrastive loss = 0.0000, lr = 0.0007636498653222099
epoch 20 iter 26500: loss = 1.9908, smooth loss = 1.9563, ce loss = 1.9908, contrastive loss = 0.0000, lr = 0.0007590456548736415
epoch 21 iter 27825: loss = 1.8587, smooth loss = 1.9322, ce loss = 1.8587, contrastive loss = 0.0000, lr = 0.0007541825935316429
epoch 22 iter 29150: loss = 1.8402, smooth loss = 1.9126, ce loss = 1.8402, contrastive loss = 0.0000, lr = 0.0007490641872943116
epoch 23 iter 30475: loss = 1.8399, smooth loss = 1.9128, ce loss = 1.8399, contrastive loss = 0.0000, lr = 0.0007436941262492827
epoch 24 iter 31800: loss = 1.7747, smooth loss = 1.8946, ce loss = 1.7747, contrastive loss = 0.0000, lr = 0.0007380762819133811
epoch 25 iter 33125: loss = 1.6594, smooth loss = 1.8863, ce loss = 1.6594, contrastive loss = 0.0000, lr = 0.0007322147044414715
epoch 26 iter 34450: loss = 1.6052, smooth loss = 1.8433, ce loss = 1.6052, contrastive loss = 0.0000, lr = 0.0007261136197065211
epoch 27 iter 35775: loss = 1.9377, smooth loss = 1.8527, ce loss = 1.9377, contrastive loss = 0.0000, lr = 0.0007197774262529791
epoch 28 iter 37100: loss = 1.8241, smooth loss = 1.8266, ce loss = 1.8241, contrastive loss = 0.0000, lr = 0.0007132106921256691
epoch 29 iter 38425: loss = 1.9850, smooth loss = 1.8052, ce loss = 1.9850, contrastive loss = 0.0000, lr = 0.0007064181515764822
epoch 30 iter 39750: loss = 2.0389, smooth loss = 1.8011, ce loss = 2.0389, contrastive loss = 0.0000, lr = 0.0006994047016512434
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.01-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_30_40000
epoch 31 iter 41075: loss = 1.7175, smooth loss = 1.7544, ce loss = 1.7175, contrastive loss = 0.0000, lr = 0.0006921753986592118
epoch 32 iter 42400: loss = 1.9508, smooth loss = 1.7584, ce loss = 1.9508, contrastive loss = 0.0000, lr = 0.0006847354545277624
epoch 33 iter 43725: loss = 1.8623, smooth loss = 1.7480, ce loss = 1.8623, contrastive loss = 0.0000, lr = 0.0006770902330448742
epoch 34 iter 45050: loss = 1.8953, smooth loss = 1.7379, ce loss = 1.8953, contrastive loss = 0.0000, lr = 0.0006692452459921362
epoch 35 iter 46375: loss = 1.8062, smooth loss = 1.7204, ce loss = 1.8062, contrastive loss = 0.0000, lr = 0.000661206149171058
epoch 36 iter 47700: loss = 1.6642, smooth loss = 1.6846, ce loss = 1.6642, contrastive loss = 0.0000, lr = 0.0006529787383255499
epoch 37 iter 49025: loss = 1.6011, smooth loss = 1.6949, ce loss = 1.6011, contrastive loss = 0.0000, lr = 0.0006445689449635119
epoch 38 iter 50350: loss = 1.6544, smooth loss = 1.6876, ce loss = 1.6544, contrastive loss = 0.0000, lr = 0.0006359828320805452
epoch 39 iter 51675: loss = 1.3849, smooth loss = 1.6683, ce loss = 1.3849, contrastive loss = 0.0000, lr = 0.0006272265897888675
epoch 40 iter 53000: loss = 1.5401, smooth loss = 1.6483, ce loss = 1.5401, contrastive loss = 0.0000, lr = 0.0006183065308545855
epoch 41 iter 54325: loss = 1.5255, smooth loss = 1.6654, ce loss = 1.5255, contrastive loss = 0.0000, lr = 0.0006092290861465388
epoch 42 iter 55650: loss = 1.7384, smooth loss = 1.6376, ce loss = 1.7384, contrastive loss = 0.0000, lr = 0.0006000008
epoch 43 iter 56975: loss = 1.8912, smooth loss = 1.6232, ce loss = 1.8912, contrastive loss = 0.0000, lr = 0.0005906283254985711
epoch 44 iter 58300: loss = 1.5040, smooth loss = 1.6275, ce loss = 1.5040, contrastive loss = 0.0000, lr = 0.0005811184196776785
epoch 45 iter 59625: loss = 1.7274, smooth loss = 1.5987, ce loss = 1.7274, contrastive loss = 0.0000, lr = 0.0005714779386531235
epoch 46 iter 60950: loss = 1.4801, smooth loss = 1.5896, ce loss = 1.4801, contrastive loss = 0.0000, lr = 0.0005617138326782039
epoch 47 iter 62275: loss = 1.4125, smooth loss = 1.5871, ce loss = 1.4125, contrastive loss = 0.0000, lr = 0.0005518331411329647
epoch 48 iter 63600: loss = 1.7280, smooth loss = 1.5676, ce loss = 1.7280, contrastive loss = 0.0000, lr = 0.000541842987449195
epoch 49 iter 64925: loss = 1.5525, smooth loss = 1.5727, ce loss = 1.5525, contrastive loss = 0.0000, lr = 0.0005317505739748281
epoch 50 iter 66250: loss = 1.5460, smooth loss = 1.5592, ce loss = 1.5460, contrastive loss = 0.0000, lr = 0.0005215631767814466
epoch 51 iter 67575: loss = 1.7234, smooth loss = 1.5599, ce loss = 1.7234, contrastive loss = 0.0000, lr = 0.0005112881404186389
epoch 52 iter 68900: loss = 1.3649, smooth loss = 1.5506, ce loss = 1.3649, contrastive loss = 0.0000, lr = 0.0005009328726189833
epoch 53 iter 70225: loss = 1.6392, smooth loss = 1.5211, ce loss = 1.6392, contrastive loss = 0.0000, lr = 0.0004905048389574851
epoch 54 iter 71550: loss = 1.4156, smooth loss = 1.5181, ce loss = 1.4156, contrastive loss = 0.0000, lr = 0.00048001155746930777
epoch 55 iter 72875: loss = 1.5014, smooth loss = 1.5184, ce loss = 1.5014, contrastive loss = 0.0000, lr = 0.00046946059322968797
epoch 56 iter 74200: loss = 1.4264, smooth loss = 1.4979, ce loss = 1.4264, contrastive loss = 0.0000, lr = 0.00045885955289993313
epoch 57 iter 75525: loss = 1.4670, smooth loss = 1.5045, ce loss = 1.4670, contrastive loss = 0.0000, lr = 0.0004482160792434408
epoch 58 iter 76850: loss = 1.5923, smooth loss = 1.5075, ce loss = 1.5923, contrastive loss = 0.0000, lr = 0.0004375378456156887
epoch 59 iter 78175: loss = 1.7691, smooth loss = 1.4955, ce loss = 1.7691, contrastive loss = 0.0000, lr = 0.00042683255043216993
epoch 60 iter 79500: loss = 1.4718, smooth loss = 1.4573, ce loss = 1.4718, contrastive loss = 0.0000, lr = 0.0004161079116182619
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.01-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_60_80000
epoch 61 iter 80825: loss = 1.4790, smooth loss = 1.4773, ce loss = 1.4790, contrastive loss = 0.0000, lr = 0.0004053716610450289
epoch 62 iter 82150: loss = 1.4696, smooth loss = 1.4675, ce loss = 1.4696, contrastive loss = 0.0000, lr = 0.0003946315389549712
epoch 63 iter 83475: loss = 1.5023, smooth loss = 1.4482, ce loss = 1.5023, contrastive loss = 0.0000, lr = 0.00038389528838173823
epoch 64 iter 84800: loss = 1.4192, smooth loss = 1.4263, ce loss = 1.4192, contrastive loss = 0.0000, lr = 0.00037317064956783006
epoch 65 iter 86125: loss = 1.4186, smooth loss = 1.4424, ce loss = 1.4186, contrastive loss = 0.0000, lr = 0.0003624653543843114
epoch 66 iter 87450: loss = 1.2731, smooth loss = 1.4140, ce loss = 1.2731, contrastive loss = 0.0000, lr = 0.00035178712075655926
epoch 67 iter 88775: loss = 1.2250, smooth loss = 1.4184, ce loss = 1.2250, contrastive loss = 0.0000, lr = 0.0003411436471000669
epoch 68 iter 90100: loss = 1.1913, smooth loss = 1.4059, ce loss = 1.1913, contrastive loss = 0.0000, lr = 0.0003305426067703122
epoch 69 iter 91425: loss = 1.4976, smooth loss = 1.4026, ce loss = 1.4976, contrastive loss = 0.0000, lr = 0.00031999164253069233
epoch 70 iter 92750: loss = 1.3703, smooth loss = 1.3892, ce loss = 1.3703, contrastive loss = 0.0000, lr = 0.0003094983610425151
epoch 71 iter 94075: loss = 1.4691, smooth loss = 1.3893, ce loss = 1.4691, contrastive loss = 0.0000, lr = 0.0002990703273810167
epoch 72 iter 95400: loss = 1.4461, smooth loss = 1.3928, ce loss = 1.4461, contrastive loss = 0.0000, lr = 0.0002887150595813612
epoch 73 iter 96725: loss = 1.1098, smooth loss = 1.3513, ce loss = 1.1098, contrastive loss = 0.0000, lr = 0.0002784400232185534
epoch 74 iter 98050: loss = 1.3925, smooth loss = 1.3780, ce loss = 1.3925, contrastive loss = 0.0000, lr = 0.0002682526260251721
epoch 75 iter 99375: loss = 1.2934, smooth loss = 1.3477, ce loss = 1.2934, contrastive loss = 0.0000, lr = 0.00025816021255080504
epoch 76 iter 100700: loss = 1.5586, smooth loss = 1.3622, ce loss = 1.5586, contrastive loss = 0.0000, lr = 0.00024817005886703536
average data time = 0.0090s, average running time = 0.4011s
epoch 76 iter 100700: eval loss = 1.0921,  ccr = 0.7690,  cwr = 0.6232,  ted = 68609.0000,  ned = 13469.8294,  ted/w = 1.0780, 
Better model found at epoch 76, iter 100700 with accuracy value: 0.6232.
epoch 77 iter 102025: loss = 1.3218, smooth loss = 1.3545, ce loss = 1.3218, contrastive loss = 0.0000, lr = 0.0002382893673217962
average data time = 0.0090s, average running time = 0.4026s
epoch 77 iter 102025: eval loss = 1.0901,  ccr = 0.7726,  cwr = 0.6253,  ted = 67313.0000,  ned = 13384.1435,  ted/w = 1.0576, 
Better model found at epoch 77, iter 102025 with accuracy value: 0.6253.
epoch 78 iter 103350: loss = 1.3748, smooth loss = 1.3278, ce loss = 1.3748, contrastive loss = 0.0000, lr = 0.00022852526134687652
average data time = 0.0090s, average running time = 0.4040s
epoch 78 iter 103350: eval loss = 1.0969,  ccr = 0.7705,  cwr = 0.6252,  ted = 67589.0000,  ned = 13304.6019,  ted/w = 1.0620, 
epoch 79 iter 104675: loss = 1.4176, smooth loss = 1.3288, ce loss = 1.4176, contrastive loss = 0.0000, lr = 0.00021888478032232176
average data time = 0.0090s, average running time = 0.4054s
epoch 79 iter 104675: eval loss = 1.0876,  ccr = 0.7718,  cwr = 0.6252,  ted = 67564.0000,  ned = 13289.4152,  ted/w = 1.0616, 
epoch 80 iter 106000: loss = 1.4120, smooth loss = 1.3357, ce loss = 1.4120, contrastive loss = 0.0000, lr = 0.0002093748745014289
average data time = 0.0090s, average running time = 0.4068s
epoch 80 iter 106000: eval loss = 1.0830,  ccr = 0.7719,  cwr = 0.6264,  ted = 67007.0000,  ned = 13270.3460,  ted/w = 1.0528, 
Better model found at epoch 80, iter 106000 with accuracy value: 0.6264.
epoch 81 iter 107325: loss = 1.1550, smooth loss = 1.3009, ce loss = 1.1550, contrastive loss = 0.0000, lr = 0.0002000024000000001
average data time = 0.0090s, average running time = 0.4081s
epoch 81 iter 107325: eval loss = 1.0774,  ccr = 0.7746,  cwr = 0.6290,  ted = 66437.0000,  ned = 13195.5072,  ted/w = 1.0439, 
Better model found at epoch 81, iter 107325 with accuracy value: 0.6290.
epoch 82 iter 108650: loss = 1.3799, smooth loss = 1.2828, ce loss = 1.3799, contrastive loss = 0.0000, lr = 0.00019077411385346127
average data time = 0.0090s, average running time = 0.4094s
epoch 82 iter 108650: eval loss = 1.0854,  ccr = 0.7741,  cwr = 0.6294,  ted = 66539.0000,  ned = 13175.1777,  ted/w = 1.0455, 
Better model found at epoch 82, iter 108650 with accuracy value: 0.6294.
epoch 83 iter 109975: loss = 1.0855, smooth loss = 1.3027, ce loss = 1.0855, contrastive loss = 0.0000, lr = 0.00018169666914541447
average data time = 0.0090s, average running time = 0.4107s
epoch 83 iter 109975: eval loss = 1.0702,  ccr = 0.7740,  cwr = 0.6303,  ted = 66398.0000,  ned = 13101.4825,  ted/w = 1.0433, 
Better model found at epoch 83, iter 109975 with accuracy value: 0.6303.
epoch 84 iter 111300: loss = 1.3056, smooth loss = 1.2931, ce loss = 1.3056, contrastive loss = 0.0000, lr = 0.0001727766102111325
average data time = 0.0090s, average running time = 0.4119s
epoch 84 iter 111300: eval loss = 1.0840,  ccr = 0.7742,  cwr = 0.6316,  ted = 66310.0000,  ned = 13095.4106,  ted/w = 1.0419, 
Better model found at epoch 84, iter 111300 with accuracy value: 0.6316.
epoch 85 iter 112625: loss = 1.1353, smooth loss = 1.2857, ce loss = 1.1353, contrastive loss = 0.0000, lr = 0.000164020367919455
average data time = 0.0090s, average running time = 0.4131s
epoch 85 iter 112625: eval loss = 1.0729,  ccr = 0.7766,  cwr = 0.6315,  ted = 66432.0000,  ned = 13142.7474,  ted/w = 1.0438, 
epoch 86 iter 113950: loss = 1.1447, smooth loss = 1.2798, ce loss = 1.1447, contrastive loss = 0.0000, lr = 0.00015543425503648805
average data time = 0.0090s, average running time = 0.4142s
epoch 86 iter 113950: eval loss = 1.0784,  ccr = 0.7758,  cwr = 0.6335,  ted = 65779.0000,  ned = 12961.4142,  ted/w = 1.0335, 
Better model found at epoch 86, iter 113950 with accuracy value: 0.6335.
epoch 87 iter 115275: loss = 1.6135, smooth loss = 1.3013, ce loss = 1.6135, contrastive loss = 0.0000, lr = 0.0001470244616744501
average data time = 0.0090s, average running time = 0.4154s
epoch 87 iter 115275: eval loss = 1.0645,  ccr = 0.7772,  cwr = 0.6335,  ted = 65376.0000,  ned = 12973.6370,  ted/w = 1.0272, 
epoch 88 iter 116600: loss = 1.1447, smooth loss = 1.2573, ce loss = 1.1447, contrastive loss = 0.0000, lr = 0.00013879705082894204
average data time = 0.0090s, average running time = 0.4165s
epoch 88 iter 116600: eval loss = 1.0766,  ccr = 0.7759,  cwr = 0.6307,  ted = 65774.0000,  ned = 13095.4974,  ted/w = 1.0335, 
epoch 89 iter 117925: loss = 1.0542, smooth loss = 1.2652, ce loss = 1.0542, contrastive loss = 0.0000, lr = 0.00013075795400786374
average data time = 0.0090s, average running time = 0.4175s
epoch 89 iter 117925: eval loss = 1.0779,  ccr = 0.7753,  cwr = 0.6317,  ted = 66137.0000,  ned = 13090.2805,  ted/w = 1.0392, 
epoch 90 iter 119250: loss = 1.3280, smooth loss = 1.2642, ce loss = 1.3280, contrastive loss = 0.0000, lr = 0.00012291296695512586
average data time = 0.0090s, average running time = 0.4186s
epoch 90 iter 119250: eval loss = 1.0782,  ccr = 0.7764,  cwr = 0.6336,  ted = 65367.0000,  ned = 12951.5432,  ted/w = 1.0271, 
Better model found at epoch 90, iter 119250 with accuracy value: 0.6336.
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.01-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_90_120000
epoch 91 iter 120575: loss = 1.4039, smooth loss = 1.2738, ce loss = 1.4039, contrastive loss = 0.0000, lr = 0.00011526774547223771
average data time = 0.0090s, average running time = 0.4197s
epoch 91 iter 120575: eval loss = 1.0657,  ccr = 0.7790,  cwr = 0.6368,  ted = 64865.0000,  ned = 12878.4939,  ted/w = 1.0192, 
Better model found at epoch 91, iter 120575 with accuracy value: 0.6368.
epoch 92 iter 121900: loss = 1.3619, smooth loss = 1.2505, ce loss = 1.3619, contrastive loss = 0.0000, lr = 0.00010782780134078822
average data time = 0.0090s, average running time = 0.4207s
epoch 92 iter 121900: eval loss = 1.0677,  ccr = 0.7785,  cwr = 0.6354,  ted = 64912.0000,  ned = 12863.9785,  ted/w = 1.0199, 
epoch 93 iter 123225: loss = 1.3835, smooth loss = 1.2529, ce loss = 1.3835, contrastive loss = 0.0000, lr = 0.00010059849834875659
average data time = 0.0090s, average running time = 0.4217s
epoch 93 iter 123225: eval loss = 1.0669,  ccr = 0.7790,  cwr = 0.6352,  ted = 64960.0000,  ned = 12880.4334,  ted/w = 1.0207, 
epoch 94 iter 124550: loss = 1.2113, smooth loss = 1.2227, ce loss = 1.2113, contrastive loss = 0.0000, lr = 9.358504842351783e-05
average data time = 0.0090s, average running time = 0.4227s
epoch 94 iter 124550: eval loss = 1.0745,  ccr = 0.7793,  cwr = 0.6379,  ted = 64255.0000,  ned = 12732.5375,  ted/w = 1.0096, 
Better model found at epoch 94, iter 124550 with accuracy value: 0.6379.
epoch 95 iter 125875: loss = 1.3224, smooth loss = 1.2455, ce loss = 1.3224, contrastive loss = 0.0000, lr = 8.679250787433099e-05
average data time = 0.0090s, average running time = 0.4237s
epoch 95 iter 125875: eval loss = 1.0666,  ccr = 0.7789,  cwr = 0.6360,  ted = 65246.0000,  ned = 12890.3583,  ted/w = 1.0252, 
epoch 96 iter 127200: loss = 1.3764, smooth loss = 1.2447, ce loss = 1.3764, contrastive loss = 0.0000, lr = 8.022577374702106e-05
average data time = 0.0090s, average running time = 0.4246s
epoch 96 iter 127200: eval loss = 1.0659,  ccr = 0.7799,  cwr = 0.6372,  ted = 64480.0000,  ned = 12763.6583,  ted/w = 1.0131, 
epoch 97 iter 128525: loss = 1.1353, smooth loss = 1.2025, ce loss = 1.1353, contrastive loss = 0.0000, lr = 7.388958029347893e-05
average data time = 0.0090s, average running time = 0.4255s
epoch 97 iter 128525: eval loss = 1.0705,  ccr = 0.7796,  cwr = 0.6379,  ted = 64313.0000,  ned = 12754.1024,  ted/w = 1.0105, 
Better model found at epoch 97, iter 128525 with accuracy value: 0.6379.
epoch 98 iter 129850: loss = 1.1674, smooth loss = 1.2160, ce loss = 1.1674, contrastive loss = 0.0000, lr = 6.778849555852853e-05
average data time = 0.0090s, average running time = 0.4264s
epoch 98 iter 129850: eval loss = 1.0640,  ccr = 0.7805,  cwr = 0.6399,  ted = 64285.0000,  ned = 12719.5718,  ted/w = 1.0101, 
Better model found at epoch 98, iter 129850 with accuracy value: 0.6399.
epoch 99 iter 131175: loss = 1.1734, smooth loss = 1.2173, ce loss = 1.1734, contrastive loss = 0.0000, lr = 6.192691808661902e-05
average data time = 0.0090s, average running time = 0.4273s
epoch 99 iter 131175: eval loss = 1.0659,  ccr = 0.7805,  cwr = 0.6378,  ted = 64596.0000,  ned = 12791.3732,  ted/w = 1.0149, 
epoch 100 iter 132500: loss = 1.1235, smooth loss = 1.2174, ce loss = 1.1235, contrastive loss = 0.0000, lr = 5.630907375071737e-05
average data time = 0.0090s, average running time = 0.4284s
epoch 100 iter 132500: eval loss = 1.0625,  ccr = 0.7805,  cwr = 0.6389,  ted = 64169.0000,  ned = 12698.2500,  ted/w = 1.0082, 
epoch 101 iter 133825: loss = 1.0709, smooth loss = 1.2228, ce loss = 1.0709, contrastive loss = 0.0000, lr = 5.093901270568848e-05
average data time = 0.0090s, average running time = 0.4293s
epoch 101 iter 133825: eval loss = 1.0640,  ccr = 0.7812,  cwr = 0.6396,  ted = 64225.0000,  ned = 12716.4219,  ted/w = 1.0091, 
epoch 102 iter 135150: loss = 1.0401, smooth loss = 1.2079, ce loss = 1.0401, contrastive loss = 0.0000, lr = 4.582060646835713e-05
average data time = 0.0090s, average running time = 0.4302s
epoch 102 iter 135150: eval loss = 1.0680,  ccr = 0.7804,  cwr = 0.6390,  ted = 64079.0000,  ned = 12721.9593,  ted/w = 1.0068, 
epoch 103 iter 136475: loss = 1.0824, smooth loss = 1.2056, ce loss = 1.0824, contrastive loss = 0.0000, lr = 4.09575451263587e-05
average data time = 0.0090s, average running time = 0.4310s
epoch 103 iter 136475: eval loss = 1.0637,  ccr = 0.7811,  cwr = 0.6396,  ted = 64005.0000,  ned = 12694.2546,  ted/w = 1.0057, 
epoch 104 iter 137800: loss = 1.1900, smooth loss = 1.2031, ce loss = 1.1900, contrastive loss = 0.0000, lr = 3.635333467779016e-05
average data time = 0.0090s, average running time = 0.4318s
epoch 104 iter 137800: eval loss = 1.0664,  ccr = 0.7811,  cwr = 0.6401,  ted = 63904.0000,  ned = 12687.0696,  ted/w = 1.0041, 
Better model found at epoch 104, iter 137800 with accuracy value: 0.6401.
epoch 105 iter 139125: loss = 1.0157, smooth loss = 1.2093, ce loss = 1.0157, contrastive loss = 0.0000, lr = 3.201129450358016e-05
average data time = 0.0091s, average running time = 0.4326s
epoch 105 iter 139125: eval loss = 1.0648,  ccr = 0.7812,  cwr = 0.6404,  ted = 63650.0000,  ned = 12614.0305,  ted/w = 1.0001, 
Better model found at epoch 105, iter 139125 with accuracy value: 0.6404.
epoch 106 iter 140450: loss = 1.2554, smooth loss = 1.2001, ce loss = 1.2554, contrastive loss = 0.0000, lr = 2.7934554974397916e-05
average data time = 0.0090s, average running time = 0.4334s
epoch 106 iter 140450: eval loss = 1.0668,  ccr = 0.7810,  cwr = 0.6397,  ted = 63861.0000,  ned = 12678.8533,  ted/w = 1.0034, 
epoch 107 iter 141775: loss = 1.1444, smooth loss = 1.2106, ce loss = 1.1444, contrastive loss = 0.0000, lr = 2.412605519382993e-05
average data time = 0.0091s, average running time = 0.4342s
epoch 107 iter 141775: eval loss = 1.0681,  ccr = 0.7809,  cwr = 0.6407,  ted = 63758.0000,  ned = 12658.7489,  ted/w = 1.0018, 
Better model found at epoch 107, iter 141775 with accuracy value: 0.6407.
epoch 108 iter 143100: loss = 1.1576, smooth loss = 1.1976, ce loss = 1.1576, contrastive loss = 0.0000, lr = 2.0588540879448922e-05
average data time = 0.0090s, average running time = 0.4349s
epoch 108 iter 143100: eval loss = 1.0618,  ccr = 0.7818,  cwr = 0.6418,  ted = 63790.0000,  ned = 12639.3441,  ted/w = 1.0023, 
Better model found at epoch 108, iter 143100 with accuracy value: 0.6418.
epoch 109 iter 144425: loss = 1.1966, smooth loss = 1.1791, ce loss = 1.1966, contrastive loss = 0.0000, lr = 1.7324562383303276e-05
average data time = 0.0090s, average running time = 0.4357s
epoch 109 iter 144425: eval loss = 1.0643,  ccr = 0.7826,  cwr = 0.6419,  ted = 63321.0000,  ned = 12589.5468,  ted/w = 0.9949, 
Better model found at epoch 109, iter 144425 with accuracy value: 0.6419.
epoch 110 iter 145750: loss = 1.2231, smooth loss = 1.1757, ce loss = 1.2231, contrastive loss = 0.0000, lr = 1.4336472853254332e-05
average data time = 0.0090s, average running time = 0.4364s
epoch 110 iter 145750: eval loss = 1.0657,  ccr = 0.7823,  cwr = 0.6419,  ted = 63498.0000,  ned = 12598.5238,  ted/w = 0.9977, 
epoch 111 iter 147075: loss = 1.0419, smooth loss = 1.1877, ce loss = 1.0419, contrastive loss = 0.0000, lr = 1.1626426536487078e-05
average data time = 0.0090s, average running time = 0.4371s
epoch 111 iter 147075: eval loss = 1.0647,  ccr = 0.7819,  cwr = 0.6416,  ted = 63726.0000,  ned = 12623.1340,  ted/w = 1.0013, 
epoch 112 iter 148400: loss = 1.2349, smooth loss = 1.1862, ce loss = 1.2349, contrastive loss = 0.0000, lr = 9.196377226417202e-06
average data time = 0.0091s, average running time = 0.4378s
epoch 112 iter 148400: eval loss = 1.0619,  ccr = 0.7818,  cwr = 0.6416,  ted = 63798.0000,  ned = 12630.4275,  ted/w = 1.0024, 
epoch 113 iter 149725: loss = 1.1925, smooth loss = 1.1759, ce loss = 1.1925, contrastive loss = 0.0000, lr = 7.04807685411396e-06
average data time = 0.0090s, average running time = 0.4385s
epoch 113 iter 149725: eval loss = 1.0631,  ccr = 0.7826,  cwr = 0.6424,  ted = 63555.0000,  ned = 12603.5375,  ted/w = 0.9986, 
Better model found at epoch 113, iter 149725 with accuracy value: 0.6424.
epoch 114 iter 151050: loss = 1.2120, smooth loss = 1.1812, ce loss = 1.2120, contrastive loss = 0.0000, lr = 5.183074225255083e-06
average data time = 0.0090s, average running time = 0.4391s
epoch 114 iter 151050: eval loss = 1.0649,  ccr = 0.7823,  cwr = 0.6419,  ted = 63576.0000,  ned = 12619.9029,  ted/w = 0.9989, 
epoch 115 iter 152375: loss = 1.2092, smooth loss = 1.1803, ce loss = 1.2092, contrastive loss = 0.0000, lr = 3.6027139035234053e-06
average data time = 0.0090s, average running time = 0.4397s
epoch 115 iter 152375: eval loss = 1.0643,  ccr = 0.7826,  cwr = 0.6423,  ted = 63495.0000,  ned = 12602.9443,  ted/w = 0.9976, 
epoch 116 iter 153700: loss = 0.9740, smooth loss = 1.1817, ce loss = 0.9740, contrastive loss = 0.0000, lr = 2.308135241251002e-06
average data time = 0.0090s, average running time = 0.4404s
epoch 116 iter 153700: eval loss = 1.0606,  ccr = 0.7823,  cwr = 0.6428,  ted = 63485.0000,  ned = 12578.3032,  ted/w = 0.9975, 
Better model found at epoch 116, iter 153700 with accuracy value: 0.6428.
epoch 117 iter 155025: loss = 1.2425, smooth loss = 1.1682, ce loss = 1.2425, contrastive loss = 0.0000, lr = 1.300271558009043e-06
average data time = 0.0090s, average running time = 0.4410s
epoch 117 iter 155025: eval loss = 1.0667,  ccr = 0.7826,  cwr = 0.6425,  ted = 63517.0000,  ned = 12603.4083,  ted/w = 0.9980, 
epoch 118 iter 156350: loss = 1.1796, smooth loss = 1.1707, ce loss = 1.1796, contrastive loss = 0.0000, lr = 5.79849467736198e-07
average data time = 0.0090s, average running time = 0.4416s
epoch 118 iter 156350: eval loss = 1.0662,  ccr = 0.7821,  cwr = 0.6420,  ted = 63612.0000,  ned = 12597.7762,  ted/w = 0.9995, 
epoch 119 iter 157675: loss = 1.0747, smooth loss = 1.1846, ce loss = 1.0747, contrastive loss = 0.0000, lr = 1.4738835489012343e-07
average data time = 0.0090s, average running time = 0.4421s
epoch 119 iter 157675: eval loss = 1.0633,  ccr = 0.7829,  cwr = 0.6429,  ted = 63387.0000,  ned = 12568.7279,  ted/w = 0.9959, 
Better model found at epoch 119, iter 157675 with accuracy value: 0.6429.
