You have chosen to seed training. This will slow down your training!
Construct dataset.
112471 training items found.
14059 valid items found.
Construct model.
Model(
  (backbone): ResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (decoder): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(size=(4, 32), mode=nearest)
        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=7935, bias=True)
)
The parameters size of model is 19.101023 MB
Construct learner.
Use 4 GPUs.
Start training.
epoch 1 iter 292: loss = 5.2606, smooth loss = 5.1836, ce loss = 5.2606, contrastive loss = 0.0000, lr = 0.00022400000000000002
epoch 2 iter 584: loss = 5.0152, smooth loss = 4.9729, ce loss = 5.0152, contrastive loss = 0.0000, lr = 0.0006079999999999999
epoch 3 iter 876: loss = 4.0807, smooth loss = 4.2663, ce loss = 4.0807, contrastive loss = 0.0000, lr = 0.0008
epoch 4 iter 1168: loss = 3.4081, smooth loss = 3.4499, ce loss = 3.4081, contrastive loss = 0.0000, lr = 0.0007998558116451099
epoch 5 iter 1460: loss = 3.1348, smooth loss = 3.0506, ce loss = 3.1348, contrastive loss = 0.0000, lr = 0.0007994233505322638
epoch 6 iter 1752: loss = 2.5754, smooth loss = 2.8113, ce loss = 2.5754, contrastive loss = 0.0000, lr = 0.000798702928441991
epoch 7 iter 2044: loss = 2.5865, smooth loss = 2.6485, ce loss = 2.5865, contrastive loss = 0.0000, lr = 0.000797695064758749
epoch 8 iter 2336: loss = 2.3879, smooth loss = 2.5439, ce loss = 2.3879, contrastive loss = 0.0000, lr = 0.0007964004860964767
epoch 9 iter 2628: loss = 2.5194, smooth loss = 2.4485, ce loss = 2.5194, contrastive loss = 0.0000, lr = 0.0007948201257747449
epoch 10 iter 2920: loss = 2.3194, smooth loss = 2.3766, ce loss = 2.3194, contrastive loss = 0.0000, lr = 0.000792955123145886
epoch 11 iter 3212: loss = 2.2068, smooth loss = 2.3321, ce loss = 2.2068, contrastive loss = 0.0000, lr = 0.0007908068227735828
epoch 12 iter 3504: loss = 2.1800, smooth loss = 2.2637, ce loss = 2.1800, contrastive loss = 0.0000, lr = 0.000788376773463513
epoch 13 iter 3796: loss = 2.1022, smooth loss = 2.2091, ce loss = 2.1022, contrastive loss = 0.0000, lr = 0.0007856667271467458
epoch 14 iter 4088: loss = 2.2032, smooth loss = 2.1680, ce loss = 2.2032, contrastive loss = 0.0000, lr = 0.0007826786376166968
epoch 15 iter 4380: loss = 1.9668, smooth loss = 2.1384, ce loss = 1.9668, contrastive loss = 0.0000, lr = 0.0007794146591205511
epoch 16 iter 4672: loss = 1.9713, smooth loss = 2.0890, ce loss = 1.9713, contrastive loss = 0.0000, lr = 0.0007758771448061701
epoch 17 iter 4964: loss = 2.0471, smooth loss = 2.0727, ce loss = 2.0471, contrastive loss = 0.0000, lr = 0.0007720686450256023
epoch 18 iter 5256: loss = 1.9426, smooth loss = 2.0395, ce loss = 1.9426, contrastive loss = 0.0000, lr = 0.0007679919054964199
epoch 19 iter 5548: loss = 1.8326, smooth loss = 1.9982, ce loss = 1.8326, contrastive loss = 0.0000, lr = 0.0007636498653222099
epoch 20 iter 5840: loss = 1.9282, smooth loss = 1.9901, ce loss = 1.9282, contrastive loss = 0.0000, lr = 0.0007590456548736415
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.15-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_20_5840
epoch 21 iter 6132: loss = 1.8628, smooth loss = 1.9592, ce loss = 1.8628, contrastive loss = 0.0000, lr = 0.0007541825935316429
epoch 22 iter 6424: loss = 1.7782, smooth loss = 1.9329, ce loss = 1.7782, contrastive loss = 0.0000, lr = 0.0007490641872943116
epoch 23 iter 6716: loss = 1.7070, smooth loss = 1.9327, ce loss = 1.7070, contrastive loss = 0.0000, lr = 0.0007436941262492827
epoch 24 iter 7008: loss = 1.7709, smooth loss = 1.8941, ce loss = 1.7709, contrastive loss = 0.0000, lr = 0.0007380762819133811
epoch 25 iter 7300: loss = 1.6897, smooth loss = 1.8868, ce loss = 1.6897, contrastive loss = 0.0000, lr = 0.0007322147044414715
epoch 26 iter 7592: loss = 1.7606, smooth loss = 1.8490, ce loss = 1.7606, contrastive loss = 0.0000, lr = 0.0007261136197065211
epoch 27 iter 7884: loss = 1.7377, smooth loss = 1.8522, ce loss = 1.7377, contrastive loss = 0.0000, lr = 0.0007197774262529791
epoch 28 iter 8176: loss = 1.8188, smooth loss = 1.8448, ce loss = 1.8188, contrastive loss = 0.0000, lr = 0.0007132106921256691
epoch 29 iter 8468: loss = 1.7846, smooth loss = 1.7998, ce loss = 1.7846, contrastive loss = 0.0000, lr = 0.0007064181515764822
epoch 30 iter 8760: loss = 1.6404, smooth loss = 1.8131, ce loss = 1.6404, contrastive loss = 0.0000, lr = 0.0006994047016512434
epoch 31 iter 9052: loss = 1.6552, smooth loss = 1.7778, ce loss = 1.6552, contrastive loss = 0.0000, lr = 0.0006921753986592118
epoch 32 iter 9344: loss = 1.7166, smooth loss = 1.7651, ce loss = 1.7166, contrastive loss = 0.0000, lr = 0.0006847354545277624
epoch 33 iter 9636: loss = 1.6997, smooth loss = 1.7662, ce loss = 1.6997, contrastive loss = 0.0000, lr = 0.0006770902330448742
epoch 34 iter 9928: loss = 1.8295, smooth loss = 1.7551, ce loss = 1.8295, contrastive loss = 0.0000, lr = 0.0006692452459921362
epoch 35 iter 10220: loss = 1.5493, smooth loss = 1.7180, ce loss = 1.5493, contrastive loss = 0.0000, lr = 0.000661206149171058
epoch 36 iter 10512: loss = 1.7531, smooth loss = 1.7215, ce loss = 1.7531, contrastive loss = 0.0000, lr = 0.0006529787383255499
epoch 37 iter 10804: loss = 1.7139, smooth loss = 1.7010, ce loss = 1.7139, contrastive loss = 0.0000, lr = 0.0006445689449635119
epoch 38 iter 11096: loss = 1.5936, smooth loss = 1.6609, ce loss = 1.5936, contrastive loss = 0.0000, lr = 0.0006359828320805452
epoch 39 iter 11388: loss = 1.4396, smooth loss = 1.6621, ce loss = 1.4396, contrastive loss = 0.0000, lr = 0.0006272265897888675
epoch 40 iter 11680: loss = 2.0384, smooth loss = 1.6571, ce loss = 2.0384, contrastive loss = 0.0000, lr = 0.0006183065308545855
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.15-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_40_11680
epoch 41 iter 11972: loss = 1.5634, smooth loss = 1.6651, ce loss = 1.5634, contrastive loss = 0.0000, lr = 0.0006092290861465388
epoch 42 iter 12264: loss = 1.6469, smooth loss = 1.6601, ce loss = 1.6469, contrastive loss = 0.0000, lr = 0.0006000008
epoch 43 iter 12556: loss = 1.3651, smooth loss = 1.6167, ce loss = 1.3651, contrastive loss = 0.0000, lr = 0.0005906283254985711
epoch 44 iter 12848: loss = 1.4965, smooth loss = 1.5973, ce loss = 1.4965, contrastive loss = 0.0000, lr = 0.0005811184196776785
epoch 45 iter 13140: loss = 1.5350, smooth loss = 1.5997, ce loss = 1.5350, contrastive loss = 0.0000, lr = 0.0005714779386531235
epoch 46 iter 13432: loss = 1.3906, smooth loss = 1.5886, ce loss = 1.3906, contrastive loss = 0.0000, lr = 0.0005617138326782039
epoch 47 iter 13724: loss = 1.5737, smooth loss = 1.5786, ce loss = 1.5737, contrastive loss = 0.0000, lr = 0.0005518331411329647
epoch 48 iter 14016: loss = 1.5766, smooth loss = 1.5741, ce loss = 1.5766, contrastive loss = 0.0000, lr = 0.000541842987449195
epoch 49 iter 14308: loss = 1.3722, smooth loss = 1.5355, ce loss = 1.3722, contrastive loss = 0.0000, lr = 0.0005317505739748281
epoch 50 iter 14600: loss = 1.5049, smooth loss = 1.5508, ce loss = 1.5049, contrastive loss = 0.0000, lr = 0.0005215631767814466
epoch 51 iter 14892: loss = 1.4770, smooth loss = 1.5534, ce loss = 1.4770, contrastive loss = 0.0000, lr = 0.0005112881404186389
epoch 52 iter 15184: loss = 1.3532, smooth loss = 1.5531, ce loss = 1.3532, contrastive loss = 0.0000, lr = 0.0005009328726189833
epoch 53 iter 15476: loss = 1.6552, smooth loss = 1.5328, ce loss = 1.6552, contrastive loss = 0.0000, lr = 0.0004905048389574851
epoch 54 iter 15768: loss = 1.5271, smooth loss = 1.5082, ce loss = 1.5271, contrastive loss = 0.0000, lr = 0.00048001155746930777
epoch 55 iter 16060: loss = 1.2983, smooth loss = 1.5066, ce loss = 1.2983, contrastive loss = 0.0000, lr = 0.00046946059322968797
epoch 56 iter 16352: loss = 1.5710, smooth loss = 1.4902, ce loss = 1.5710, contrastive loss = 0.0000, lr = 0.00045885955289993313
epoch 57 iter 16644: loss = 1.2987, smooth loss = 1.4644, ce loss = 1.2987, contrastive loss = 0.0000, lr = 0.0004482160792434408
epoch 58 iter 16936: loss = 1.4874, smooth loss = 1.4648, ce loss = 1.4874, contrastive loss = 0.0000, lr = 0.0004375378456156887
epoch 59 iter 17228: loss = 1.3934, smooth loss = 1.4526, ce loss = 1.3934, contrastive loss = 0.0000, lr = 0.00042683255043216993
epoch 60 iter 17520: loss = 1.5177, smooth loss = 1.4590, ce loss = 1.5177, contrastive loss = 0.0000, lr = 0.0004161079116182619
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.15-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_60_17520
epoch 61 iter 17812: loss = 1.3512, smooth loss = 1.4366, ce loss = 1.3512, contrastive loss = 0.0000, lr = 0.0004053716610450289
epoch 62 iter 18104: loss = 1.5069, smooth loss = 1.4279, ce loss = 1.5069, contrastive loss = 0.0000, lr = 0.0003946315389549712
epoch 63 iter 18396: loss = 1.3718, smooth loss = 1.4177, ce loss = 1.3718, contrastive loss = 0.0000, lr = 0.00038389528838173823
epoch 64 iter 18688: loss = 1.2294, smooth loss = 1.4033, ce loss = 1.2294, contrastive loss = 0.0000, lr = 0.00037317064956783006
epoch 65 iter 18980: loss = 1.4144, smooth loss = 1.4088, ce loss = 1.4144, contrastive loss = 0.0000, lr = 0.0003624653543843114
epoch 66 iter 19272: loss = 1.6357, smooth loss = 1.3813, ce loss = 1.6357, contrastive loss = 0.0000, lr = 0.00035178712075655926
epoch 67 iter 19564: loss = 1.4670, smooth loss = 1.3909, ce loss = 1.4670, contrastive loss = 0.0000, lr = 0.0003411436471000669
epoch 68 iter 19856: loss = 1.1613, smooth loss = 1.3551, ce loss = 1.1613, contrastive loss = 0.0000, lr = 0.0003305426067703122
epoch 69 iter 20148: loss = 1.4115, smooth loss = 1.3336, ce loss = 1.4115, contrastive loss = 0.0000, lr = 0.00031999164253069233
average data time = 0.0191s, average running time = 0.4071s
epoch 69 iter 20148: eval loss = 1.2747,  ccr = 0.7237,  cwr = 0.5855,  ted = 18774.0000,  ned = 3501.1230,  ted/w = 1.3354, 
Better model found at epoch 69, iter 20148 with accuracy value: 0.5855.
epoch 70 iter 20440: loss = 1.2325, smooth loss = 1.3185, ce loss = 1.2325, contrastive loss = 0.0000, lr = 0.0003094983610425151
average data time = 0.0191s, average running time = 0.4090s
epoch 70 iter 20440: eval loss = 1.2931,  ccr = 0.7192,  cwr = 0.5822,  ted = 18190.0000,  ned = 3473.4576,  ted/w = 1.2938, 
epoch 71 iter 20732: loss = 1.2205, smooth loss = 1.3331, ce loss = 1.2205, contrastive loss = 0.0000, lr = 0.0002990703273810167
average data time = 0.0191s, average running time = 0.4107s
epoch 71 iter 20732: eval loss = 1.2694,  ccr = 0.7265,  cwr = 0.5857,  ted = 18269.0000,  ned = 3469.0401,  ted/w = 1.2995, 
Better model found at epoch 71, iter 20732 with accuracy value: 0.5857.
epoch 72 iter 21024: loss = 1.4007, smooth loss = 1.3187, ce loss = 1.4007, contrastive loss = 0.0000, lr = 0.0002887150595813612
average data time = 0.0191s, average running time = 0.4125s
epoch 72 iter 21024: eval loss = 1.2848,  ccr = 0.7252,  cwr = 0.5891,  ted = 17784.0000,  ned = 3442.2270,  ted/w = 1.2650, 
Better model found at epoch 72, iter 21024 with accuracy value: 0.5891.
epoch 73 iter 21316: loss = 1.3601, smooth loss = 1.3029, ce loss = 1.3601, contrastive loss = 0.0000, lr = 0.0002784400232185534
average data time = 0.0190s, average running time = 0.4142s
epoch 73 iter 21316: eval loss = 1.2865,  ccr = 0.7275,  cwr = 0.5890,  ted = 17845.0000,  ned = 3467.3897,  ted/w = 1.2693, 
epoch 74 iter 21608: loss = 1.1798, smooth loss = 1.2922, ce loss = 1.1798, contrastive loss = 0.0000, lr = 0.0002682526260251721
average data time = 0.0190s, average running time = 0.4158s
epoch 74 iter 21608: eval loss = 1.2840,  ccr = 0.7252,  cwr = 0.5886,  ted = 18242.0000,  ned = 3450.4504,  ted/w = 1.2975, 
epoch 75 iter 21900: loss = 1.2618, smooth loss = 1.2628, ce loss = 1.2618, contrastive loss = 0.0000, lr = 0.00025816021255080504
average data time = 0.0190s, average running time = 0.4173s
epoch 75 iter 21900: eval loss = 1.2938,  ccr = 0.7245,  cwr = 0.5921,  ted = 17944.0000,  ned = 3404.6956,  ted/w = 1.2763, 
Better model found at epoch 75, iter 21900 with accuracy value: 0.5921.
epoch 76 iter 22192: loss = 1.0618, smooth loss = 1.2787, ce loss = 1.0618, contrastive loss = 0.0000, lr = 0.00024817005886703536
average data time = 0.0190s, average running time = 0.4189s
epoch 76 iter 22192: eval loss = 1.2799,  ccr = 0.7264,  cwr = 0.5926,  ted = 17753.0000,  ned = 3375.8242,  ted/w = 1.2627, 
Better model found at epoch 76, iter 22192 with accuracy value: 0.5926.
epoch 77 iter 22484: loss = 1.3335, smooth loss = 1.2787, ce loss = 1.3335, contrastive loss = 0.0000, lr = 0.0002382893673217962
average data time = 0.0190s, average running time = 0.4204s
epoch 77 iter 22484: eval loss = 1.2858,  ccr = 0.7275,  cwr = 0.5872,  ted = 17727.0000,  ned = 3381.4260,  ted/w = 1.2609, 
epoch 78 iter 22776: loss = 1.3202, smooth loss = 1.2441, ce loss = 1.3202, contrastive loss = 0.0000, lr = 0.00022852526134687652
average data time = 0.0190s, average running time = 0.4218s
epoch 78 iter 22776: eval loss = 1.2640,  ccr = 0.7318,  cwr = 0.5993,  ted = 17773.0000,  ned = 3343.3653,  ted/w = 1.2642, 
Better model found at epoch 78, iter 22776 with accuracy value: 0.5993.
epoch 79 iter 23068: loss = 1.2431, smooth loss = 1.2277, ce loss = 1.2431, contrastive loss = 0.0000, lr = 0.00021888478032232176
average data time = 0.0190s, average running time = 0.4233s
epoch 79 iter 23068: eval loss = 1.2937,  ccr = 0.7258,  cwr = 0.5939,  ted = 17724.0000,  ned = 3348.8295,  ted/w = 1.2607, 
epoch 80 iter 23360: loss = 1.1652, smooth loss = 1.2279, ce loss = 1.1652, contrastive loss = 0.0000, lr = 0.0002093748745014289
average data time = 0.0190s, average running time = 0.4246s
epoch 80 iter 23360: eval loss = 1.2709,  ccr = 0.7295,  cwr = 0.5986,  ted = 17319.0000,  ned = 3276.9592,  ted/w = 1.2319, 
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.15-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_80_23360
epoch 81 iter 23652: loss = 1.4369, smooth loss = 1.2167, ce loss = 1.4369, contrastive loss = 0.0000, lr = 0.0002000024000000001
average data time = 0.0190s, average running time = 0.4260s
epoch 81 iter 23652: eval loss = 1.2800,  ccr = 0.7283,  cwr = 0.5958,  ted = 17168.0000,  ned = 3273.6339,  ted/w = 1.2211, 
epoch 82 iter 23944: loss = 1.0559, smooth loss = 1.2108, ce loss = 1.0559, contrastive loss = 0.0000, lr = 0.00019077411385346127
average data time = 0.0190s, average running time = 0.4272s
epoch 82 iter 23944: eval loss = 1.2630,  ccr = 0.7343,  cwr = 0.6017,  ted = 17633.0000,  ned = 3355.1461,  ted/w = 1.2542, 
Better model found at epoch 82, iter 23944 with accuracy value: 0.6017.
epoch 83 iter 24236: loss = 1.2370, smooth loss = 1.1895, ce loss = 1.2370, contrastive loss = 0.0000, lr = 0.00018169666914541447
average data time = 0.0190s, average running time = 0.4285s
epoch 83 iter 24236: eval loss = 1.2844,  ccr = 0.7336,  cwr = 0.6031,  ted = 16960.0000,  ned = 3244.3531,  ted/w = 1.2063, 
Better model found at epoch 83, iter 24236 with accuracy value: 0.6031.
epoch 84 iter 24528: loss = 1.1955, smooth loss = 1.1963, ce loss = 1.1955, contrastive loss = 0.0000, lr = 0.0001727766102111325
average data time = 0.0190s, average running time = 0.4299s
epoch 84 iter 24528: eval loss = 1.2567,  ccr = 0.7343,  cwr = 0.6025,  ted = 17229.0000,  ned = 3295.3423,  ted/w = 1.2255, 
epoch 85 iter 24820: loss = 1.0573, smooth loss = 1.1700, ce loss = 1.0573, contrastive loss = 0.0000, lr = 0.000164020367919455
average data time = 0.0190s, average running time = 0.4310s
epoch 85 iter 24820: eval loss = 1.2687,  ccr = 0.7343,  cwr = 0.6037,  ted = 17090.0000,  ned = 3224.5138,  ted/w = 1.2156, 
Better model found at epoch 85, iter 24820 with accuracy value: 0.6037.
epoch 86 iter 25112: loss = 0.9794, smooth loss = 1.1712, ce loss = 0.9794, contrastive loss = 0.0000, lr = 0.00015543425503648805
average data time = 0.0190s, average running time = 0.4324s
epoch 86 iter 25112: eval loss = 1.2768,  ccr = 0.7355,  cwr = 0.6026,  ted = 17110.0000,  ned = 3300.7629,  ted/w = 1.2170, 
epoch 87 iter 25404: loss = 0.9392, smooth loss = 1.1412, ce loss = 0.9392, contrastive loss = 0.0000, lr = 0.0001470244616744501
average data time = 0.0190s, average running time = 0.4337s
epoch 87 iter 25404: eval loss = 1.2643,  ccr = 0.7357,  cwr = 0.6057,  ted = 16883.0000,  ned = 3194.7412,  ted/w = 1.2009, 
Better model found at epoch 87, iter 25404 with accuracy value: 0.6057.
epoch 88 iter 25696: loss = 1.2249, smooth loss = 1.1362, ce loss = 1.2249, contrastive loss = 0.0000, lr = 0.00013879705082894204
average data time = 0.0190s, average running time = 0.4349s
epoch 88 iter 25696: eval loss = 1.2404,  ccr = 0.7393,  cwr = 0.6091,  ted = 16877.0000,  ned = 3196.6369,  ted/w = 1.2004, 
Better model found at epoch 88, iter 25696 with accuracy value: 0.6091.
epoch 89 iter 25988: loss = 1.1262, smooth loss = 1.1284, ce loss = 1.1262, contrastive loss = 0.0000, lr = 0.00013075795400786374
average data time = 0.0190s, average running time = 0.4362s
epoch 89 iter 25988: eval loss = 1.2488,  ccr = 0.7410,  cwr = 0.6100,  ted = 17033.0000,  ned = 3225.4261,  ted/w = 1.2115, 
Better model found at epoch 89, iter 25988 with accuracy value: 0.6100.
epoch 90 iter 26280: loss = 1.1688, smooth loss = 1.1247, ce loss = 1.1688, contrastive loss = 0.0000, lr = 0.00012291296695512586
average data time = 0.0190s, average running time = 0.4373s
epoch 90 iter 26280: eval loss = 1.2429,  ccr = 0.7396,  cwr = 0.6108,  ted = 16818.0000,  ned = 3180.7231,  ted/w = 1.1962, 
Better model found at epoch 90, iter 26280 with accuracy value: 0.6108.
epoch 91 iter 26572: loss = 1.0571, smooth loss = 1.1173, ce loss = 1.0571, contrastive loss = 0.0000, lr = 0.00011526774547223771
average data time = 0.0190s, average running time = 0.4384s
epoch 91 iter 26572: eval loss = 1.2585,  ccr = 0.7407,  cwr = 0.6117,  ted = 16595.0000,  ned = 3159.7961,  ted/w = 1.1804, 
Better model found at epoch 91, iter 26572 with accuracy value: 0.6117.
epoch 92 iter 26864: loss = 1.2984, smooth loss = 1.1235, ce loss = 1.2984, contrastive loss = 0.0000, lr = 0.00010782780134078822
average data time = 0.0189s, average running time = 0.4395s
epoch 92 iter 26864: eval loss = 1.2711,  ccr = 0.7397,  cwr = 0.6104,  ted = 16507.0000,  ned = 3158.3755,  ted/w = 1.1741, 
epoch 93 iter 27156: loss = 0.9932, smooth loss = 1.0890, ce loss = 0.9932, contrastive loss = 0.0000, lr = 0.00010059849834875659
average data time = 0.0189s, average running time = 0.4404s
epoch 93 iter 27156: eval loss = 1.2560,  ccr = 0.7403,  cwr = 0.6105,  ted = 16746.0000,  ned = 3193.0761,  ted/w = 1.1911, 
epoch 94 iter 27448: loss = 1.0263, smooth loss = 1.0778, ce loss = 1.0263, contrastive loss = 0.0000, lr = 9.358504842351783e-05
average data time = 0.0189s, average running time = 0.4414s
epoch 94 iter 27448: eval loss = 1.2756,  ccr = 0.7377,  cwr = 0.6118,  ted = 16598.0000,  ned = 3152.1173,  ted/w = 1.1806, 
Better model found at epoch 94, iter 27448 with accuracy value: 0.6118.
epoch 95 iter 27740: loss = 0.8582, smooth loss = 1.0724, ce loss = 0.8582, contrastive loss = 0.0000, lr = 8.679250787433099e-05
average data time = 0.0189s, average running time = 0.4424s
epoch 95 iter 27740: eval loss = 1.2707,  ccr = 0.7408,  cwr = 0.6148,  ted = 16383.0000,  ned = 3150.3185,  ted/w = 1.1653, 
Better model found at epoch 95, iter 27740 with accuracy value: 0.6148.
epoch 96 iter 28032: loss = 0.9156, smooth loss = 1.0797, ce loss = 0.9156, contrastive loss = 0.0000, lr = 8.022577374702106e-05
average data time = 0.0189s, average running time = 0.4434s
epoch 96 iter 28032: eval loss = 1.2788,  ccr = 0.7401,  cwr = 0.6117,  ted = 16535.0000,  ned = 3132.9751,  ted/w = 1.1761, 
epoch 97 iter 28324: loss = 1.0066, smooth loss = 1.0558, ce loss = 1.0066, contrastive loss = 0.0000, lr = 7.388958029347893e-05
average data time = 0.0189s, average running time = 0.4443s
epoch 97 iter 28324: eval loss = 1.2673,  ccr = 0.7426,  cwr = 0.6162,  ted = 16706.0000,  ned = 3163.6719,  ted/w = 1.1883, 
Better model found at epoch 97, iter 28324 with accuracy value: 0.6162.
epoch 98 iter 28616: loss = 0.9878, smooth loss = 1.0485, ce loss = 0.9878, contrastive loss = 0.0000, lr = 6.778849555852853e-05
average data time = 0.0189s, average running time = 0.4452s
epoch 98 iter 28616: eval loss = 1.2647,  ccr = 0.7427,  cwr = 0.6141,  ted = 16572.0000,  ned = 3188.3694,  ted/w = 1.1787, 
epoch 99 iter 28908: loss = 1.0256, smooth loss = 1.0475, ce loss = 1.0256, contrastive loss = 0.0000, lr = 6.192691808661902e-05
average data time = 0.0189s, average running time = 0.4461s
epoch 99 iter 28908: eval loss = 1.2659,  ccr = 0.7418,  cwr = 0.6178,  ted = 16351.0000,  ned = 3132.3376,  ted/w = 1.1630, 
Better model found at epoch 99, iter 28908 with accuracy value: 0.6178.
epoch 100 iter 29200: loss = 1.1148, smooth loss = 1.0297, ce loss = 1.1148, contrastive loss = 0.0000, lr = 5.630907375071737e-05
average data time = 0.0188s, average running time = 0.4469s
epoch 100 iter 29200: eval loss = 1.2552,  ccr = 0.7441,  cwr = 0.6185,  ted = 16228.0000,  ned = 3087.7406,  ted/w = 1.1543, 
Better model found at epoch 100, iter 29200 with accuracy value: 0.6185.
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.15-PA-decoder-max-len-40-sup_con-0.0-ce-2.0-temperature-0.15-warm_up-0.025_100_29200
epoch 101 iter 29492: loss = 1.0883, smooth loss = 1.0435, ce loss = 1.0883, contrastive loss = 0.0000, lr = 5.093901270568848e-05
average data time = 0.0188s, average running time = 0.4479s
epoch 101 iter 29492: eval loss = 1.2664,  ccr = 0.7437,  cwr = 0.6159,  ted = 16349.0000,  ned = 3106.1151,  ted/w = 1.1629, 
epoch 102 iter 29784: loss = 0.9921, smooth loss = 1.0299, ce loss = 0.9921, contrastive loss = 0.0000, lr = 4.582060646835713e-05
average data time = 0.0188s, average running time = 0.4486s
epoch 102 iter 29784: eval loss = 1.2537,  ccr = 0.7440,  cwr = 0.6193,  ted = 16119.0000,  ned = 3066.9584,  ted/w = 1.1465, 
Better model found at epoch 102, iter 29784 with accuracy value: 0.6193.
epoch 103 iter 30076: loss = 1.2123, smooth loss = 1.0235, ce loss = 1.2123, contrastive loss = 0.0000, lr = 4.09575451263587e-05
average data time = 0.0188s, average running time = 0.4494s
epoch 103 iter 30076: eval loss = 1.2541,  ccr = 0.7448,  cwr = 0.6200,  ted = 16214.0000,  ned = 3078.5160,  ted/w = 1.1533, 
Better model found at epoch 103, iter 30076 with accuracy value: 0.6200.
epoch 104 iter 30368: loss = 1.0247, smooth loss = 1.0160, ce loss = 1.0247, contrastive loss = 0.0000, lr = 3.635333467779016e-05
average data time = 0.0188s, average running time = 0.4502s
epoch 104 iter 30368: eval loss = 1.2675,  ccr = 0.7470,  cwr = 0.6190,  ted = 16280.0000,  ned = 3102.8744,  ted/w = 1.1580, 
epoch 105 iter 30660: loss = 1.0202, smooth loss = 1.0175, ce loss = 1.0202, contrastive loss = 0.0000, lr = 3.201129450358016e-05
average data time = 0.0188s, average running time = 0.4509s
epoch 105 iter 30660: eval loss = 1.2548,  ccr = 0.7460,  cwr = 0.6185,  ted = 16314.0000,  ned = 3112.8181,  ted/w = 1.1604, 
epoch 106 iter 30952: loss = 1.1377, smooth loss = 1.0082, ce loss = 1.1377, contrastive loss = 0.0000, lr = 2.7934554974397916e-05
average data time = 0.0188s, average running time = 0.4516s
epoch 106 iter 30952: eval loss = 1.2622,  ccr = 0.7461,  cwr = 0.6203,  ted = 16202.0000,  ned = 3111.4779,  ted/w = 1.1524, 
Better model found at epoch 106, iter 30952 with accuracy value: 0.6203.
epoch 107 iter 31244: loss = 1.0411, smooth loss = 1.0002, ce loss = 1.0411, contrastive loss = 0.0000, lr = 2.412605519382993e-05
average data time = 0.0188s, average running time = 0.4523s
epoch 107 iter 31244: eval loss = 1.2628,  ccr = 0.7466,  cwr = 0.6215,  ted = 16230.0000,  ned = 3071.3658,  ted/w = 1.1544, 
Better model found at epoch 107, iter 31244 with accuracy value: 0.6215.
epoch 108 iter 31536: loss = 0.9784, smooth loss = 0.9832, ce loss = 0.9784, contrastive loss = 0.0000, lr = 2.0588540879448922e-05
average data time = 0.0188s, average running time = 0.4531s
epoch 108 iter 31536: eval loss = 1.2670,  ccr = 0.7454,  cwr = 0.6202,  ted = 16135.0000,  ned = 3069.6431,  ted/w = 1.1477, 
epoch 109 iter 31828: loss = 1.0162, smooth loss = 0.9901, ce loss = 1.0162, contrastive loss = 0.0000, lr = 1.7324562383303276e-05
average data time = 0.0187s, average running time = 0.4537s
epoch 109 iter 31828: eval loss = 1.2612,  ccr = 0.7464,  cwr = 0.6217,  ted = 16126.0000,  ned = 3071.9970,  ted/w = 1.1470, 
Better model found at epoch 109, iter 31828 with accuracy value: 0.6217.
epoch 110 iter 32120: loss = 0.9389, smooth loss = 0.9924, ce loss = 0.9389, contrastive loss = 0.0000, lr = 1.4336472853254332e-05
average data time = 0.0187s, average running time = 0.4545s
epoch 110 iter 32120: eval loss = 1.2661,  ccr = 0.7457,  cwr = 0.6195,  ted = 16141.0000,  ned = 3071.5081,  ted/w = 1.1481, 
epoch 111 iter 32412: loss = 0.8829, smooth loss = 1.0016, ce loss = 0.8829, contrastive loss = 0.0000, lr = 1.1626426536487078e-05
average data time = 0.0187s, average running time = 0.4551s
epoch 111 iter 32412: eval loss = 1.2678,  ccr = 0.7464,  cwr = 0.6193,  ted = 16137.0000,  ned = 3074.0960,  ted/w = 1.1478, 
epoch 112 iter 32704: loss = 0.9214, smooth loss = 0.9672, ce loss = 0.9214, contrastive loss = 0.0000, lr = 9.196377226417202e-06
average data time = 0.0187s, average running time = 0.4557s
epoch 112 iter 32704: eval loss = 1.2710,  ccr = 0.7473,  cwr = 0.6220,  ted = 16048.0000,  ned = 3057.0045,  ted/w = 1.1415, 
Better model found at epoch 112, iter 32704 with accuracy value: 0.6220.
epoch 113 iter 32996: loss = 0.9177, smooth loss = 0.9814, ce loss = 0.9177, contrastive loss = 0.0000, lr = 7.04807685411396e-06
average data time = 0.0187s, average running time = 0.4564s
epoch 113 iter 32996: eval loss = 1.2643,  ccr = 0.7475,  cwr = 0.6219,  ted = 16122.0000,  ned = 3065.6258,  ted/w = 1.1467, 
epoch 114 iter 33288: loss = 0.9503, smooth loss = 0.9863, ce loss = 0.9503, contrastive loss = 0.0000, lr = 5.183074225255083e-06
average data time = 0.0187s, average running time = 0.4570s
epoch 114 iter 33288: eval loss = 1.2666,  ccr = 0.7472,  cwr = 0.6227,  ted = 16069.0000,  ned = 3059.7509,  ted/w = 1.1430, 
Better model found at epoch 114, iter 33288 with accuracy value: 0.6227.
epoch 115 iter 33580: loss = 0.9149, smooth loss = 0.9823, ce loss = 0.9149, contrastive loss = 0.0000, lr = 3.6027139035234053e-06
average data time = 0.0187s, average running time = 0.4577s
epoch 115 iter 33580: eval loss = 1.2735,  ccr = 0.7475,  cwr = 0.6219,  ted = 16151.0000,  ned = 3095.3722,  ted/w = 1.1488, 
epoch 116 iter 33872: loss = 1.0520, smooth loss = 0.9932, ce loss = 1.0520, contrastive loss = 0.0000, lr = 2.308135241251002e-06
average data time = 0.0187s, average running time = 0.4582s
epoch 116 iter 33872: eval loss = 1.2663,  ccr = 0.7478,  cwr = 0.6239,  ted = 16056.0000,  ned = 3065.7075,  ted/w = 1.1420, 
Better model found at epoch 116, iter 33872 with accuracy value: 0.6239.
epoch 117 iter 34164: loss = 0.8918, smooth loss = 0.9720, ce loss = 0.8918, contrastive loss = 0.0000, lr = 1.300271558009043e-06
average data time = 0.0187s, average running time = 0.4589s
epoch 117 iter 34164: eval loss = 1.2681,  ccr = 0.7471,  cwr = 0.6227,  ted = 16094.0000,  ned = 3056.3503,  ted/w = 1.1447, 
epoch 118 iter 34456: loss = 1.1996, smooth loss = 0.9850, ce loss = 1.1996, contrastive loss = 0.0000, lr = 5.79849467736198e-07
average data time = 0.0187s, average running time = 0.4595s
epoch 118 iter 34456: eval loss = 1.2625,  ccr = 0.7474,  cwr = 0.6222,  ted = 16056.0000,  ned = 3059.7359,  ted/w = 1.1420, 
epoch 119 iter 34748: loss = 0.9772, smooth loss = 0.9773, ce loss = 0.9772, contrastive loss = 0.0000, lr = 1.4738835489012343e-07
average data time = 0.0187s, average running time = 0.4600s
epoch 119 iter 34748: eval loss = 1.2630,  ccr = 0.7480,  cwr = 0.6232,  ted = 16036.0000,  ned = 3054.9038,  ted/w = 1.1406, 
