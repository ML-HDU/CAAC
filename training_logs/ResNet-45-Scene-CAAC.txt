You have chosen to seed training. This will slow down your training!
Construct dataset.
509164 training items found.
63645 valid items found.
Construct model.
Model(
  (backbone): ResNet(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (decoder): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(size=(4, 32), mode=nearest)
        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=7935, bias=True)
)
The parameters size of model is 19.101023 MB
Construct learner.
Use 4 GPUs.
Start training.
epoch 1 iter 1325: loss = 12.2351, smooth loss = 12.2392, ce loss = 5.2755, contrastive loss = 1.6840, lr = 0.00022400000000000002
epoch 2 iter 2650: loss = 9.0461, smooth loss = 9.0368, ce loss = 3.7253, contrastive loss = 1.5955, lr = 0.0006079999999999999
epoch 3 iter 3975: loss = 7.0316, smooth loss = 7.7009, ce loss = 2.7473, contrastive loss = 1.5371, lr = 0.0008
epoch 4 iter 5300: loss = 7.5648, smooth loss = 7.1262, ce loss = 3.0186, contrastive loss = 1.5277, lr = 0.0007998558116451099
epoch 5 iter 6625: loss = 6.9731, smooth loss = 6.7431, ce loss = 2.7419, contrastive loss = 1.4893, lr = 0.0007994233505322638
epoch 6 iter 7950: loss = 6.1374, smooth loss = 6.4536, ce loss = 2.3310, contrastive loss = 1.4754, lr = 0.000798702928441991
epoch 7 iter 9275: loss = 6.3868, smooth loss = 6.2514, ce loss = 2.4603, contrastive loss = 1.4662, lr = 0.000797695064758749
epoch 8 iter 10600: loss = 5.5024, smooth loss = 6.0487, ce loss = 2.0399, contrastive loss = 1.4225, lr = 0.0007964004860964767
epoch 9 iter 11925: loss = 5.3822, smooth loss = 5.9060, ce loss = 1.9832, contrastive loss = 1.4157, lr = 0.0007948201257747449
epoch 10 iter 13250: loss = 5.7923, smooth loss = 5.7809, ce loss = 2.1813, contrastive loss = 1.4297, lr = 0.000792955123145886
epoch 11 iter 14575: loss = 5.8020, smooth loss = 5.6222, ce loss = 2.1880, contrastive loss = 1.4261, lr = 0.0007908068227735828
epoch 12 iter 15900: loss = 6.0597, smooth loss = 5.5568, ce loss = 2.3083, contrastive loss = 1.4431, lr = 0.000788376773463513
epoch 13 iter 17225: loss = 4.9873, smooth loss = 5.4874, ce loss = 1.8079, contrastive loss = 1.3715, lr = 0.0007856667271467458
epoch 14 iter 18550: loss = 5.1582, smooth loss = 5.3992, ce loss = 1.8815, contrastive loss = 1.3952, lr = 0.0007826786376166968
epoch 15 iter 19875: loss = 4.9555, smooth loss = 5.3554, ce loss = 1.7866, contrastive loss = 1.3823, lr = 0.0007794146591205511
epoch 16 iter 21200: loss = 5.0284, smooth loss = 5.2349, ce loss = 1.8116, contrastive loss = 1.4052, lr = 0.0007758771448061701
epoch 17 iter 22525: loss = 5.2149, smooth loss = 5.2737, ce loss = 1.9128, contrastive loss = 1.3894, lr = 0.0007720686450256023
epoch 18 iter 23850: loss = 5.2191, smooth loss = 5.1373, ce loss = 1.9149, contrastive loss = 1.3893, lr = 0.0007679919054964199
epoch 19 iter 25175: loss = 5.1572, smooth loss = 5.1056, ce loss = 1.8834, contrastive loss = 1.3904, lr = 0.0007636498653222099
epoch 20 iter 26500: loss = 4.9985, smooth loss = 5.0443, ce loss = 1.7872, contrastive loss = 1.4241, lr = 0.0007590456548736415
epoch 21 iter 27825: loss = 4.8390, smooth loss = 4.9700, ce loss = 1.7285, contrastive loss = 1.3821, lr = 0.0007541825935316429
epoch 22 iter 29150: loss = 4.6705, smooth loss = 4.9394, ce loss = 1.6559, contrastive loss = 1.3588, lr = 0.0007490641872943116
epoch 23 iter 30475: loss = 4.9955, smooth loss = 4.9389, ce loss = 1.8053, contrastive loss = 1.3849, lr = 0.0007436941262492827
epoch 24 iter 31800: loss = 5.0146, smooth loss = 4.9389, ce loss = 1.8148, contrastive loss = 1.3849, lr = 0.0007380762819133811
epoch 25 iter 33125: loss = 4.5764, smooth loss = 4.8972, ce loss = 1.5998, contrastive loss = 1.3768, lr = 0.0007322147044414715
epoch 26 iter 34450: loss = 4.5742, smooth loss = 4.7907, ce loss = 1.6014, contrastive loss = 1.3714, lr = 0.0007261136197065211
epoch 27 iter 35775: loss = 4.8872, smooth loss = 4.7984, ce loss = 1.7533, contrastive loss = 1.3806, lr = 0.0007197774262529791
epoch 28 iter 37100: loss = 4.5247, smooth loss = 4.7535, ce loss = 1.5734, contrastive loss = 1.3779, lr = 0.0007132106921256691
epoch 29 iter 38425: loss = 5.1192, smooth loss = 4.7331, ce loss = 1.8609, contrastive loss = 1.3974, lr = 0.0007064181515764822
epoch 30 iter 39750: loss = 4.8297, smooth loss = 4.7006, ce loss = 1.7191, contrastive loss = 1.3916, lr = 0.0006994047016512434
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.01-PA-decoder-max-len-40-sup_con-1.0-ce-2.0-temperature-0.15-warm_up-0.025_30_40000
epoch 31 iter 41075: loss = 4.7348, smooth loss = 4.6380, ce loss = 1.6742, contrastive loss = 1.3864, lr = 0.0006921753986592118
epoch 32 iter 42400: loss = 5.1875, smooth loss = 4.6268, ce loss = 1.8957, contrastive loss = 1.3961, lr = 0.0006847354545277624
epoch 33 iter 43725: loss = 4.9258, smooth loss = 4.6241, ce loss = 1.7631, contrastive loss = 1.3996, lr = 0.0006770902330448742
epoch 34 iter 45050: loss = 5.0718, smooth loss = 4.5918, ce loss = 1.8226, contrastive loss = 1.4266, lr = 0.0006692452459921362
epoch 35 iter 46375: loss = 4.5925, smooth loss = 4.5629, ce loss = 1.5970, contrastive loss = 1.3985, lr = 0.000661206149171058
epoch 36 iter 47700: loss = 4.2506, smooth loss = 4.4778, ce loss = 1.4509, contrastive loss = 1.3488, lr = 0.0006529787383255499
epoch 37 iter 49025: loss = 4.2085, smooth loss = 4.5062, ce loss = 1.4353, contrastive loss = 1.3380, lr = 0.0006445689449635119
epoch 38 iter 50350: loss = 4.4254, smooth loss = 4.4947, ce loss = 1.5380, contrastive loss = 1.3493, lr = 0.0006359828320805452
epoch 39 iter 51675: loss = 4.1267, smooth loss = 4.4650, ce loss = 1.3950, contrastive loss = 1.3367, lr = 0.0006272265897888675
epoch 40 iter 53000: loss = 4.5403, smooth loss = 4.4053, ce loss = 1.5861, contrastive loss = 1.3681, lr = 0.0006183065308545855
epoch 41 iter 54325: loss = 4.2591, smooth loss = 4.4227, ce loss = 1.4580, contrastive loss = 1.3430, lr = 0.0006092290861465388
epoch 42 iter 55650: loss = 4.6297, smooth loss = 4.3935, ce loss = 1.6218, contrastive loss = 1.3860, lr = 0.0006000008
epoch 43 iter 56975: loss = 4.4894, smooth loss = 4.3688, ce loss = 1.5634, contrastive loss = 1.3626, lr = 0.0005906283254985711
epoch 44 iter 58300: loss = 4.1489, smooth loss = 4.3432, ce loss = 1.4092, contrastive loss = 1.3305, lr = 0.0005811184196776785
epoch 45 iter 59625: loss = 4.3717, smooth loss = 4.3297, ce loss = 1.5237, contrastive loss = 1.3243, lr = 0.0005714779386531235
epoch 46 iter 60950: loss = 4.0679, smooth loss = 4.3040, ce loss = 1.3679, contrastive loss = 1.3320, lr = 0.0005617138326782039
epoch 47 iter 62275: loss = 4.1779, smooth loss = 4.2897, ce loss = 1.4120, contrastive loss = 1.3540, lr = 0.0005518331411329647
epoch 48 iter 63600: loss = 4.4225, smooth loss = 4.2774, ce loss = 1.5403, contrastive loss = 1.3418, lr = 0.000541842987449195
epoch 49 iter 64925: loss = 4.0916, smooth loss = 4.2744, ce loss = 1.3834, contrastive loss = 1.3247, lr = 0.0005317505739748281
epoch 50 iter 66250: loss = 4.0840, smooth loss = 4.2269, ce loss = 1.3878, contrastive loss = 1.3085, lr = 0.0005215631767814466
epoch 51 iter 67575: loss = 4.5507, smooth loss = 4.2389, ce loss = 1.5842, contrastive loss = 1.3824, lr = 0.0005112881404186389
epoch 52 iter 68900: loss = 3.9299, smooth loss = 4.1924, ce loss = 1.3044, contrastive loss = 1.3211, lr = 0.0005009328726189833
epoch 53 iter 70225: loss = 4.2845, smooth loss = 4.1751, ce loss = 1.4739, contrastive loss = 1.3367, lr = 0.0004905048389574851
epoch 54 iter 71550: loss = 3.8927, smooth loss = 4.1367, ce loss = 1.2881, contrastive loss = 1.3166, lr = 0.00048001155746930777
epoch 55 iter 72875: loss = 4.1568, smooth loss = 4.1239, ce loss = 1.4132, contrastive loss = 1.3305, lr = 0.00046946059322968797
epoch 56 iter 74200: loss = 3.7337, smooth loss = 4.1117, ce loss = 1.2165, contrastive loss = 1.3007, lr = 0.00045885955289993313
epoch 57 iter 75525: loss = 3.6828, smooth loss = 4.0957, ce loss = 1.1883, contrastive loss = 1.3063, lr = 0.0004482160792434408
epoch 58 iter 76850: loss = 3.9744, smooth loss = 4.1183, ce loss = 1.3307, contrastive loss = 1.3129, lr = 0.0004375378456156887
epoch 59 iter 78175: loss = 4.2094, smooth loss = 4.0752, ce loss = 1.4379, contrastive loss = 1.3335, lr = 0.00042683255043216993
epoch 60 iter 79500: loss = 3.8904, smooth loss = 4.0213, ce loss = 1.2884, contrastive loss = 1.3137, lr = 0.0004161079116182619
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.01-PA-decoder-max-len-40-sup_con-1.0-ce-2.0-temperature-0.15-warm_up-0.025_60_80000
epoch 61 iter 80825: loss = 3.9153, smooth loss = 4.0778, ce loss = 1.3025, contrastive loss = 1.3103, lr = 0.0004053716610450289
epoch 62 iter 82150: loss = 3.8942, smooth loss = 4.0441, ce loss = 1.2811, contrastive loss = 1.3321, lr = 0.0003946315389549712
epoch 63 iter 83475: loss = 4.4158, smooth loss = 4.0091, ce loss = 1.5329, contrastive loss = 1.3500, lr = 0.00038389528838173823
epoch 64 iter 84800: loss = 4.0568, smooth loss = 3.9847, ce loss = 1.3591, contrastive loss = 1.3386, lr = 0.00037317064956783006
epoch 65 iter 86125: loss = 3.9582, smooth loss = 3.9891, ce loss = 1.3129, contrastive loss = 1.3323, lr = 0.0003624653543843114
epoch 66 iter 87450: loss = 3.7327, smooth loss = 3.9714, ce loss = 1.2058, contrastive loss = 1.3211, lr = 0.00035178712075655926
epoch 67 iter 88775: loss = 3.6389, smooth loss = 3.9530, ce loss = 1.1566, contrastive loss = 1.3257, lr = 0.0003411436471000669
epoch 68 iter 90100: loss = 3.3964, smooth loss = 3.9260, ce loss = 1.0653, contrastive loss = 1.2658, lr = 0.0003305426067703122
epoch 69 iter 91425: loss = 3.9388, smooth loss = 3.8975, ce loss = 1.3017, contrastive loss = 1.3354, lr = 0.00031999164253069233
epoch 70 iter 92750: loss = 3.8022, smooth loss = 3.9197, ce loss = 1.2484, contrastive loss = 1.3054, lr = 0.0003094983610425151
epoch 71 iter 94075: loss = 4.1154, smooth loss = 3.8650, ce loss = 1.3947, contrastive loss = 1.3260, lr = 0.0002990703273810167
epoch 72 iter 95400: loss = 3.8137, smooth loss = 3.9003, ce loss = 1.2580, contrastive loss = 1.2977, lr = 0.0002887150595813612
epoch 73 iter 96725: loss = 3.2468, smooth loss = 3.8549, ce loss = 0.9886, contrastive loss = 1.2696, lr = 0.0002784400232185534
epoch 74 iter 98050: loss = 3.9866, smooth loss = 3.8659, ce loss = 1.3327, contrastive loss = 1.3213, lr = 0.0002682526260251721
epoch 75 iter 99375: loss = 3.9767, smooth loss = 3.7899, ce loss = 1.3180, contrastive loss = 1.3407, lr = 0.00025816021255080504
epoch 76 iter 100700: loss = 3.8718, smooth loss = 3.8206, ce loss = 1.2726, contrastive loss = 1.3265, lr = 0.00024817005886703536
average data time = 0.0142s, average running time = 0.7248s
epoch 76 iter 100700: eval loss = 1.0890,  ccr = 0.7782,  cwr = 0.6359,  ted = 64618.0000,  ned = 12996.8921,  ted/w = 1.0153, 
Better model found at epoch 76, iter 100700 with accuracy value: 0.6359.
epoch 77 iter 102025: loss = 3.9001, smooth loss = 3.8208, ce loss = 1.2791, contrastive loss = 1.3419, lr = 0.0002382893673217962
average data time = 0.0142s, average running time = 0.7262s
epoch 77 iter 102025: eval loss = 1.0942,  ccr = 0.7798,  cwr = 0.6356,  ted = 64704.0000,  ned = 13045.7308,  ted/w = 1.0166, 
epoch 78 iter 103350: loss = 3.8083, smooth loss = 3.7647, ce loss = 1.2415, contrastive loss = 1.3252, lr = 0.00022852526134687652
average data time = 0.0142s, average running time = 0.7276s
epoch 78 iter 103350: eval loss = 1.0892,  ccr = 0.7802,  cwr = 0.6387,  ted = 64213.0000,  ned = 12830.6429,  ted/w = 1.0089, 
Better model found at epoch 78, iter 103350 with accuracy value: 0.6387.
epoch 79 iter 104675: loss = 3.9511, smooth loss = 3.7890, ce loss = 1.3163, contrastive loss = 1.3185, lr = 0.00021888478032232176
average data time = 0.0142s, average running time = 0.7292s
epoch 79 iter 104675: eval loss = 1.0884,  ccr = 0.7803,  cwr = 0.6393,  ted = 64056.0000,  ned = 12771.2159,  ted/w = 1.0065, 
Better model found at epoch 79, iter 104675 with accuracy value: 0.6393.
epoch 80 iter 106000: loss = 4.0282, smooth loss = 3.7973, ce loss = 1.3478, contrastive loss = 1.3326, lr = 0.0002093748745014289
average data time = 0.0142s, average running time = 0.7307s
epoch 80 iter 106000: eval loss = 1.0852,  ccr = 0.7809,  cwr = 0.6404,  ted = 64070.0000,  ned = 12801.2190,  ted/w = 1.0067, 
Better model found at epoch 80, iter 106000 with accuracy value: 0.6404.
epoch 81 iter 107325: loss = 3.8040, smooth loss = 3.7083, ce loss = 1.2475, contrastive loss = 1.3091, lr = 0.0002000024000000001
average data time = 0.0142s, average running time = 0.7322s
epoch 81 iter 107325: eval loss = 1.0761,  ccr = 0.7819,  cwr = 0.6411,  ted = 63397.0000,  ned = 12726.5255,  ted/w = 0.9961, 
Better model found at epoch 81, iter 107325 with accuracy value: 0.6411.
epoch 82 iter 108650: loss = 3.8853, smooth loss = 3.7146, ce loss = 1.2858, contrastive loss = 1.3137, lr = 0.00019077411385346127
average data time = 0.0142s, average running time = 0.7337s
epoch 82 iter 108650: eval loss = 1.0761,  ccr = 0.7823,  cwr = 0.6437,  ted = 63058.0000,  ned = 12610.4002,  ted/w = 0.9908, 
Better model found at epoch 82, iter 108650 with accuracy value: 0.6437.
epoch 83 iter 109975: loss = 3.3125, smooth loss = 3.7128, ce loss = 1.0105, contrastive loss = 1.2915, lr = 0.00018169666914541447
average data time = 0.0142s, average running time = 0.7351s
epoch 83 iter 109975: eval loss = 1.0860,  ccr = 0.7809,  cwr = 0.6406,  ted = 63449.0000,  ned = 12704.4034,  ted/w = 0.9969, 
epoch 84 iter 111300: loss = 3.5126, smooth loss = 3.6895, ce loss = 1.1183, contrastive loss = 1.2760, lr = 0.0001727766102111325
average data time = 0.0142s, average running time = 0.7365s
epoch 84 iter 111300: eval loss = 1.0850,  ccr = 0.7830,  cwr = 0.6432,  ted = 63354.0000,  ned = 12681.7511,  ted/w = 0.9954, 
epoch 85 iter 112625: loss = 3.6223, smooth loss = 3.7067, ce loss = 1.1579, contrastive loss = 1.3065, lr = 0.000164020367919455
average data time = 0.0143s, average running time = 0.7379s
epoch 85 iter 112625: eval loss = 1.0825,  ccr = 0.7811,  cwr = 0.6404,  ted = 63719.0000,  ned = 12776.0610,  ted/w = 1.0012, 
epoch 86 iter 113950: loss = 3.4938, smooth loss = 3.6585, ce loss = 1.1042, contrastive loss = 1.2854, lr = 0.00015543425503648805
average data time = 0.0143s, average running time = 0.7392s
epoch 86 iter 113950: eval loss = 1.0819,  ccr = 0.7833,  cwr = 0.6441,  ted = 62906.0000,  ned = 12590.8099,  ted/w = 0.9884, 
Better model found at epoch 86, iter 113950 with accuracy value: 0.6441.
epoch 87 iter 115275: loss = 3.7375, smooth loss = 3.7482, ce loss = 1.2045, contrastive loss = 1.3286, lr = 0.0001470244616744501
average data time = 0.0143s, average running time = 0.7406s
epoch 87 iter 115275: eval loss = 1.0678,  ccr = 0.7845,  cwr = 0.6445,  ted = 62694.0000,  ned = 12616.5819,  ted/w = 0.9851, 
Better model found at epoch 87, iter 115275 with accuracy value: 0.6445.
epoch 88 iter 116600: loss = 3.2029, smooth loss = 3.6436, ce loss = 0.9606, contrastive loss = 1.2817, lr = 0.00013879705082894204
average data time = 0.0143s, average running time = 0.7419s
epoch 88 iter 116600: eval loss = 1.0836,  ccr = 0.7831,  cwr = 0.6437,  ted = 62679.0000,  ned = 12563.6298,  ted/w = 0.9848, 
epoch 89 iter 117925: loss = 3.5160, smooth loss = 3.6317, ce loss = 1.1144, contrastive loss = 1.2871, lr = 0.00013075795400786374
average data time = 0.0143s, average running time = 0.7431s
epoch 89 iter 117925: eval loss = 1.0792,  ccr = 0.7843,  cwr = 0.6436,  ted = 62613.0000,  ned = 12582.6509,  ted/w = 0.9838, 
epoch 90 iter 119250: loss = 3.6393, smooth loss = 3.6361, ce loss = 1.1779, contrastive loss = 1.2834, lr = 0.00012291296695512586
average data time = 0.0143s, average running time = 0.7444s
epoch 90 iter 119250: eval loss = 1.0822,  ccr = 0.7839,  cwr = 0.6446,  ted = 62747.0000,  ned = 12578.6589,  ted/w = 0.9859, 
Better model found at epoch 90, iter 119250 with accuracy value: 0.6446.
Save model train-seed-42-FuDan-Scene-32-256-bs-384-lr-0.0008-d_model-512-epoch-120-decay-cos-grad-clip-20-AdamW-wd-0.01-PA-decoder-max-len-40-sup_con-1.0-ce-2.0-temperature-0.15-warm_up-0.025_90_120000
epoch 91 iter 120575: loss = 3.9131, smooth loss = 3.6564, ce loss = 1.2957, contrastive loss = 1.3216, lr = 0.00011526774547223771
average data time = 0.0143s, average running time = 0.7456s
epoch 91 iter 120575: eval loss = 1.0821,  ccr = 0.7843,  cwr = 0.6446,  ted = 62666.0000,  ned = 12609.6552,  ted/w = 0.9846, 
epoch 92 iter 121900: loss = 3.6080, smooth loss = 3.6107, ce loss = 1.1618, contrastive loss = 1.2843, lr = 0.00010782780134078822
average data time = 0.0143s, average running time = 0.7468s
epoch 92 iter 121900: eval loss = 1.0850,  ccr = 0.7836,  cwr = 0.6438,  ted = 62479.0000,  ned = 12557.0722,  ted/w = 0.9817, 
epoch 93 iter 123225: loss = 3.4909, smooth loss = 3.5996, ce loss = 1.0988, contrastive loss = 1.2933, lr = 0.00010059849834875659
average data time = 0.0143s, average running time = 0.7480s
epoch 93 iter 123225: eval loss = 1.0731,  ccr = 0.7862,  cwr = 0.6477,  ted = 61757.0000,  ned = 12412.1461,  ted/w = 0.9703, 
Better model found at epoch 93, iter 123225 with accuracy value: 0.6477.
epoch 94 iter 124550: loss = 3.4022, smooth loss = 3.6057, ce loss = 1.0555, contrastive loss = 1.2913, lr = 9.358504842351783e-05
average data time = 0.0143s, average running time = 0.7491s
epoch 94 iter 124550: eval loss = 1.0741,  ccr = 0.7861,  cwr = 0.6473,  ted = 61891.0000,  ned = 12470.5727,  ted/w = 0.9724, 
epoch 95 iter 125875: loss = 3.6422, smooth loss = 3.6270, ce loss = 1.1735, contrastive loss = 1.2952, lr = 8.679250787433099e-05
average data time = 0.0143s, average running time = 0.7502s
epoch 95 iter 125875: eval loss = 1.0809,  ccr = 0.7848,  cwr = 0.6456,  ted = 62334.0000,  ned = 12496.3692,  ted/w = 0.9794, 
epoch 96 iter 127200: loss = 3.7917, smooth loss = 3.5836, ce loss = 1.2365, contrastive loss = 1.3187, lr = 8.022577374702106e-05
average data time = 0.0143s, average running time = 0.7512s
epoch 96 iter 127200: eval loss = 1.0780,  ccr = 0.7867,  cwr = 0.6472,  ted = 62152.0000,  ned = 12491.5165,  ted/w = 0.9765, 
epoch 97 iter 128525: loss = 3.4698, smooth loss = 3.5481, ce loss = 1.0998, contrastive loss = 1.2703, lr = 7.388958029347893e-05
average data time = 0.0143s, average running time = 0.7523s
epoch 97 iter 128525: eval loss = 1.0818,  ccr = 0.7851,  cwr = 0.6459,  ted = 62360.0000,  ned = 12486.5034,  ted/w = 0.9798, 
epoch 98 iter 129850: loss = 3.4115, smooth loss = 3.5700, ce loss = 1.0552, contrastive loss = 1.3010, lr = 6.778849555852853e-05
average data time = 0.0143s, average running time = 0.7533s
epoch 98 iter 129850: eval loss = 1.0788,  ccr = 0.7869,  cwr = 0.6482,  ted = 61628.0000,  ned = 12400.8586,  ted/w = 0.9683, 
Better model found at epoch 98, iter 129850 with accuracy value: 0.6482.
epoch 99 iter 131175: loss = 3.4032, smooth loss = 3.5680, ce loss = 1.0654, contrastive loss = 1.2723, lr = 6.192691808661902e-05
average data time = 0.0143s, average running time = 0.7543s
epoch 99 iter 131175: eval loss = 1.0833,  ccr = 0.7859,  cwr = 0.6477,  ted = 61812.0000,  ned = 12396.9042,  ted/w = 0.9712, 
epoch 100 iter 132500: loss = 3.4866, smooth loss = 3.5830, ce loss = 1.1044, contrastive loss = 1.2777, lr = 5.630907375071737e-05
average data time = 0.0144s, average running time = 0.7553s
epoch 100 iter 132500: eval loss = 1.0762,  ccr = 0.7868,  cwr = 0.6490,  ted = 61650.0000,  ned = 12409.4830,  ted/w = 0.9687, 
Better model found at epoch 100, iter 132500 with accuracy value: 0.6490.
epoch 101 iter 133825: loss = 3.1457, smooth loss = 3.5433, ce loss = 0.9338, contrastive loss = 1.2781, lr = 5.093901270568848e-05
average data time = 0.0144s, average running time = 0.7562s
epoch 101 iter 133825: eval loss = 1.0783,  ccr = 0.7882,  cwr = 0.6512,  ted = 61212.0000,  ned = 12278.3130,  ted/w = 0.9618, 
Better model found at epoch 101, iter 133825 with accuracy value: 0.6512.
epoch 102 iter 135150: loss = 3.3014, smooth loss = 3.5307, ce loss = 0.9968, contrastive loss = 1.3078, lr = 4.582060646835713e-05
average data time = 0.0144s, average running time = 0.7572s
epoch 102 iter 135150: eval loss = 1.0818,  ccr = 0.7866,  cwr = 0.6483,  ted = 61733.0000,  ned = 12408.0473,  ted/w = 0.9700, 
epoch 103 iter 136475: loss = 3.3793, smooth loss = 3.5275, ce loss = 1.0469, contrastive loss = 1.2855, lr = 4.09575451263587e-05
average data time = 0.0144s, average running time = 0.7582s
epoch 103 iter 136475: eval loss = 1.0789,  ccr = 0.7870,  cwr = 0.6487,  ted = 61646.0000,  ned = 12378.8400,  ted/w = 0.9686, 
epoch 104 iter 137800: loss = 3.5693, smooth loss = 3.5454, ce loss = 1.1325, contrastive loss = 1.3042, lr = 3.635333467779016e-05
average data time = 0.0144s, average running time = 0.7591s
epoch 104 iter 137800: eval loss = 1.0763,  ccr = 0.7870,  cwr = 0.6490,  ted = 61676.0000,  ned = 12394.7007,  ted/w = 0.9691, 
epoch 105 iter 139125: loss = 3.4750, smooth loss = 3.5528, ce loss = 1.1012, contrastive loss = 1.2725, lr = 3.201129450358016e-05
average data time = 0.0144s, average running time = 0.7601s
epoch 105 iter 139125: eval loss = 1.0794,  ccr = 0.7878,  cwr = 0.6503,  ted = 61392.0000,  ned = 12350.5830,  ted/w = 0.9646, 
epoch 106 iter 140450: loss = 3.7590, smooth loss = 3.5270, ce loss = 1.2177, contrastive loss = 1.3236, lr = 2.7934554974397916e-05
average data time = 0.0144s, average running time = 0.7610s
epoch 106 iter 140450: eval loss = 1.0778,  ccr = 0.7873,  cwr = 0.6493,  ted = 61558.0000,  ned = 12362.4013,  ted/w = 0.9672, 
epoch 107 iter 141775: loss = 3.2773, smooth loss = 3.5507, ce loss = 0.9909, contrastive loss = 1.2956, lr = 2.412605519382993e-05
average data time = 0.0144s, average running time = 0.7619s
epoch 107 iter 141775: eval loss = 1.0801,  ccr = 0.7870,  cwr = 0.6496,  ted = 61396.0000,  ned = 12349.7912,  ted/w = 0.9647, 
epoch 108 iter 143100: loss = 3.5845, smooth loss = 3.5022, ce loss = 1.1394, contrastive loss = 1.3057, lr = 2.0588540879448922e-05
average data time = 0.0144s, average running time = 0.7628s
epoch 108 iter 143100: eval loss = 1.0779,  ccr = 0.7882,  cwr = 0.6520,  ted = 61213.0000,  ned = 12243.2758,  ted/w = 0.9618, 
Better model found at epoch 108, iter 143100 with accuracy value: 0.6520.
epoch 109 iter 144425: loss = 3.8227, smooth loss = 3.4881, ce loss = 1.2454, contrastive loss = 1.3319, lr = 1.7324562383303276e-05
average data time = 0.0144s, average running time = 0.7637s
epoch 109 iter 144425: eval loss = 1.0823,  ccr = 0.7878,  cwr = 0.6500,  ted = 61305.0000,  ned = 12334.3545,  ted/w = 0.9632, 
epoch 110 iter 145750: loss = 3.6932, smooth loss = 3.4800, ce loss = 1.1987, contrastive loss = 1.2957, lr = 1.4336472853254332e-05
average data time = 0.0144s, average running time = 0.7645s
epoch 110 iter 145750: eval loss = 1.0811,  ccr = 0.7879,  cwr = 0.6513,  ted = 61154.0000,  ned = 12278.1182,  ted/w = 0.9609, 
epoch 111 iter 147075: loss = 3.1539, smooth loss = 3.5002, ce loss = 0.9427, contrastive loss = 1.2686, lr = 1.1626426536487078e-05
average data time = 0.0144s, average running time = 0.7653s
epoch 111 iter 147075: eval loss = 1.0802,  ccr = 0.7879,  cwr = 0.6511,  ted = 61201.0000,  ned = 12303.6654,  ted/w = 0.9616, 
epoch 112 iter 148400: loss = 3.4569, smooth loss = 3.5018, ce loss = 1.0725, contrastive loss = 1.3119, lr = 9.196377226417202e-06
average data time = 0.0145s, average running time = 0.7662s
epoch 112 iter 148400: eval loss = 1.0772,  ccr = 0.7876,  cwr = 0.6503,  ted = 61350.0000,  ned = 12334.1984,  ted/w = 0.9639, 
epoch 113 iter 149725: loss = 3.4253, smooth loss = 3.4770, ce loss = 1.0623, contrastive loss = 1.3007, lr = 7.04807685411396e-06
average data time = 0.0145s, average running time = 0.7670s
epoch 113 iter 149725: eval loss = 1.0748,  ccr = 0.7885,  cwr = 0.6514,  ted = 61238.0000,  ned = 12297.1559,  ted/w = 0.9622, 
epoch 114 iter 151050: loss = 3.4981, smooth loss = 3.4962, ce loss = 1.1032, contrastive loss = 1.2918, lr = 5.183074225255083e-06
average data time = 0.0145s, average running time = 0.7677s
epoch 114 iter 151050: eval loss = 1.0805,  ccr = 0.7879,  cwr = 0.6507,  ted = 61150.0000,  ned = 12290.2025,  ted/w = 0.9608, 
epoch 115 iter 152375: loss = 3.4850, smooth loss = 3.4737, ce loss = 1.0990, contrastive loss = 1.2870, lr = 3.6027139035234053e-06
average data time = 0.0145s, average running time = 0.7685s
epoch 115 iter 152375: eval loss = 1.0785,  ccr = 0.7881,  cwr = 0.6508,  ted = 61187.0000,  ned = 12292.3373,  ted/w = 0.9614, 
epoch 116 iter 153700: loss = 3.2612, smooth loss = 3.4980, ce loss = 0.9902, contrastive loss = 1.2808, lr = 2.308135241251002e-06
average data time = 0.0145s, average running time = 0.7692s
epoch 116 iter 153700: eval loss = 1.0777,  ccr = 0.7881,  cwr = 0.6514,  ted = 61147.0000,  ned = 12281.1401,  ted/w = 0.9608, 
epoch 117 iter 155025: loss = 3.6526, smooth loss = 3.4761, ce loss = 1.1758, contrastive loss = 1.3010, lr = 1.300271558009043e-06
average data time = 0.0145s, average running time = 0.7700s
epoch 117 iter 155025: eval loss = 1.0832,  ccr = 0.7880,  cwr = 0.6507,  ted = 61143.0000,  ned = 12288.4128,  ted/w = 0.9607, 
epoch 118 iter 156350: loss = 3.4430, smooth loss = 3.4530, ce loss = 1.0672, contrastive loss = 1.3085, lr = 5.79849467736198e-07
average data time = 0.0145s, average running time = 0.7707s
epoch 118 iter 156350: eval loss = 1.0835,  ccr = 0.7878,  cwr = 0.6503,  ted = 61179.0000,  ned = 12284.7935,  ted/w = 0.9613, 
epoch 119 iter 157675: loss = 3.3358, smooth loss = 3.4681, ce loss = 1.0294, contrastive loss = 1.2770, lr = 1.4738835489012343e-07
average data time = 0.0145s, average running time = 0.7714s
epoch 119 iter 157675: eval loss = 1.0770,  ccr = 0.7884,  cwr = 0.6513,  ted = 60990.0000,  ned = 12242.7258,  ted/w = 0.9583, 
